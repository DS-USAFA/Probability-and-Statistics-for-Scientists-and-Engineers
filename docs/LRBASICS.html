<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 27 Linear Regression Basics | Computational Probability and Statistics - MASTER</title>
<meta name="author" content="Matthew Davis">
<meta name="author" content="Brianna Hitt">
<meta name="author" content="Ken Horton">
<meta name="author" content="Kris Pruitt">
<meta name="author" content="Bradley Warner">
<meta name="description" content="27.1 Objectives Obtain parameter estimates of a simple linear regression model, given a sample of data. Interpret the coefficients of a simple linear regression. Create a scatterplot with a...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 27 Linear Regression Basics | Computational Probability and Statistics - MASTER">
<meta property="og:type" content="book">
<meta property="og:image" content="/figures/Cover_Majors.png">
<meta property="og:description" content="27.1 Objectives Obtain parameter estimates of a simple linear regression model, given a sample of data. Interpret the coefficients of a simple linear regression. Create a scatterplot with a...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 27 Linear Regression Basics | Computational Probability and Statistics - MASTER">
<meta name="twitter:description" content="27.1 Objectives Obtain parameter estimates of a simple linear regression model, given a sample of data. Interpret the coefficients of a simple linear regression. Create a scatterplot with a...">
<meta name="twitter:image" content="/figures/Cover_Majors.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Probability and Statistics - MASTER</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="objectives.html">Objectives</a></li>
<li class="book-part">Descriptive Statistical Modeling</li>
<li><a class="" href="CS1.html"><span class="header-section-number">1</span> Data Case Study</a></li>
<li><a class="" href="DB.html"><span class="header-section-number">2</span> Data Basics</a></li>
<li><a class="" href="ODCP.html"><span class="header-section-number">3</span> Overview of Data Collection Principles</a></li>
<li><a class="" href="STUDY.html"><span class="header-section-number">4</span> Studies</a></li>
<li><a class="" href="NUMDATA.html"><span class="header-section-number">5</span> Numerical Data</a></li>
<li><a class="" href="CATDATA.html"><span class="header-section-number">6</span> Categorical Data</a></li>
<li class="book-part">Probability Modeling</li>
<li><a class="" href="CS2.html"><span class="header-section-number">7</span> Probability Case Study</a></li>
<li><a class="" href="PROBRULES.html"><span class="header-section-number">8</span> Probability Rules</a></li>
<li><a class="" href="CONDPROB.html"><span class="header-section-number">9</span> Conditional Probability</a></li>
<li><a class="" href="RANDVAR.html"><span class="header-section-number">10</span> Random Variables</a></li>
<li><a class="" href="CONRANDVAR.html"><span class="header-section-number">11</span> Continuous Random Variables</a></li>
<li><a class="" href="DISCRETENAMED.html"><span class="header-section-number">12</span> Named Discrete Distributions</a></li>
<li><a class="" href="CONTNNAMED.html"><span class="header-section-number">13</span> Named Continuous Distributions</a></li>
<li><a class="" href="MULTIDISTS.html"><span class="header-section-number">14</span> Multivariate Distributions</a></li>
<li><a class="" href="MULTIEXP.html"><span class="header-section-number">15</span> Multivariate Expectation</a></li>
<li><a class="" href="TRANS.html"><span class="header-section-number">16</span> Transformations</a></li>
<li><a class="" href="EST.html"><span class="header-section-number">17</span> Estimation Methods</a></li>
<li class="book-part">Inferential Statistical Modeling</li>
<li><a class="" href="CS3.html"><span class="header-section-number">18</span> Hypothesis Testing Case Study</a></li>
<li><a class="" href="HYPTESTSIM.html"><span class="header-section-number">19</span> Hypothesis Testing with Simulation</a></li>
<li><a class="" href="HYPTESTDIST.html"><span class="header-section-number">20</span> Hypothesis Testing with Known Distributions</a></li>
<li><a class="" href="HYPTESTCLT.html"><span class="header-section-number">21</span> Hypothesis Testing with the Central Limit Theorem</a></li>
<li><a class="" href="ADDTESTS.html"><span class="header-section-number">22</span> Additional Hypothesis Tests</a></li>
<li><a class="" href="ANOVA.html"><span class="header-section-number">23</span> Analysis of Variance</a></li>
<li><a class="" href="CI.html"><span class="header-section-number">24</span> Confidence Intervals</a></li>
<li><a class="" href="BOOT.html"><span class="header-section-number">25</span> Bootstrap</a></li>
<li class="book-part">Predictive Statistical Modeling</li>
<li><a class="" href="CS4.html"><span class="header-section-number">26</span> Linear Regression Case Study</a></li>
<li><a class="active" href="LRBASICS.html"><span class="header-section-number">27</span> Linear Regression Basics</a></li>
<li><a class="" href="LRINF.html"><span class="header-section-number">28</span> Linear Regression Inference</a></li>
<li><a class="" href="LRDIAG.html"><span class="header-section-number">29</span> Regression Diagnostics</a></li>
<li><a class="" href="LRSIM.html"><span class="header-section-number">30</span> Simulation-Based Linear Regression</a></li>
<li><a class="" href="LRMULTI.html"><span class="header-section-number">31</span> Multiple Linear Regression</a></li>
<li><a class="" href="LOGREG.html"><span class="header-section-number">32</span> Logistic Regression</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/DS-USAFA/Probability-and-Statistics-MASTER">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="LRBASICS" class="section level1" number="27">
<h1>
<span class="header-section-number">27</span> Linear Regression Basics<a class="anchor" aria-label="anchor" href="#LRBASICS"><i class="fas fa-link"></i></a>
</h1>
<div id="objectives-27" class="section level2" number="27.1">
<h2>
<span class="header-section-number">27.1</span> Objectives<a class="anchor" aria-label="anchor" href="#objectives-27"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>Obtain parameter estimates of a simple linear regression model, given a sample of data.</p></li>
<li><p>Interpret the coefficients of a simple linear regression.</p></li>
<li><p>Create a scatterplot with a regression line.</p></li>
<li><p>Explain and check the assumptions of linear regression.</p></li>
<li><p>Use and be able to explain all new terminology, to include: <em>response</em>, <em>predictor</em>, <em>linear regression</em>, <em>simple linear regression</em>, <em>coefficients</em>, <em>residual</em>, <em>extrapolation</em>.</p></li>
</ol>
</div>
<div id="linear-regression-models" class="section level2" number="27.2">
<h2>
<span class="header-section-number">27.2</span> Linear regression models<a class="anchor" aria-label="anchor" href="#linear-regression-models"><i class="fas fa-link"></i></a>
</h2>
<p>The rest of this block will serve as a brief introduction to linear models. In general, a model estimates the relationship between one variable (a <strong>response</strong>) and one or more other variables (<strong>predictors</strong>). Models typically serve two purposes: <em>prediction</em> and <em>inference</em>. A model allows us to predict the value of a response given particular values of predictors. Also, a model allows us to make inferences about the relationship between the response and the predictors.</p>
<p>Not all models are used or built on the same principles. Some models are better for inference and others are better for prediction. Some models are better for qualitative responses and others are better for quantitative responses. Also, many models require making assumptions about the nature of the relationship between variables. If these assumptions are violated, the model loses usefulness. In a machine learning course, a wide array of models are discussed but most of which are used for the purpose of prediction.</p>
<p>In this block, we will focus on <em>linear models</em> and the use of linear regression to produce and evaluate the model. Linear regression is a very powerful statistical technique. Many people have some familiarity with regression just from reading the news, where graphs with straight lines are overlaid on scatterplots, much like we did in the last lesson. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables.</p>
<p>Suppose we were interested in exploring the relationship between one response variable (<span class="math inline">\(Y\)</span>) and one predictor variable (<span class="math inline">\(X\)</span>). We can postulate a linear relationship between the two:
<span class="math display">\[
Y=\beta_0+\beta_1 X
\]</span></p>
<p>A linear model can be expanded to include multiple predictor variables:
<span class="math display">\[
Y=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p
\]</span></p>
<p>This model is often referred to as a <strong>linear regression</strong> model. (When there is only one predictor variable, we often refer to it as a <strong>simple linear regression</strong> model.) The <span class="math inline">\(\beta_j\)</span> terms are referred to as <strong>coefficients</strong>. Note that the coefficients are <strong>parameters</strong> and thus represented as Greek letters. We typically don’t know the true values of <span class="math inline">\(\beta_j\)</span>, so we have to estimate them with samples from the population of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, our data. Estimating these parameters and using them for prediction and inference will be the majority of the work of this last block.</p>
<p>We consider the models above to be linear models because they are linear in the <strong>parameters</strong>. This means that the following model is also a linear model:</p>
<p><span class="math display">\[
Y=\beta_0+\beta_1 X^2
\]</span></p>
<p>Technically, we can write the parameters as a vector and the explanatory variables as a matrix. The response will then be an inner product of this vector and matrix and thus a linear combination.</p>
<p>Even if we expect two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to share a linear relationship, we don’t expect it to be perfect. There will be some scatter around the estimated line. For example, consider the head length and total length of 104 brushtail possums from Australia. The data is in the file <code>possum.csv</code> in the <code>data</code> folder.</p>
<blockquote>
<p><strong>Exercise</strong>:<br>
Read in the data from <code>data/possum.csv</code> and look at the first few rows of data.</p>
</blockquote>
<div class="sourceCode" id="cb852"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">possum</span><span class="op">&lt;-</span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span><span class="st">"data/possum.csv"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb853"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">possum</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Rows: 104
## Columns: 8
## $ site    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
## $ pop     &lt;chr&gt; "Vic", "Vic", "Vic", "Vic", "Vic", "Vic", "Vic", "Vic", "Vic",~
## $ sex     &lt;chr&gt; "m", "f", "f", "f", "f", "f", "m", "f", "f", "f", "f", "f", "m~
## $ age     &lt;dbl&gt; 8, 6, 6, 6, 2, 1, 2, 6, 9, 6, 9, 5, 5, 3, 5, 4, 1, 2, 5, 4, 3,~
## $ head_l  &lt;dbl&gt; 94.1, 92.5, 94.0, 93.2, 91.5, 93.1, 95.3, 94.8, 93.4, 91.8, 93~
## $ skull_w &lt;dbl&gt; 60.4, 57.6, 60.0, 57.1, 56.3, 54.8, 58.2, 57.6, 56.3, 58.0, 57~
## $ total_l &lt;dbl&gt; 89.0, 91.5, 95.5, 92.0, 85.5, 90.5, 89.5, 91.0, 91.5, 89.5, 89~
## $ tail_l  &lt;dbl&gt; 36.0, 36.5, 39.0, 38.0, 36.0, 35.5, 36.0, 37.0, 37.0, 37.5, 39~</code></pre>
<div class="sourceCode" id="cb855"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">possum</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 6 x 8
##    site pop   sex     age head_l skull_w total_l tail_l
##   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1     1 Vic   m         8   94.1    60.4    89     36  
## 2     1 Vic   f         6   92.5    57.6    91.5   36.5
## 3     1 Vic   f         6   94      60      95.5   39  
## 4     1 Vic   f         6   93.2    57.1    92     38  
## 5     1 Vic   f         2   91.5    56.3    85.5   36  
## 6     1 Vic   f         1   93.1    54.8    90.5   35.5</code></pre>
<p>We think the head and total length variables are linearly associated. Possums with an above average total length also tend to have above average head lengths. To visualize this data, we will use a scatterplot. We have used scatterplots multiple times in this book. Scatterplots are a graphical technique to present two numerical variables simultaneously. Such plots permit the relationship between the variables to be examined with ease. The following figure shows a scatterplot for the head length and total length of the possums. Each point represents a single possum from the data.</p>
<blockquote>
<p><strong>Exercise</strong>:<br>
Create a scatterplot of head length and total length.</p>
</blockquote>
<div class="sourceCode" id="cb857"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">possum</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_point</span><span class="op">(</span><span class="va">head_l</span><span class="op">~</span><span class="va">total_l</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>x<span class="op">=</span><span class="st">"Total Length (cm)"</span>,y<span class="op">=</span><span class="st">"Head Length (mm)"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_classic</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:scat261-fig"></span>
<img src="27-Linear-Regression-Basics_files/figure-html/scat261-fig-1.png" alt="A scatterplot of possum total length and head length." width="672"><p class="caption">
Figure 27.1: A scatterplot of possum total length and head length.
</p>
</div>
<p>From Figure <a href="LRBASICS.html#fig:scat261-fig">27.1</a>, we see that the relationship is not perfectly linear; however, it could be helpful to partially explain the connection between these variables with a straight line. Since some longer possums will have shorter heads and some shorter possums will have longer heads, there is no straight line that can go through all the data points. We expect some deviation from the linear fit. This deviation is represented by random variable <span class="math inline">\(e\)</span> (this is not the Euler number), which we refer to as an error term or residual:</p>
<p><span class="math display">\[
Y=\beta_0+\beta_1X+e
\]</span></p>
<p>For our problem, <span class="math inline">\(Y\)</span> is head length and <span class="math inline">\(X\)</span> is total length.</p>
<p>In general we have:</p>
<p><span class="math display">\[ \text{Data} = \text{Fit} + \text{Residual} \]</span></p>
<p>and what will change in our modeling process is how we specify the <span class="math inline">\(\text{Fit}\)</span>.</p>
<p>The error term is assumed to follow a normal distribution with mean 0 and constant standard deviation <span class="math inline">\(\sigma\)</span>. Note: the assumption of normality is only for inference using the <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> distributions and we can relax this assumption by using a bootstrap. Among other things, these assumptions imply that a linear model should only be used when the response variable is continuous in nature. There are other approaches for non-continuous response variables (for example logistic regression).</p>
<div id="estimation-1" class="section level3" number="27.2.1">
<h3>
<span class="header-section-number">27.2.1</span> Estimation<a class="anchor" aria-label="anchor" href="#estimation-1"><i class="fas fa-link"></i></a>
</h3>
<p>We want to describe the relationship between the head length and total length variables in the possum data set using a line. In this example, we will use the total length as the predictor variable, <span class="math inline">\(x\)</span>, to predict a possum’s head length, <span class="math inline">\(y\)</span>. Just as a side note, the choice of the predictor and response are not arbitrary. The response is typically what we want to predict or in the case of experiments is the causal result of the predictor(s).</p>
<p>In the possum data we have <span class="math inline">\(n = 104\)</span> observations: <span class="math inline">\((x_1,y_1), (x_2,y_2),...,(x_{104},y_{104})\)</span>. Then for each observation the implied linear model is:</p>
<p><span class="math display">\[
y_i=\beta_0+\beta x_i + e_i
\]</span></p>
<p>We could fit the linear relationship by eye like we did in the case study and obtain estimates of the slope and intercept but this is too ad hoc.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;If you want to do try this again use the &lt;code&gt;plot_ss()&lt;/code&gt; from the last lesson.&lt;/p&gt;"><sup>99</sup></a> So, given a set of data like the <code>possum</code>, how do we actually obtain estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>? What is the <strong>best</strong> fit line?</p>
<p>We begin by thinking about what we mean by ``best’’. Mathematically, we want a line that has small residuals. There are multiple methods, but the most common is the <em>method of least squares</em>. In this method, our goal is to find the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the squared vertical distance between the points and the resulting line, the <strong>residuals</strong>. See <a href="LRBASICS.html#fig:resid1-fig">27.2</a> for a visual representation involving only four observations from <em>made up</em> data.</p>
<div class="figure">
<span style="display:block;" id="fig:resid1-fig"></span>
<img src="27-Linear-Regression-Basics_files/figure-html/resid1-fig-1.png" alt="An illustration of the least squares method." width="672"><p class="caption">
Figure 27.2: An illustration of the least squares method.
</p>
</div>
<p>Our criterion for best is the estimates of slope and intercept that minimize the sum of the squared residuals:</p>
<p><span class="math display">\[e_{1}^2 + e_{2}^2 + \dots + e_{n}^2\]</span></p>
<p>The following are three possible reasons to choose the least squares criterion over other criteria such as the sum of the absolute value of residuals:</p>
<ol style="list-style-type: lower-roman">
<li>It is the most commonly used method.<br>
</li>
<li>Computing a line based on least squares was much easier by hand when computers were not available.<br>
</li>
<li>In many applications, a residual twice as large as another residual is more than twice as bad. For example, being off by 4 is usually more than twice as bad as being off by 2. Squaring the residuals accounts for this discrepancy.</li>
</ol>
<p>The first two reasons are largely for tradition and convenience; the last reason explains why least squares is typically helpful.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;There are applications where least absolute deviation may be more useful, and there are plenty of other criteria we might consider. However, this course only applies the least squares criterion. Math 378 will look at other criteria such as a shrinkage method called the lasso.&lt;/p&gt;"><sup>100</sup></a></p>
<p>So, we need to find <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the expression, observed minus expected:
<span class="math display">\[
\sum_{i=1}^n (y_i-\beta_0-\beta_1 x_i)^2
\]</span></p>
<p>Using calculus-based optimization yields the following estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[
\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
\]</span></p>
<p>Notice that this implies that the line will always go through the point <span class="math inline">\(\left(\bar{x},\bar{y} \right)\)</span>. As a reminder <span class="math inline">\(\bar{x}\)</span> is the sample mean of the explanatory variable and <span class="math inline">\(\bar{y}\)</span> is the sample mean of the response.</p>
<p>And</p>
<p><span class="math display">\[
\hat{\beta}_1 = {\sum x_i y_i - n\bar{x}\bar{y} \over{\sum x_i^2 -n\bar{x}^2}}
\]</span></p>
<p>A more intuitive formula for the slope and one that links <strong>correlation</strong> to linear regression is:</p>
<p><span class="math display">\[
\hat{\beta}_1 = \frac{s_y}{s_x} R
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is the correlation between the two variables, and <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the sample standard deviations of the explanatory variable and response, respectively. Thus the slope is proportional to the correlation.</p>
<p>You may also be interested in estimating <span class="math inline">\(\sigma\)</span>, the standard deviation of the error:
<span class="math display">\[
\hat{\sigma}=\sqrt{{1\over{n-2}} \sum_{i=1}^n \hat{e}_i^2}
\]</span></p>
<p>where <span class="math inline">\(\hat{e}_i\)</span> is the observed <span class="math inline">\(i\)</span>th <strong>residual</strong> (<span class="math inline">\(\hat{e}_i=y_i-\hat{\beta}_0-\hat{\beta}_1x_i\)</span>). This estimate is based only on the assumption of constant variance.</p>
</div>
<div id="possum-example" class="section level3" number="27.2.2">
<h3>
<span class="header-section-number">27.2.2</span> Possum example<a class="anchor" aria-label="anchor" href="#possum-example"><i class="fas fa-link"></i></a>
</h3>
<p>We will let <code>R</code> do the heavy work of minimizing the sum of squares. The function is <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> as we learned in the case study. This function needs a formula and data for input. The formula notation should be easy for us since we have worked with formulas so much in the <strong>mosaic</strong> package.</p>
<p>First create the model:</p>
<div class="sourceCode" id="cb858"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">poss_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">head_l</span><span class="op">~</span><span class="va">total_l</span>,data<span class="op">=</span><span class="va">possum</span><span class="op">)</span></span></code></pre></div>
<p>The output of the model object is minimal with just the estimated slope and intercept.</p>
<div class="sourceCode" id="cb859"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">poss_mod</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = head_l ~ total_l, data = possum)
## 
## Coefficients:
## (Intercept)      total_l  
##     42.7098       0.5729</code></pre>
<p>We can get more information using the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function:</p>
<div class="sourceCode" id="cb861"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">poss_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = head_l ~ total_l, data = possum)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1877 -1.5340 -0.3345  1.2788  7.3968 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 42.70979    5.17281   8.257 5.66e-13 ***
## total_l      0.57290    0.05933   9.657 4.68e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.595 on 102 degrees of freedom
## Multiple R-squared:  0.4776, Adjusted R-squared:  0.4725 
## F-statistic: 93.26 on 1 and 102 DF,  p-value: 4.681e-16</code></pre>
<p>The model object, <code>poss_mod</code>, contains much more information. Using the function <code><a href="https://rdrr.io/r/base/names.html">names()</a></code> function on the model objects, gives you a list of other quantities available, such as residuals.</p>
<div class="sourceCode" id="cb863"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">poss_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>##  [1] "coefficients"  "residuals"     "effects"       "rank"         
##  [5] "fitted.values" "assign"        "qr"            "df.residual"  
##  [9] "xlevels"       "call"          "terms"         "model"</code></pre>
<p>Figure <a href="LRBASICS.html#fig:scat262-fig">27.3</a> is a plot the data points and least squares line together.</p>
<div class="sourceCode" id="cb865"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">possum</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_point</span><span class="op">(</span> <span class="va">head_l</span> <span class="op">~</span> <span class="va">total_l</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_lm</span><span class="op">(</span>color<span class="op">=</span><span class="st">"black"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>x<span class="op">=</span><span class="st">"Total Length (cm)"</span>,y<span class="op">=</span><span class="st">"Head Length (mm)"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>title<span class="op">=</span><span class="st">"Possum data including regression line"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_classic</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:scat262-fig"></span>
<img src="27-Linear-Regression-Basics_files/figure-html/scat262-fig-1.png" alt="A scatterplot of possum total length and head length including a regression line." width="672"><p class="caption">
Figure 27.3: A scatterplot of possum total length and head length including a regression line.
</p>
</div>
</div>
<div id="interpretation" class="section level3" number="27.2.3">
<h3>
<span class="header-section-number">27.2.3</span> Interpretation<a class="anchor" aria-label="anchor" href="#interpretation"><i class="fas fa-link"></i></a>
</h3>
<p>Interpreting parameters in a regression model is often one of the most important steps in the analysis. The intercept term, <span class="math inline">\(\beta_0\)</span>, is usually uninteresting. It represents the <strong>average</strong> value of the response when the predictor is 0. Unless we center our predictor variable around 0, the actual value of the intercept is usually not important; it typically just gives the slope more flexibility to fit the data. The slope term, <span class="math inline">\(\beta_1\)</span> represents the <strong>average</strong> increase in the response variable per unit increase in the predictor variable. We keep using the word <strong>average</strong> in our discussion. With the assumption of a mean of 0 for the residuals, which least squares ensures with a line going through the point <span class="math inline">\(\left(\bar{x},\bar{y} \right)\)</span>, the output of the model is the expected or average response for the given input. Mathematically we have:</p>
<p><span class="math display">\[
E(Y|X=x)=E(\beta_0+\beta_1x+e) = E(\beta_0+\beta_1x)+E(e)=\beta_0+\beta_1x
\]</span></p>
<p>Predicting a value of the response variable simply becomes a matter of substituting the value of the predictor variable into the estimated regression equation. Again, it is important to note that for a given value of <span class="math inline">\(x\)</span>, the predicted response, <span class="math inline">\(\hat{y}\)</span> is what we expect the average value of <span class="math inline">\(y\)</span> to be given that specific value of the predictor.</p>
<blockquote>
<p><strong>Exercise</strong>: The slope and intercept estimates for the possum data are 0.5729 and 42.7098. What do these numbers really mean, interpret them?</p>
</blockquote>
<p>Interpreting the slope parameter is helpful in almost any application. For each additional 1 cm of total length of a possum, we would expect the possum’s head to be 0.5729 mm longer on average. Note that a longer total length corresponds to longer head because the slope coefficient is positive. We must be cautious in this interpretation: while there is a real association, we cannot interpret a causal connection between the variables because these data are observational. That is, increasing a possum’s total length may not cause the possum’s head to be longer.</p>
<p>The estimated intercept <span class="math inline">\(b_0=42.7\)</span> describes the average head length of a possum with zero total length! The meaning of the intercept is irrelevant to this application since a possum can not practically have a total length of 0.</p>
<p>Earlier we noted a relationship between the slope estimate and the correlation coefficient estimate.</p>
<blockquote>
<p><strong>Exercise</strong>
Find the slope from the correlation and standard deviations.</p>
</blockquote>
<div class="sourceCode" id="cb866"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">possum</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>correlation<span class="op">=</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/aggregating.html">cor</a></span><span class="op">(</span><span class="va">head_l</span>,<span class="va">total_l</span><span class="op">)</span>,sd_head<span class="op">=</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/aggregating.html">sd</a></span><span class="op">(</span><span class="va">head_l</span><span class="op">)</span>,</span>
<span>            sd_total<span class="op">=</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/aggregating.html">sd</a></span><span class="op">(</span><span class="va">total_l</span><span class="op">)</span>,slope<span class="op">=</span><span class="va">correlation</span><span class="op">*</span><span class="va">sd_head</span><span class="op">/</span><span class="va">sd_total</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   correlation sd_head sd_total slope
##         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
## 1       0.691    3.57     4.31 0.573</code></pre>
</div>
<div id="extrapolation-is-dangerous" class="section level3" number="27.2.4">
<h3>
<span class="header-section-number">27.2.4</span> Extrapolation is dangerous<a class="anchor" aria-label="anchor" href="#extrapolation-is-dangerous"><i class="fas fa-link"></i></a>
</h3>
<blockquote>
<p>“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February <span class="math inline">\(6^{th}\)</span> it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”
<em>Stephen Colbert</em>
April 6th, 2010<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;a href="http://www.colbertnation.com/the-colbert-report-videos/269929/" class="uri"&gt;http://www.colbertnation.com/the-colbert-report-videos/269929/&lt;/a&gt;&lt;/p&gt;'><sup>101</sup></a></p>
</blockquote>
<p>Linear models can be used to approximate the relationship between two variables and are built on an observed random sample. These models have real limitations as linear regression is simply a modeling framework. The truth is almost always much more complex than our simple line. <em>Extrapolation</em> occurs when one tries to make a prediction of a response given a value of the predictor that is outside the range of values used to build the model. We only have information about the relationship between two variables in the region around our observed data. We do not know how the data outside of our limited window will behave. Be careful about extrapolating.</p>
</div>
<div id="reading-computer-output" class="section level3" number="27.2.5">
<h3>
<span class="header-section-number">27.2.5</span> Reading computer output<a class="anchor" aria-label="anchor" href="#reading-computer-output"><i class="fas fa-link"></i></a>
</h3>
<p>We stored the results of our linear regression model for the possum data in the object <code>poss_mod</code> but it provided only the bare minimum of information. We can get more information using the function <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>.</p>
<div class="sourceCode" id="cb868"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">poss_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = head_l ~ total_l, data = possum)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1877 -1.5340 -0.3345  1.2788  7.3968 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 42.70979    5.17281   8.257 5.66e-13 ***
## total_l      0.57290    0.05933   9.657 4.68e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.595 on 102 degrees of freedom
## Multiple R-squared:  0.4776, Adjusted R-squared:  0.4725 
## F-statistic: 93.26 on 1 and 102 DF,  p-value: 4.681e-16</code></pre>
<p>The first line repeats the model formula. The second line is a descriptive summary of the residuals, plots of the residuals are more useful than this summary. We then have a table of the model fit. The first column of numbers provides estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively. For the next columns, we’ll describe the meaning of the columns using the second row, which corresponds to information about the slope estimate. Again, the first column provides the point estimate for <span class="math inline">\(\beta_1\)</span>. The second column is a standard error for this point estimate. The third column is a <span class="math inline">\(t\)</span> test statistic for the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>: <span class="math inline">\(T=9.657\)</span>. The last column is the <span class="math inline">\(p\)</span>-value for the <span class="math inline">\(t\)</span> test statistic for the null hypothesis <span class="math inline">\(\beta_1=0\)</span> and a two-sided alternative hypothesis. We will get into more of these details in the next chapters.</p>
<p>The row with the residual standard error is an estimate of the unexplained variance. The next rows give a summary of the model fit and we will discuss in the next chapters.</p>
<p>In the <code>tidyverse</code> we may want to have the table above in a <code>tibble</code>. The <strong>broom</strong> package, which we have seen before, helps with this effort.</p>
<div class="sourceCode" id="cb870"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb871"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="va">poss_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   42.7      5.17        8.26 5.66e-13
## 2 total_l        0.573    0.0593      9.66 4.68e-16</code></pre>
<blockquote>
<p><strong>Exercise</strong>:<br>
The <code>cars</code> dataset (built-in to <code>R</code>) contains 50 observations of 2 variables. The data give the speed of cars (in mph) and the corresponding distance (in feet) that it took to stop. Attempt to answer the following questions.</p>
</blockquote>
<ol style="list-style-type: lower-alpha">
<li>Build a simple linear regression model fitting distance against speed.</li>
</ol>
<div class="sourceCode" id="cb873"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cars_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">dist</span><span class="op">~</span><span class="va">speed</span>,data<span class="op">=</span><span class="va">cars</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">cars_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -29.069  -9.525  -2.272   9.215  43.201 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -17.5791     6.7584  -2.601   0.0123 *  
## speed         3.9324     0.4155   9.464 1.49e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 15.38 on 48 degrees of freedom
## Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 
## F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</code></pre>
<p>Figure <a href="LRBASICS.html#fig:scat263-fig">27.4</a> is a scatterplot of the <code>cars</code> data set.</p>
<div class="sourceCode" id="cb875"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cars</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_point</span><span class="op">(</span><span class="va">dist</span><span class="op">~</span><span class="va">speed</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_lm</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>x<span class="op">=</span><span class="st">"Speed (mph)"</span>,y<span class="op">=</span><span class="st">"Stopping distance (ft)"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:scat263-fig"></span>
<img src="27-Linear-Regression-Basics_files/figure-html/scat263-fig-1.png" alt="A scatterplot of speed and stopping distance." width="672"><p class="caption">
Figure 27.4: A scatterplot of speed and stopping distance.
</p>
</div>
<p>As expected, it appears that for larger speeds, stopping distance is greater.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Report and interpret the estimated model coefficients.</li>
</ol>
<p>The estimated intercept term, <span class="math inline">\(\hat{\beta}_0\)</span> is equal to -17.6. This estimate doesn’t have a helpful interpretation. Technically, it is the estimated average stopping distance for speed 0. However, “stopping distance” doesn’t make sense when a car has no speed. Also, a negative stopping distance doesn’t make sense. Furthermore, a speed of 0 is outside of the observed speeds in the data set, so even if a speed of 0 made sense, it is outside the scope of this data and thus an extrapolation.</p>
<p>The estimated slope term, <span class="math inline">\(\hat{\beta}_1\)</span> is equal to 3.9. This means that for an increase of one mph, we expect stopping distance to increase by 3.9 feet, on average.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Report the estimated common standard deviation of the residuals.</li>
</ol>
<p>The estimated standard deviation of the error (residual), <span class="math inline">\(\hat{\sigma}\)</span> is equal to 15.4.</p>
</div>
<div id="assumptions-1" class="section level3" number="27.2.6">
<h3>
<span class="header-section-number">27.2.6</span> Assumptions<a class="anchor" aria-label="anchor" href="#assumptions-1"><i class="fas fa-link"></i></a>
</h3>
<p>Anytime we build a model, there are assumptions behind it that, if violated, could invalidate the conclusions of the model. In the description of simple linear regression, we briefly mentioned these assumptions. Next chapter, we will discuss how to validate these assumptions.</p>
<p><strong>Fit</strong>. When we build a simple linear regression, we assume that the relationship between the response and the predictor is as we specify in the fit formula. This in simple linear regression is often just a linear relationship. Suppose two variables are non-linearly related, see <a href="LRBASICS.html#fig:resid2-fig">27.5</a>. While we could build a linear regression model between the two, the resulting model would not be very useful. If we built a model with the fit formulated as a quadratic, a similar plot of the residuals would look flat. We will discuss this more in a later chapter.</p>
<div class="figure">
<span style="display:block;" id="fig:resid2-fig"></span>
<img src="27-Linear-Regression-Basics_files/figure-html/resid2-fig-1.png" alt="An example of non-linear relationship between two variables fitted with a linear regression line." width="672"><p class="caption">
Figure 27.5: An example of non-linear relationship between two variables fitted with a linear regression line.
</p>
</div>
<p><strong>Independent Observations</strong>. Another assumption is that all observations in a data set are independent of one another. A common way this assumption is violated is by using time as the predictor variable. For example, suppose we were interested in how an individual’s weight changes over time. While it may be tempting to plot this and fit a regression line through the data, the resulting model is inappropriate, as simple linear regression assumes that each observation is independent. Figure <a href="LRBASICS.html#fig:resid3-fig">27.6</a> shows correlated data fitted with a linear regression line.</p>
<div class="figure">
<span style="display:block;" id="fig:resid3-fig"></span>
<img src="27-Linear-Regression-Basics_files/figure-html/resid3-fig-1.png" alt="A scatterplot of correlated data fit using a linear regression model with the assumption of independence." width="672"><p class="caption">
Figure 27.6: A scatterplot of correlated data fit using a linear regression model with the assumption of independence.
</p>
</div>
<p><strong>Constant Error Variance</strong>. In simple linear regression, we assume that the residuals come from a normal distribution with mean 0 and constant standard deviation <span class="math inline">\(\sigma\)</span>. Violation of this assumption is usually manifested as a “megaphone” pattern in the scatterplot. Specifically, as the value of the predictor increases, the variance in the response also increases, resulting in greater spread for larger values of the predictor.</p>
<p><strong>Normality of Errors</strong>. Again, we assume that the residuals are normally distributed. Normality of residuals is not easy to see graphically, so we have to use other diagnostics to check this assumption.</p>
<p>The last three assumptions are important not necessarily for estimating the relationship, but for <em>inferring</em> about the relationship. In future chapters, we will discuss how to use a model for prediction, and how to build a confidence/prediction interval around a prediction. Also, we will discuss inference about the coefficient estimates in a model. Violation of one of the last three assumptions will impact our ability to conduct inference about the population parameters.</p>
</div>
<div id="residual-plots" class="section level3" number="27.2.7">
<h3>
<span class="header-section-number">27.2.7</span> Residual plots<a class="anchor" aria-label="anchor" href="#residual-plots"><i class="fas fa-link"></i></a>
</h3>
<p>One purpose of residual plots is to identify characteristics or patterns still apparent in data after fitting a model. These can help to evaluate the assumptions.</p>
<p>Figure <a href="LRBASICS.html#fig:resid4-fig">27.7</a> shows three scatterplots with linear models in the first row and residual plots in the second row.</p>
<div class="figure">
<span style="display:block;" id="fig:resid4-fig"></span>
<img src="27-Linear-Regression-Basics_files/figure-html/resid4-fig-1.png" alt="Residual plots and associated scatterplots." width="672"><p class="caption">
Figure 27.7: Residual plots and associated scatterplots.
</p>
</div>
<p>In the first data set (first column), the residuals show no obvious patterns. The residuals appear to be scattered randomly around the dashed line that represents 0.</p>
<p>The second data set shows a pattern in the residuals. There is some curvature in the scatterplot, which is more obvious in the residual plot. We should not use a straight line to model these data. Instead, a more advanced technique should be used.</p>
<p>In the last plot the spread, variance of the data, seems to increase as the explanatory variable increases. We can see this clearly in the residual plot. To make inference using the <span class="math inline">\(t\)</span> or <span class="math inline">\(F\)</span> distribution would require a transformation to equalize the variance.</p>
</div>
<div id="summary-1" class="section level3" number="27.2.8">
<h3>
<span class="header-section-number">27.2.8</span> Summary<a class="anchor" aria-label="anchor" href="#summary-1"><i class="fas fa-link"></i></a>
</h3>
<p>We have introduced the ideas of linear regression in this lesson. There are many new terms as well as new <code>R</code> functions to learn. We will continue to use these ideas in the remainder of this block of material. Next we will learn about inference and prediction.</p>
</div>
</div>
<div id="homework-problems-26" class="section level2" number="27.3">
<h2>
<span class="header-section-number">27.3</span> Homework Problems<a class="anchor" aria-label="anchor" href="#homework-problems-26"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>Nutrition at Starbucks</li>
</ol>
<p>In the <code>data</code> folder is a file named <code>starbucks.csv</code>. Use it to answer the questions below.</p>
<ol style="list-style-type: lower-alpha">
<li>Create a scatterplot of number of calories and amount of carbohydrates.<br>
</li>
<li>Describe the relationship in the graph.<br>
</li>
<li>In this scenario, what are the explanatory and response variables?<br>
</li>
<li>Why might we want to fit a regression line to these data?<br>
</li>
<li>Create a scatterplot of number of calories and amount of carbohydrates with the regression line included.<br>
</li>
<li>Using ’lm()` fit a least squares line to the data.<br>
</li>
<li>Report and interpret the slope coefficient.<br>
</li>
<li>For a menu item with 51 g of carbs, what is the estimated calorie count?<br>
</li>
<li>Could we use the model for a menu item with 100 g of carbs?<br>
</li>
<li>Does the assumption of constant variance seem reasonable for this problem?<br>
</li>
<li>Verify that the line passes through the mean carb and mean calories, do this mathematically.<br>
</li>
<li>What is the estimate of the standard deviation of the residuals? How could you use this information?</li>
</ol>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="CS4.html"><span class="header-section-number">26</span> Linear Regression Case Study</a></div>
<div class="next"><a href="LRINF.html"><span class="header-section-number">28</span> Linear Regression Inference</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#LRBASICS"><span class="header-section-number">27</span> Linear Regression Basics</a></li>
<li><a class="nav-link" href="#objectives-27"><span class="header-section-number">27.1</span> Objectives</a></li>
<li>
<a class="nav-link" href="#linear-regression-models"><span class="header-section-number">27.2</span> Linear regression models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimation-1"><span class="header-section-number">27.2.1</span> Estimation</a></li>
<li><a class="nav-link" href="#possum-example"><span class="header-section-number">27.2.2</span> Possum example</a></li>
<li><a class="nav-link" href="#interpretation"><span class="header-section-number">27.2.3</span> Interpretation</a></li>
<li><a class="nav-link" href="#extrapolation-is-dangerous"><span class="header-section-number">27.2.4</span> Extrapolation is dangerous</a></li>
<li><a class="nav-link" href="#reading-computer-output"><span class="header-section-number">27.2.5</span> Reading computer output</a></li>
<li><a class="nav-link" href="#assumptions-1"><span class="header-section-number">27.2.6</span> Assumptions</a></li>
<li><a class="nav-link" href="#residual-plots"><span class="header-section-number">27.2.7</span> Residual plots</a></li>
<li><a class="nav-link" href="#summary-1"><span class="header-section-number">27.2.8</span> Summary</a></li>
</ul>
</li>
<li><a class="nav-link" href="#homework-problems-26"><span class="header-section-number">27.3</span> Homework Problems</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/DS-USAFA/Probability-and-Statistics-MASTER/blob/master/27-Linear-Regression-Basics.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/DS-USAFA/Probability-and-Statistics-MASTER/edit/master/27-Linear-Regression-Basics.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Probability and Statistics - MASTER</strong>" was written by Matthew Davis, Brianna Hitt, Ken Horton, Kris Pruitt, Bradley Warner. It was last built on 2022-07-17.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
