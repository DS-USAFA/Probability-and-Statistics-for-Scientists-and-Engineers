[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"book based notes created students part one semester course probability statistics. developed notes three primary resources. important Openintro Introductory Statistics Randomization Simulation (Diez, Barr, Çetinkaya-Rundel 2014) book. parts, used notes homework problems. However, cases altered work fit needs. second important book work Introduction Probability Statistics Using R (Kerns 2010). Finally, used examples, code, ideas first edition Prium’s book, Foundations Applications Statistics: Introduction Using R (R. J. Pruim 2011).","code":""},{"path":"index.html","id":"who-is-this-book-for","chapter":"Preface","heading":"0.1 Who is this book for?","text":"designed book study statistics maximizes computational ideas minimizing algebraic symbol manipulation. Although discuss traditional small-sample, normal-based inference classical probability distributions, rely heavily ideas simulation, permutations, bootstrap. means students background differential integral calculus successful book.book makes extensive using R programming language. particular focus tidyverse mosaic packages. include significant amount code notes frequently demonstrate multiple ways completing task. used book junior sophomore college students.","code":""},{"path":"index.html","id":"book-structure-and-how-to-use-it","chapter":"Preface","heading":"0.2 Book structure and how to use it","text":"book divided four parts. part begins case study introduces many main ideas part. chapter designed standalone 50 minute lesson. Within lesson, give exercises can worked class provide learning objectives.book assumes students access R. Finally, keep number homework problems reasonable level assign problems.four parts book :Descriptive Statistical Modeling: part introduces student data collection methods, summary statistics, visual summaries, exploratory data analysis.Descriptive Statistical Modeling: part introduces student data collection methods, summary statistics, visual summaries, exploratory data analysis.Probability Modeling: discuss foundational ideas probability, counting methods, common distributions. use calculus simulation find moments probabilities. introduce basic ideas multivariate probability. include method moments maximum likelihood estimators.Probability Modeling: discuss foundational ideas probability, counting methods, common distributions. use calculus simulation find moments probabilities. introduce basic ideas multivariate probability. include method moments maximum likelihood estimators.Inferential Statistical Modeling: discuss many basic inference ideas found traditional introductory statistics class add ideas bootstrap permutation methods.Inferential Statistical Modeling: discuss many basic inference ideas found traditional introductory statistics class add ideas bootstrap permutation methods.Predictive Statistical Modeling: final part introduces prediction methods, mainly form linear regression. part also includes inference regression.Predictive Statistical Modeling: final part introduces prediction methods, mainly form linear regression. part also includes inference regression.learning outcomes course use computational mathematical statistical/probabilistic concepts :Developing probabilistic models.Developing statistical models description, inference, prediction.Advancing practical theoretical analytic experience skills.","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"0.3 Prerequisites","text":"take course, students expected completed calculus including integral calculus. multivariate ideas course, easily taught don’t require calculus III. don’t assume students programming experience , thus, include great deal code. historically supplemented course Data Camp courses. also used RStudio Cloud help students get started R without burden loading maintaining software.","code":""},{"path":"index.html","id":"packages","chapter":"Preface","heading":"0.4 Packages","text":"notes make use following packages R: knitr (Xie 2022b), rmarkdown (Allaire et al. 2022), mosaic (R. Pruim, Kaplan, Horton 2021), mosaicCalc (Kaplan, Pruim, Horton 2020), tidyverse (Wickham 2021), ISLR (James et al. 2021), vcd (Meyer, Zeileis, Hornik 2022), ggplot2 (Wickham et al. 2021), MASS (Ripley 2022), openintro (Çetinkaya-Rundel et al. 2022), broom (Robinson, Hayes, Couch 2022), infer (Bray et al. 2021), kableExtra (Zhu 2021), DT (Xie, Cheng, Tan 2022).","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Preface","heading":"0.5 Acknowledgements","text":"lucky numerous open sources help facilitate work. Thank helped correct mistakes include Skyler Royse.book written using bookdown package (Xie 2022a).book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"index.html","id":"file-creation-information","chapter":"Preface","heading":"0.6 File Creation Information","text":"File creation date: 2022-06-30R version 4.1.3 (2022-03-10)","code":""},{"path":"CS1.html","id":"CS1","chapter":"1 Case Study","heading":"1 Case Study","text":"","code":""},{"path":"CS1.html","id":"objectives","chapter":"1 Case Study","heading":"1.1 Objectives","text":"Use R basic analysis visualization.Compile report using knitr.","code":""},{"path":"CS1.html","id":"introduction-to-descriptive-statistical-modeling","chapter":"1 Case Study","heading":"1.2 Introduction to descriptive statistical modeling","text":"first block material, focus data types, collection methods, summaries, visualizations. also intend introduce computing via R package. Programming R requires focus early course supplement online courses. relatively little mathematics first block.","code":""},{"path":"CS1.html","id":"the-data-analytic-process","chapter":"1 Case Study","heading":"1.3 The data analytic process","text":"Scientists seek answer questions using rigorous methods careful observations. observations – collected likes field notes, surveys, experiments – form backbone statistical investigation called data. Statistics study best collect, analyze, draw conclusions data. helpful put statistics context general process investigation:Identify question problem.Identify question problem.Collect relevant data topic.Collect relevant data topic.Explore understand data.Explore understand data.Analyze data.Analyze data.Form conclusion.Form conclusion.Make decisions based conclusion.Make decisions based conclusion.typical explanatory process starts research question proceeds. However, sometimes analysis exploratory nature. data necessarily research question. purpose analysis find interesting features data sometimes generate hypotheses. course focus explanatory aspects analysis.Statistics subject focuses making stages 2-5 objective, rigorous, efficient. , statistics three primary components:best can collect data?analyzed?can infer analysis?topics scientists investigate diverse questions ask. However, many investigations can addressed small number data collection techniques, analytic tools, fundamental concepts statistical inference. lesson provides glimpse themes encounter throughout rest course.","code":""},{"path":"CS1.html","id":"case-study","chapter":"1 Case Study","heading":"1.4 Case study","text":"lesson consider experiment studies effectiveness stents treating patients risk stroke. 1 2 Stents small mesh tubes placed inside narrow weak arteries assist patient recovery cardiac events reduce risk additional heart attack death. Many doctors hoped similar benefits patients risk stroke. start writing principal question researchers hope answer:","code":""},{"path":"CS1.html","id":"research-question","chapter":"1 Case Study","heading":"1.4.1 Research question","text":"use stents reduce risk stroke?","code":""},{"path":"CS1.html","id":"collect-the-relevant-data","chapter":"1 Case Study","heading":"1.4.2 Collect the relevant data","text":"researchers asked question collected data 451 -risk patients. volunteer patient randomly assigned one two groups:Treatment group. Patients treatment group received stent medical management. medical management included medications, management risk factors, help lifestyle modification.Control group. Patients control group received medical management treatment group receive stents.Researchers randomly assigned 224 patients treatment group 227 control group. study, control group provides reference point can measure medical impact stents treatment group.experiment observational study. learn ideas block.Researchers studied effect stents two time points: 30 days enrollment 365 days enrollment.","code":""},{"path":"CS1.html","id":"import-data","chapter":"1 Case Study","heading":"1.4.3 Import data","text":"begin first use R.need install package, likely CRAN, Comprehensive R Archive Network. \npackage can used, must installed computer (per computer account) loaded session (per R session). exit R, package stays installed computer reloaded R started .summary, R packages can downloaded installed online repositories CRAN. install package, needs done per computer account, R placing source code library folder designated installation R. Packages typically collections functions variables specific certain task subject matter.example, install mosaic package, enter:RStudio, Packages tab makes easy add maintain packages.use package session, must load . makes available current session . start R , load packages . command library() package name supplied argument needed. session, load tidyverse mosaic. Note: box executing R commands, known reproducible research since can see code can run modify need.Next read data working environment.Let’s break code . reading .csv file assigning results object called stent_study. assignment arrow <- means assign right left. R function use case read_csv(). using R functions, ask :want R ?want R ?information must provide R ?information must provide R ?want R read .csv file. can get help function typing ?read_csv help(read_csv) prompt. required input read_csv() file location. data stored folder called “data” working directory. can determine working directory typing getwd() prompt.Similarly, wish change working directory, can using setwd() function:R use view(), see data looks like standard spreadsheet.","code":"install.packages(\"mosaic\") # fetch package from CRAN\nlibrary(tidyverse)\nlibrary(mosaic)\nstent_study <- read_csv(\"data/stent_study.csv\")\ngetwd()\nsetwd('C:/Users/Brad.Warner/Documents/Classes/Prob Stat/Another Folder')\nview(stent_study)"},{"path":"CS1.html","id":"explore-data","chapter":"1 Case Study","heading":"1.4.4 Explore data","text":"attempt answer research question, let’s look data. want R print first 10 rows data. appropriate function head() needs data object. default, R output first 6 rows. using n = argument, can specify many rows want view.also want “inspect” data. function inspect() R needs data object stent_study.keep things simple, look outcome30 variable case study. summarize data table. Later course, learn using tidy package; now use mosaic package. package makes use modeling formula use extensively later course. modeling formula also used Math 378.want summarize data making table. mosaic, use tally() function. using function, understand basic formula notation mosaic uses. basic format :read y ~ x “y tilde x” interpret equivalent forms: “y broken x”; “y modeled x”; “y explained x”; “y depends x”; “y accounted x.” graphics, ’s reasonable read formula “y vs. x”, exactly convention used coordinate axes.exercise, want apply tally() variables group outcome30. case matter call y x; however, natural think outcome30 dependent variable.margins option totals columns.224 patients treatment group, 33 stroke end first month. Using two numbers, can use R compute proportion patients treatment group stroke end first month.Exercise:\nproportion control group stroke first 30 days study? proportion different proportion reported inspect()?Let’s R calculate proportions us. Use ? help() look help menu tally(). Note one option arguments tally() function format =. Setting equal proportion output proportions instead counts.can compute summary statistics table. summary statistic single number summarizing large amount data.3 instance, primary results study 1 month described two summary statistics: proportion people stroke treatment group proportion people stroke control group.Proportion stroke treatment (stent) group: \\(33/224 = 0.15 = 15\\%\\)Proportion stroke treatment (stent) group: \\(33/224 = 0.15 = 15\\%\\)Proportion stroke control group: \\(13/227 = 0.06 = 6\\%\\)Proportion stroke control group: \\(13/227 = 0.06 = 6\\%\\)","code":"\nhead(stent_study, n = 10)## # A tibble: 10 x 3\n##    group   outcome30 outcome365\n##    <chr>   <chr>     <chr>     \n##  1 control no_event  no_event  \n##  2 trmt    no_event  no_event  \n##  3 control no_event  no_event  \n##  4 trmt    no_event  no_event  \n##  5 trmt    no_event  no_event  \n##  6 control no_event  no_event  \n##  7 trmt    no_event  no_event  \n##  8 control no_event  no_event  \n##  9 control no_event  no_event  \n## 10 control no_event  no_event\ninspect(stent_study)## \n## categorical variables:  \n##         name     class levels   n missing\n## 1      group character      2 451       0\n## 2  outcome30 character      2 451       0\n## 3 outcome365 character      2 451       0\n##                                    distribution\n## 1 control (50.3%), trmt (49.7%)                \n## 2 no_event (89.8%), stroke (10.2%)             \n## 3 no_event (83.8%), stroke (16.2%)goal(y ~ x, data = MyData, ...) # pseudo-code for the formula template\ntally(outcome30 ~ group, data = stent_study, margins = TRUE)##           group\n## outcome30  control trmt\n##   no_event     214  191\n##   stroke        13   33\n##   Total        227  224\n33 / (33 + 191)## [1] 0.1473214\ntally(outcome30 ~ group, data = stent_study, format = 'proportion', margins = TRUE)##           group\n## outcome30     control       trmt\n##   no_event 0.94273128 0.85267857\n##   stroke   0.05726872 0.14732143\n##   Total    1.00000000 1.00000000"},{"path":"CS1.html","id":"visualize-the-data","chapter":"1 Case Study","heading":"1.4.5 Visualize the data","text":"often important visualize data. table type visualization, section introduce graphical method called bar charts.use ggformula package visualize data. wrapper ggplot2 package becoming industry standard generating professional graphics. However, interface ggplot2 can difficult learn ease using ggformula, makes use formula notation introduced . ggformula package loaded loaded mosaic.4To generate basic graphic, need ask information trying see, particular type graph best, corresponding R function use, information R function needs order build plot. categorical data, want bar chart R function gf_bar() needs data object variable(s) interest.first attempt. Figure 1.1, leave y portion formula blank. implies simply want view number/count outcome30 type. see two levels outcome30 x-axis counts y-axis.\nFigure 1.1: Using ggformula create bar chart.\nExercise:\nExplain Figure 1.1.plot graphically shows us total number “stroke” total number “no_event”. However, want. want compare 30-day outcomes treatment groups. , need break data different groups based treatment type. formula notation, now update form:read y ~ x|z “y tilde x z” interpret equivalent forms: “y modeled x z”; “y explained x within z”; “y accounted x within z.” graphics, ’s reasonable read formula “y vs. x z”. Figure 1.2 shows results.\nFigure 1.2: Bar charts conditioned group variable.\n","code":"\ngf_bar(~ outcome30, data = stent_study)goal(y ~ x|z, data = MyData, ...) # pseudo-code for the formula template\ngf_bar(~ outcome30 | group, data = stent_study) "},{"path":"CS1.html","id":"more-advanced-graphics","chapter":"1 Case Study","heading":"1.4.5.1 More advanced graphics","text":"prelude things come, graphic needs work. labels don’t help title. add color. make sense use proportions? code results better graph, see Figure 1.3. Don’t worry seems bit advanced, feel free examine new component code.\nFigure 1.3: Better graph.\nNotice used pipe operator, %>%. operator allows us string functions together manner makes easier read code. code, sending data object stent_study function gf_props() use data, don’t need data = argument. math, composition functions. Instead f(g(x)) use pipe f(g(x)) = g(x) %>% f().","code":"\nstent_study %>%\ngf_props(~ group, fill = ~ outcome30, position = 'fill') %>%\n  gf_labs(title = \"Impact of Stents of Stroke\",\n          subtitle = 'Experiment with 451 Patients',\n          x = \"Experimental Group\",\n          y = \"Number of Events\") %>%\n  gf_theme(theme_bw())"},{"path":"CS1.html","id":"conclusion","chapter":"1 Case Study","heading":"1.4.6 Conclusion","text":"two summary statistics (proportions people stroke) useful looking differences groups, surprise: additional 9% patients treatment group stroke! important two reasons. First, contrary doctors expected, stents reduce rate strokes. Second, leads statistical question: data show real difference due treatment?second question subtle. Suppose flip coin 100 times. chance coin lands heads given coin flip 50%, probably won’t observe exactly 50 heads. type fluctuation part almost type data generating process. possible 9% difference stent study due natural variation. However, larger difference observe (particular sample size), less believable difference due chance. really asking following: difference large reject notion due chance?preview step 4, analyze data, step 5, form conclusion, analysis cycle. haven’t yet covered statistical tools fully address steps, can comprehend conclusions published analysis: compelling evidence harm stents study stroke patients.careful: generalize results study patients stents. study looked patients specific characteristics volunteered part study may representative stroke patients. addition, many types stents study considered self-expanding Wingspan stent (Boston Scientific). However, study leave us important lesson: keep eyes open surprises.","code":""},{"path":"CS1.html","id":"homework-problems","chapter":"1 Case Study","heading":"1.5 Homework Problems","text":"Create Rmd file 01 Data Case Study Application.Rmd R (may provided), start inserting name header. Code blocks can inserted can complete code answer questions. done, knit pdf file.create R code chunk, use CTRL-ALT-insert tab window, use drop select R. Anything dashes interpreted R code.RMarkdown, see following video: https://www.youtube.com/watch?v=DNS7i2m4sB0. video assumes using R computer, using RStudio Cloud. Thus can knit pdf since setup us. can also take first chapter Data Camp course, Reporting R Markdown, learn .Stent study continued. Complete similar analysis stent data, time use one year outcome. particular,Read data working directory.Complete similar steps class notes. start code provided .\n. Use inspect data.\nii. Create table outcome365 group. Comment results.\niii. Create barchart data.Using inspect:table:Barchart:Migraine acupuncture. migraine particularly painful type headache, patients sometimes wish treat acupuncture. determine whether acupuncture relieves migraine pain, researchers conducted randomized controlled study 89 females diagnosed migraine headaches randomly assigned one two groups: treatment control. 43 patients treatment group received acupuncture specifically designed treat migraines. 46 patients control group received placebo acupuncture (needle insertion nonacupoint locations). 24 hours patients received acupuncture, asked pain free.5The data file migraine_study.csv data folder.Complete following work:Read data object called migraine_study.Create table data.Report percent patients treatment group pain free 24 hours receiving acupuncture.Repeat control group.first glance, acupuncture appear effective treatment migraines? Explain reasoning.data provide convincing evidence real pain reduction patients treatment group? think observed difference might just due chance?Compile, knit, report html pdf. order knit report pdf, may need install knitr tinytex packages R.","code":"stent_study <- read_csv(___)inspect(___)tally(outcome365 ~ ___, data = stent_study, format = ___, margins = TRUE)stent_study %>%\n  gf_props(~ ___, fill = ~ ___, position = 'fill') %>%\n  gf_labs(title = ___,\n          subtitle = ___,\n          x = ___,\n          y = ___)migraine_study <- read_csv(\"data/___\")head(migraine_study)tally(___)"},{"path":"DB.html","id":"DB","chapter":"2 Data Basics","heading":"2 Data Basics","text":"","code":""},{"path":"DB.html","id":"objectives-1","chapter":"2 Data Basics","heading":"2.1 Objectives","text":"Define use properly context new terminology include, limited , case, observational unit, variables, data frame, associated variables, independent, discrete continuous variables.Identify define different types variables.Given study description, explain research question.Create scatterplot R determine association two numerical variables plot.","code":""},{"path":"DB.html","id":"data-basics","chapter":"2 Data Basics","heading":"2.2 Data basics","text":"Effective presentation description data first step analyses. lesson introduces one structure organizing data, well terminology used throughout course.","code":""},{"path":"DB.html","id":"observations-variables-and-data-matrices","chapter":"2 Data Basics","heading":"2.2.1 Observations, variables, and data matrices","text":"reference using data set concerning 50 emails received 2012. observations referred email50 data set, random sample larger data set. data openintro package let’s install load package.Table 2.1 shows 4 rows email50 data set elected list 5 variables ease observation.row table represents single email case.6 columns represent characteristics, called variables, emails. example, first row represents email 1, spam, contains 21,705 characters, 551 line breaks, written HTML format, contains small numbers.\nTable 2.1: First 5 rows email data frame\nLet’s look first 10 rows data email50 using R. Remember ask two questions:want R ? andWhat must give R ?want first 10 rows use head() R needs data object number rows. data object called email50 accessible openintro package loaded.practice, especially important ask clarifying questions ensure important aspects data understood. instance, always important sure know variable means units measurement. Descriptions variables email50 data set given documentation can accessed R using ? command:(Note data sets associated documentation; authors openintro package included documentation email50 data set contained package.)data email50 represent data matrix, R terminology data frame tibble 7, common way organize data. row data matrix corresponds unique case, column corresponds variable. called tidy data.8 data frame stroke study introduced previous lesson patients cases three variables recorded patient. thinking patients unit observation, data tidy.think outcome unit observation, tidy since two outcome columns variable values (month year). tidy data case :three interrelated rules make data set tidy:variable must column.observation must row.value must cell.ensure data tidy? two main advantages:’s general advantage picking one consistent way storing data. consistent data structure, ’s easier learn tools work underlying uniformity.’s general advantage picking one consistent way storing data. consistent data structure, ’s easier learn tools work underlying uniformity.’s specific advantage placing variables columns allows R’s vectorized nature shine. clear progress studies. Since built-R functions work vectors values, makes transforming tidy data feel particularly natural.’s specific advantage placing variables columns allows R’s vectorized nature shine. clear progress studies. Since built-R functions work vectors values, makes transforming tidy data feel particularly natural.Data frames convenient way record store data. another individual case added data set, additional row can easily added. Similarly, another column can added new variable.Exercise:\nconsider publicly available data set summarizes information 3,142 counties United States, create data set called county_subset data set. data set include information county: name, state resides, population 2000 2010, per capita federal spending, poverty rate, four additional characteristics. create data object code following description. parent data set part usdata library called county_complete. variables summarized help menu built usdata package9. might data organized data matrix? 10Using R create data object. First load library usdata.want subset columns use select verb dplyr select rename columns. also create new variable federal spending per capita using mutate function.Using R, display seven rows county_subset data frame.","code":"\ninstall.packages(\"openintro\")\nlibrary(openintro)\nhead(email50, n = 10)## # A tibble: 10 x 21\n##    spam  to_multiple from     cc sent_email time                image attach\n##    <fct> <fct>       <fct> <int> <fct>      <dttm>              <dbl>  <dbl>\n##  1 0     0           1         0 1          2012-01-04 06:19:16     0      0\n##  2 0     0           1         0 0          2012-02-16 13:10:06     0      0\n##  3 1     0           1         4 0          2012-01-04 08:36:23     0      2\n##  4 0     0           1         0 0          2012-01-04 10:49:52     0      0\n##  5 0     0           1         0 0          2012-01-27 02:34:45     0      0\n##  6 0     0           1         0 0          2012-01-17 10:31:57     0      0\n##  7 0     0           1         0 0          2012-03-17 22:18:55     0      0\n##  8 0     0           1         0 1          2012-03-31 07:58:56     0      0\n##  9 0     0           1         1 1          2012-01-10 18:57:54     0      0\n## 10 0     0           1         0 0          2012-01-07 12:29:16     0      0\n## # ... with 13 more variables: dollar <dbl>, winner <fct>, inherit <dbl>,\n## #   viagra <dbl>, password <dbl>, num_char <dbl>, line_breaks <int>,\n## #   format <fct>, re_subj <fct>, exclaim_subj <dbl>, urgent_subj <fct>,\n## #   exclaim_mess <dbl>, number <fct>?email50## # A tibble: 10 x 3\n##    group   outcome30 outcome365\n##    <chr>   <chr>     <chr>     \n##  1 control no_event  no_event  \n##  2 trmt    no_event  no_event  \n##  3 control no_event  no_event  \n##  4 trmt    no_event  no_event  \n##  5 trmt    no_event  no_event  \n##  6 control no_event  no_event  \n##  7 trmt    no_event  no_event  \n##  8 control no_event  no_event  \n##  9 control no_event  no_event  \n## 10 control no_event  no_event## # A tibble: 10 x 4\n##    patient_id group   time  result  \n##         <int> <chr>   <chr> <chr>   \n##  1          1 control month no_event\n##  2          1 control year  no_event\n##  3          2 trmt    month no_event\n##  4          2 trmt    year  no_event\n##  5          3 control month no_event\n##  6          3 control year  no_event\n##  7          4 trmt    month no_event\n##  8          4 trmt    year  no_event\n##  9          5 trmt    month no_event\n## 10          5 trmt    year  no_event\nlibrary(usdata)\ncounty_subset <- county_complete %>% \n  select(name, state, pop2000, pop2010, fed_spend = fed_spending_2009, \n         poverty = poverty_2010, homeownership = homeownership_2010, \n         multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, \n         med_income = median_household_income_2010) %>%\n  mutate(fed_spend = fed_spend / pop2010)\nhead(county_subset, n = 7)##             name   state pop2000 pop2010 fed_spend poverty homeownership\n## 1 Autauga County Alabama   43671   54571  6.068095    10.6          77.5\n## 2 Baldwin County Alabama  140415  182265  6.139862    12.2          76.7\n## 3 Barbour County Alabama   29038   27457  8.752158    25.0          68.0\n## 4    Bibb County Alabama   20826   22915  7.122016    12.6          82.9\n## 5  Blount County Alabama   51024   57322  5.130910    13.4          82.0\n## 6 Bullock County Alabama   11714   10914  9.973062    25.3          76.9\n## 7  Butler County Alabama   21399   20947  9.311835    25.0          69.0\n##   multi_unit income med_income\n## 1        7.2  24568      53255\n## 2       22.6  26469      50147\n## 3       11.1  15875      33219\n## 4        6.6  19918      41770\n## 5        3.7  21070      45549\n## 6        9.9  20289      31602\n## 7       13.7  16916      30659"},{"path":"DB.html","id":"types-of-variables","chapter":"2 Data Basics","heading":"2.2.2 Types of variables","text":"Examine fed_spend, pop2010, state variables county data set. variables inherently different others, yet many share certain characteristics.First consider fed_spend. said numerical variable since can take wide range numerical values, sensible add, subtract, take averages values. hand, classify variable reporting telephone area codes numerical; even though area codes made numerical digits, average, sum, difference clear meaning.pop2010 variable also numerical; sensible add, subtract, take averages values, although seems little different fed_spend. variable population count can whole non-negative number (\\(0\\), \\(1\\), \\(2\\), \\(...\\)). reason, population variable said discrete since can take specific numerical values. hand, federal spending variable said continuous. Now technically, truly continuous numerical variables since measurements finite level accuracy measurement precision. However, course treat types numerical variables , continuous variables statistical modeling. place different course probability models, see probability modeling section.variable state can take 51 values, accounting Washington, DC, summarized : Alabama, Alaska, …, Wyoming. responses categories, state called categorical variable,11 possible values called variable’s levels.\nFigure 2.1: Taxonomy Variables.\nFinally, consider hypothetical variable education, describes highest level education completed takes one values noHS, HS, College Graduate_school. variable seems hybrid: categorical variable levels natural ordering. variable properties called ordinal variable. categorical variable levels natural ordering called nominal variable. simplify analyses, ordinal variables course treated nominal categorical variables. R, categorical variables can treated different ways; one key differences can leave character values factors. R handles factors, concerned levels values factors. learn progress.Figure 2.1 captures classification variables described.Exercise:\nData collected students statistics course. Three variables recorded student: number siblings, student height, whether student previously taken statistics course. Classify variables continuous numerical, discrete numerical, categorical.number siblings student height represent numerical variables. number siblings count, discrete. Height varies continuously, continuous numerical variable. last variable classifies students two categories – taken statistics course – makes variable categorical.Exercise:\nConsider variables group outcome30 stent study case study lesson. numerical categorical variables? 12","code":""},{"path":"DB.html","id":"relationships-between-variables","chapter":"2 Data Basics","heading":"2.2.3 Relationships between variables","text":"Many analyses motivated researcher looking relationship two variables. heart statistical modeling. social scientist may like answer following questions:federal spending, average, higher lower counties high rates poverty?homeownership lower national average one county, percent multi-unit structures county likely national average?answer questions, data must collected, county_complete data set. Examining summary statistics provide insights two questions counties. Graphs can used visually summarize data useful answering questions well.Scatterplots one type graph used study relationship two numerical variables. Figure 2.2 compares variables fed_spend poverty. point plot represents single county. instance, highlighted dot corresponds County 1088 county_subset data set: Owsley County, Kentucky, poverty rate 41.5% federal spending $21.50 per capita. dense cloud scatterplot suggests relationship two variables: counties high poverty rate also tend slightly federal spending. might brainstorm relationship exists investigate idea determine reasonable explanation.\nFigure 2.2: scatterplot showing fed_spend poverty. Owsley County Kentucky, poverty rate 41.5% federal spending $21.50 per capita, highlighted.\nExercise:\nExamine variables email50 data set. Create two research questions relationships variables interest .13The fed_spend poverty variables said associated plot shows discernible pattern. two variables show connection one another, called associated variables. Associated variables can also called dependent variables vice-versa.Example:\nrelationship homeownership rate percent units multi-unit structures (e.g. apartments, condos) visualized using scatterplot Figure 2.3. variables associated?appears larger fraction units multi-unit structures, lower homeownership rate. Since relationship variables, associated.\nFigure 2.3: scatterplot homeownership rate versus percent units multi-unit structures 3,143 counties.\ndownward trend Figure 2.3 – counties units multi-unit structures associated lower homeownership – variables said negatively associated. positive association shown relationship poverty fed_spend variables represented Figure 2.2, counties higher poverty rates tend receive federal spending per capita.two variables associated, said independent. , two variables independent evident relationship two.pair variables either related way (associated) (independent). pair variables associated independent.","code":""},{"path":"DB.html","id":"creating-a-scatterplot","chapter":"2 Data Basics","heading":"2.2.4 Creating a scatterplot","text":"section, create simple scatterplot ask create one . First, recreate scatterplot seen Figure 2.2. figure uses county_subset data set.two questions:want R ? andWhat must give R ?want R create scatterplot needs, minimum, data object, want \\(x\\)-axis, want \\(y\\)-axis. information ggformula can found clicking link.14\nFigure 2.4: Scatterplot ggformula.\nFigure 2.4 bad. poor axis labels, title, dense clustering points, \\(y\\)-axis driven couple extreme points. need clear . , try read code use help() ? determine purpose command Figure 2.5.\nFigure 2.5: Better example scatterplot.\nExercise:\nCreate scatterplot Figure 2.3.","code":"\ncounty_subset %>%\n  gf_point(fed_spend ~ poverty)\ncounty_subset %>%\n  filter(fed_spend < 32) %>%\n  gf_point(fed_spend ~ poverty,\n           xlab = \"Poverty Rate (Percent)\", \n           ylab = \"Federal Spending Per Capita\",\n           title = \"A scatterplot showing fed_spend against poverty\", \n           cex = 1, alpha = 0.2) %>%\n  gf_theme(theme_classic())"},{"path":"DB.html","id":"homework-problems-1","chapter":"2 Data Basics","heading":"2.3 Homework Problems","text":"Identify study componentsIdentify () cases, (ii) variables types, (iii) main research question studies described .Researchers collected data examine relationship pollutants preterm births Southern California. study, air pollution levels measured air quality monitoring stations. Specifically, levels carbon monoxide recorded parts per million, nitrogen dioxide ozone parts per hundred million, coarse particulate matter (PM\\(_{10}\\)) \\(\\mu g/m^3\\). Length gestation data collected 143,196 births years 1989 1993, air pollution exposure gestation calculated birth. analysis suggested increased ambient PM\\(_{10}\\) , lesser degree, CO concentrations may associated occurrence preterm births.15Researchers collected data examine relationship pollutants preterm births Southern California. study, air pollution levels measured air quality monitoring stations. Specifically, levels carbon monoxide recorded parts per million, nitrogen dioxide ozone parts per hundred million, coarse particulate matter (PM\\(_{10}\\)) \\(\\mu g/m^3\\). Length gestation data collected 143,196 births years 1989 1993, air pollution exposure gestation calculated birth. analysis suggested increased ambient PM\\(_{10}\\) , lesser degree, CO concentrations may associated occurrence preterm births.15The Buteyko method shallow breathing technique developed Konstantin Buteyko, Russian doctor, 1952. Anecdotal evidence suggests Buteyko method can reduce asthma symptoms improve quality life. scientific study determine effectiveness method, researchers recruited 600 asthma patients aged 18-69 relied medication asthma treatment. patients split two research groups: one practiced Buteyko method . Patients scored quality life, activity, asthma symptoms, medication reduction scale 0 10. average, participants Buteyko group experienced significant reduction asthma symptoms improvement quality life.16The Buteyko method shallow breathing technique developed Konstantin Buteyko, Russian doctor, 1952. Anecdotal evidence suggests Buteyko method can reduce asthma symptoms improve quality life. scientific study determine effectiveness method, researchers recruited 600 asthma patients aged 18-69 relied medication asthma treatment. patients split two research groups: one practiced Buteyko method . Patients scored quality life, activity, asthma symptoms, medication reduction scale 0 10. average, participants Buteyko group experienced significant reduction asthma symptoms improvement quality life.16In package Stat2Data data set called Election16. Create scatterplot percent advanced degree versus per capita income state. Describe relationship two variables. Note: may load library.package Stat2Data data set called Election16. Create scatterplot percent advanced degree versus per capita income state. Describe relationship two variables. Note: may load library.","code":""},{"path":"ODCP.html","id":"ODCP","chapter":"3 Overview of Data Collection Principles","heading":"3 Overview of Data Collection Principles","text":"","code":""},{"path":"ODCP.html","id":"objectives-2","chapter":"3 Overview of Data Collection Principles","heading":"3.1 Objectives","text":"Define use properly context new terminology.description research project, minimum able describe population interest, generalizability study, response predictor variables, differentiate whether observational experimental, determine type sample.Explain context problem conduct sample different types sampling procedures.","code":""},{"path":"ODCP.html","id":"overview-of-data-collection-principles","chapter":"3 Overview of Data Collection Principles","heading":"3.2 Overview of data collection principles","text":"first step conducting research identify topics questions investigated. clearly laid research question helpful identifying subjects cases studied variables important. also important consider data collected reliable help achieve research goals.","code":""},{"path":"ODCP.html","id":"populations-and-samples","chapter":"3 Overview of Data Collection Principles","heading":"3.2.1 Populations and samples","text":"Consider following three research questions:average mercury content swordfish Atlantic Ocean?last 5 years, average time complete degree Duke undergraduate students?new drug reduce number deaths patients severe heart disease?research question refers target population. first question, target population swordfish Atlantic Ocean, fish represents case. usually expensive collect data every case population. Instead, sample taken. sample represents subset cases often small fraction population. instance, 60 swordfish (number) population might selected, sample data may used provide estimate population average answer research question.Exercise:\nsecond third questions , identify target population represents individual case.17","code":""},{"path":"ODCP.html","id":"anecdotal-evidence","chapter":"3 Overview of Data Collection Principles","heading":"3.2.2 Anecdotal evidence","text":"Consider following possible responses three research questions:man news got mercury poisoning eating swordfish, average mercury concentration swordfish must dangerously high.met two students took 7 years graduate Duke, must take longer graduate Duke many colleges.friend’s dad heart attack died gave new heart disease drug, drug must work.conclusion based data. However, two problems. First, data represent one two cases. Second, importantly, unclear whether cases actually representative population. Data collected haphazard fashion called anecdotal evidence.\nFigure 3.1: February 2010, media pundits cited one large snow storm evidence global warming. comedian Jon Stewart pointed , ‘’s one storm, one region, one country.’\nAnecdotal evidence:\ncareful data collected haphazardly. evidence may true verifiable, may represent extraordinary cases.Anecdotal evidence typically composed unusual cases recall based striking characteristics. instance, likely remember two people met took 7 years graduate six others graduated four years. Instead looking unusual cases, examine sample many cases represent population.","code":""},{"path":"ODCP.html","id":"sampling-from-a-population","chapter":"3 Overview of Data Collection Principles","heading":"3.2.3 Sampling from a population","text":"might try estimate time graduation Duke undergraduates last 5 years collecting sample students. graduates last 5 years represent population, graduates selected review collectively called sample. general, always seek randomly select sample population. basic type random selection equivalent raffles conducted. example, selecting graduates, write graduate’s name raffle ticket draw 100 tickets. selected names represent random sample 100 graduates. illustrated Figure 3.2.\nFigure 3.2: graphic, five graduates randomly selected population included sample.\npick sample randomly? just pick sample hand? Consider following scenario.Example:\nSuppose ask student happens majoring nutrition select several graduates study. kind students think might collect? think sample representative graduates?\n18\nFigure 3.3: Instead sampling graduates equally, nutrition major might inadvertently pick graduates health-related majors disproportionately often.\nsomeone permitted pick choose exactly graduates included sample, entirely possible sample skewed person’s interests, may entirely unintentional. introduces bias sample, see Figure 3.3. Sampling randomly helps resolve problem. basic random sample called simple random sample, equivalent using raffle select cases. means case population equal chance included implied connection cases sample.Sometimes simple random sample difficult implement alternative method helpful. One substitute systematic sample, one case sampled letting fixed number others, say 10 cases, pass . Since approach uses mechanism easily subject personal biases, often yields reasonably representative sample. course focus simple random samples since use systematic samples uncommon requires additional considerations context.act taking simple random sample helps minimize bias. However, bias can crop ways. Even people picked random, e.g. surveys, caution must exercised non-response high. instance, 30% people randomly sampled survey actually respond, unclear whether respondents representative entire population, survey might suffer non-response bias.\nFigure 3.4: Due possibility non-response, surveys studies may reach certain group within population. difficult, often impossible, completely fix problem\nAnother common pitfall convenience sample, individuals easily accessible likely included sample, see Figure 3.4 . instance, political survey done stopping people walking Bronx, represent New York City. often difficult discern sub-population convenience sample represents.Exercise:\ncan easily access ratings products, sellers, companies websites. ratings based people go way provide rating. 50% online reviews product negative, think means 50% buyers dissatisfied product?19","code":""},{"path":"ODCP.html","id":"explanatory-and-response-variables","chapter":"3 Overview of Data Collection Principles","heading":"3.2.4 Explanatory and response variables","text":"Consider following question county data set:federal spending, average, higher lower counties high rates poverty?suspect poverty might affect spending county, poverty explanatory variable federal spending response variable relationship.20 many variables, may possible consider number explanatory variables.Explanatory response variables\nidentify explanatory variable pair variables, identify two suspected affecting .Caution:\nAssociation imply causation. Labeling variables explanatory response guarantee relationship two actually causal, even association identified two variables. use labels keep track variable suspect affects . also use language help use R formula notation.cases, explanatory response variable. Consider following question:homeownership particular county lower national average, percent multi-unit structures county likely national average?difficult decide variables considered explanatory response variable; .e. direction ambiguous, explanatory response labels suggested .","code":""},{"path":"ODCP.html","id":"introducing-observational-studies-and-experiments","chapter":"3 Overview of Data Collection Principles","heading":"3.2.5 Introducing observational studies and experiments","text":"two primary types data collection: observational studies experiments.Researchers perform observational study collect data way directly interfere data arise. instance, researchers may collect information via surveys, review medical company records, follow cohort many similar individuals study certain diseases might develop. situations, researchers merely observe happens. general, observational studies can provide evidence naturally occurring association variables, , show causal connection.researchers want investigate possibility causal connection, conduct experiment. Usually explanatory response variable. instance, may suspect administering drug reduce mortality heart attack patients following year. check really causal connection explanatory variable response, researchers collect sample individuals split groups. individuals group assigned treatment. individuals randomly assigned treatment group, experiment called randomized experiment. example, heart attack patient drug trial randomly assigned, perhaps flipping coin, one two groups: first group receives placebo (fake treatment) second group receives drug. case study beginning semester another example experiment, though study employ placebo. Math 359 course design analysis experimental data, DOE. Air Force types experiments important part test evaluation. Many Air Force analysts expert practitioners DOE. course minimize discussion DOE.Association \\(\\neq\\) Causation\n, association imply causation. data analysis, association imply causation, causation can inferred randomized experiment. Although, hot field analysis causal relationships observational data. important consider cigarette smoking, know causes lung cancer? observational data clearly experiment. think analysts charged near future using causal reasoning observational data.","code":""},{"path":"ODCP.html","id":"homework-problems-2","chapter":"3 Overview of Data Collection Principles","heading":"3.3 Homework Problems","text":"Generalizability causality. Identify population interest sample studies described . studies previous lesson. Also comment whether results study can generalized population findings study can used establish causal relationships.Researchers collected data examine relationship pollutants preterm births Southern California. study air pollution levels measured air quality monitoring stations. Specifically, levels carbon monoxide recorded parts per million, nitrogen dioxide ozone parts per hundred million, coarse particulate matter (PM\\(_{10}\\)) \\(\\mu g/m^3\\). Length gestation data collected 143,196 births years 1989 1993, air pollution exposure gestation calculated birth. analysis suggested increased ambient PM\\(_{10}\\) , lesser degree, CO concentrations may associated occurrence preterm births.21The Buteyko method shallow breathing technique developed Konstantin Buteyko, Russian doctor, 1952. Anecdotal evidence suggests Buteyko method can reduce asthma symptoms improve quality life. scientific study determine effectiveness method, researchers recruited 600 asthma patients aged 18-69 relied medication asthma treatment. patients split two research groups: one practiced Buteyko method . Patients scored quality life, activity, asthma symptoms, medication reduction scale 0 10. average, participants Buteyko group experienced significant reduction asthma symptoms improvement quality life.22GPA study time. survey conducted 55 undergraduates Duke University took introductory statistics course Spring 2012. Among many questions, survey asked GPA number hours spent studying per week. scatterplot displays relationship two variables.explanatory variable response variable?Describe relationship two variables. Make sure discuss unusual observations, .experiment observational study?Can conclude studying longer hours leads higher GPAs?Income education scatterplot shows relationship per capita income (thousands dollars) percent population bachelor’s degree 3,143 counties US 2010.explanatory response variables?Describe relationship two variables. Make sure discuss unusual observations, .Can conclude bachelor’s degree increases one’s income?","code":""},{"path":"STUDY.html","id":"STUDY","chapter":"4 Studies","heading":"4 Studies","text":"","code":""},{"path":"STUDY.html","id":"objectives-3","chapter":"4 Studies","heading":"4.1 Objectives","text":"Define use properly context new terminology.Given study description, able identify explain study using correct terms.Given scenario, describe flaws reasoning propose study sampling designs.","code":""},{"path":"STUDY.html","id":"observation-studies-sampling-strategies-and-experiments","chapter":"4 Studies","heading":"4.2 Observation studies, sampling strategies, and experiments","text":"","code":""},{"path":"STUDY.html","id":"observational-studies","chapter":"4 Studies","heading":"4.2.1 Observational studies","text":"Generally, data observational studies collected monitoring occurs, experiments require primary explanatory variable study assigned subject researchers.Making causal conclusions based experiments often reasonable. However, making causal conclusions based observational data can treacherous recommended. Thus, observational studies generally sufficient show associations.Exercise:\nSuppose observational study tracked sunscreen use skin cancer, found sunscreen someone used, likely person skin cancer. mean sunscreen causes skin cancer?23Some previous research24 tells us using sunscreen actually reduces skin cancer risk, maybe another variable can explain hypothetical association sunscreen usage skin cancer. One important piece information absent sun exposure. someone sun day, likely use sunscreen likely get skin cancer. Exposure sun unaccounted simple investigation.\nFigure 4.1: Sun exposure confounding variable related response explanatory variables.\nSun exposure called confounding variable,25 variable correlated explanatory response variables, see Figure 4.1 . one method justify making causal conclusions observational studies exhaust search confounding variables, guarantee confounding variables can examined measured.Let’s look example confounding visually. Using SAT data mosaic package let’s look expenditure per pupil versus SAT scores. Figure 4.2 plot data.Exercise:\nconclusion reach plot Figure 4.2?26\nFigure 4.2: Average SAT score versus expenditure per pupil; reminder: observation represents individual state.\nimplication spending less might give better results justified. Expenditures confounded proportion students take exam, scores higher states fewer students take exam.interesting look original plot place states two groups depending whether \nfewer 40% students take SAT. Figure 4.3 plot data broken 2 groups.\nFigure 4.3: Average SAT score versus expenditure per pupil; broken level participation.\naccount fraction students taking SAT, relationship expenditures SAT scores changes.way, county data set observational study confounding variables, data easily used make causal conclusions.Exercise:\nFigure 4.4 shows negative association homeownership rate percentage multi-unit structures county. However, unreasonable conclude causal relationship two variables. Suggest one variables might explain relationship Figure 4.4.27\nFigure 4.4: scatterplot homeownership rate versus percent units multi-unit structures 3,143 counties.\nObservational studies come two forms: prospective retrospective studies. prospective study identifies individuals collects information events unfold. instance, medical researchers may identify follow group similar individuals many years assess possible influences behavior cancer risk. One example study Nurses Health Study, started 1976 expanded 1989.28 prospective study recruits registered nurses collects data using questionnaires.Retrospective studies collect data events taken place; e.g. researchers may review past events medical records. data sets, county, may contain prospectively- retrospectively-collected variables. Local governments prospectively collect variables events unfolded (e.g. retail sales) federal government retrospectively collected others 2010 census (e.g. county population).","code":""},{"path":"STUDY.html","id":"three-sampling-methods","chapter":"4 Studies","heading":"4.2.2 Three sampling methods","text":"Almost statistical methods based notion implied randomness. observational data collected random framework population, results statistical methods reliable. consider three random sampling techniques: simple, stratified, cluster sampling. Figures 4.5 , 4.6 , 4.7 provides graphical representation techniques.\nFigure 4.5: Examples simple random sampling. figure, simple random sampling used randomly select 18 cases.\n\nFigure 4.6: figure, stratified sampling used: cases grouped strata, simple random sampling employed within stratum.\n\nFigure 4.7: figure, cluster sampling used, data binned nine clusters, three clusters randomly selected.\nSimple random sampling probably intuitive form random sampling. Consider salaries Major League Baseball (MLB) players, player member one league’s 30 teams. take simple random sample 120 baseball players salaries 2010 season, write names season’s 828 players onto slips paper, drop slips bucket, shake bucket around sure names mixed , draw slips sample 120 players. general, sample referred ``simple random’’ case population equal chance included final sample knowing case included sample provide useful information cases included.Stratified sampling divide--conquer sampling strategy. population divided groups called strata. strata chosen similar cases grouped together, second sampling method, usually simple random sampling, employed within stratum. baseball salary example, teams represent strata; teams lot money (’re looking , Yankees). might randomly sample 4 players team total 120 players.Stratified sampling especially useful cases stratum similar respect outcome interest. downside analyzing data stratified sample complex task analyzing data simple random sample. analysis methods introduced course need extended analyze data collected using stratified sampling.Example:\ngood cases within stratum similar?29In cluster sampling, group observations clusters, randomly sample clusters. Sometimes cluster sampling can economical technique alternatives. Also, unlike stratified sampling, cluster sampling helpful lot case--case variability within cluster clusters don’t look different one another. example, neighborhoods represented clusters, sampling method works best neighborhoods diverse. downside cluster sampling advanced analysis techniques typically required, though methods course can extended handle data.Example:\nSuppose interested estimating malaria rate densely tropical portion rural Indonesia. learn 30 villages part Indonesian jungle, less similar next. sampling method employed?30Another technique called multistage sampling similar cluster sampling, except take simple random sample within selected cluster. instance, sampled neighborhoods using cluster sampling, next sample subset homes within selected neighborhood using multistage sampling.","code":""},{"path":"STUDY.html","id":"experiments","chapter":"4 Studies","heading":"4.2.3 Experiments","text":"Studies researchers assign treatments cases called experiments. assignment includes randomization, e.g. using coin flip decide treatment patient receives, called randomized experiment. Randomized experiments fundamentally important trying show causal connection two variables.","code":""},{"path":"STUDY.html","id":"principles-of-experimental-design","chapter":"4 Studies","heading":"4.2.3.1 Principles of experimental design","text":"Randomized experiments generally built four principles.Controlling. Researchers assign treatments cases, best control differences groups. example, patients take drug pill form, patients take pill sip water others may entire glass water. control effect water consumption, doctor may ask patients drink 12 ounce glass water pill.Controlling. Researchers assign treatments cases, best control differences groups. example, patients take drug pill form, patients take pill sip water others may entire glass water. control effect water consumption, doctor may ask patients drink 12 ounce glass water pill.Randomization. Researchers randomize patients treatment groups account variables controlled. example, patients may susceptible disease others due dietary habits. Randomizing patients treatment control group helps even differences, also prevents accidental bias entering study.Randomization. Researchers randomize patients treatment groups account variables controlled. example, patients may susceptible disease others due dietary habits. Randomizing patients treatment control group helps even differences, also prevents accidental bias entering study.Replication. cases researchers observe, accurately can estimate effect explanatory variable response. single study, replicate collecting sufficiently large sample. Additionally, group scientists may replicate entire study verify earlier finding. replicate level variability want estimate. example, flight test, can run flight conditions get replicate; however, plane pilot used, replicate getting pilot--pilot plane--plane variability.Replication. cases researchers observe, accurately can estimate effect explanatory variable response. single study, replicate collecting sufficiently large sample. Additionally, group scientists may replicate entire study verify earlier finding. replicate level variability want estimate. example, flight test, can run flight conditions get replicate; however, plane pilot used, replicate getting pilot--pilot plane--plane variability.Blocking. Researchers sometimes know suspect variables, treatment, influence response. circumstances, may first group individuals based variable randomize cases within block treatment groups. strategy often referred blocking. instance, looking effect drug heart attacks, might first split patients low-risk high-risk blocks, randomly assign half patients block control group half treatment group, shown Figure 4.8. strategy ensures treatment group equal number low-risk high-risk patients.Blocking. Researchers sometimes know suspect variables, treatment, influence response. circumstances, may first group individuals based variable randomize cases within block treatment groups. strategy often referred blocking. instance, looking effect drug heart attacks, might first split patients low-risk high-risk blocks, randomly assign half patients block control group half treatment group, shown Figure 4.8. strategy ensures treatment group equal number low-risk high-risk patients.\nFigure 4.8: Blocking using variable depicting patient risk. Patients first divided low-risk high-risk blocks, block evenly divided treatment groups using randomization. strategy ensures equal representation patients treatment group low-risk high-risk categories.\nimportant incorporate first three experimental design principles study, course describes methods analyzing data experiments. Blocking slightly advanced technique, statistical methods course may extended analyze data collected using blocking. Math 359 entire course devoted design analysis experiments.","code":""},{"path":"STUDY.html","id":"reducing-bias-in-human-experiments","chapter":"4 Studies","heading":"4.2.3.2 Reducing bias in human experiments","text":"Randomized experiments gold standard data collection, ensure unbiased perspective cause effect relationships cases. Human studies perfect examples bias can unintentionally arise. reconsider study new drug used treat heart attack patients.31 particular, researchers wanted know drug reduced deaths patients.researchers designed randomized experiment wanted draw causal conclusions drug’s effect. Study volunteers32 randomly placed two study groups. One group, treatment group, received drug. group, called control group, receive drug treatment.Put place person study. treatment group, given fancy new drug anticipate help . hand, person group doesn’t receive drug sits idly, hoping participation doesn’t increase risk death. perspectives suggest actually two effects: one interest effectiveness drug, second emotional effect difficult quantify.Researchers aren’t usually interested emotional effect, might bias study. circumvent problem, researchers want patients know group . researchers keep patients uninformed treatment, study said blind. one problem: patient doesn’t receive treatment, know control group. solution problem give fake treatments patients control group. fake treatment called placebo, effective placebo key making study truly blind. classic example placebo sugar pill made look like actual treatment pill. Often times, placebo results slight real improvement patients. effect dubbed placebo effect.patients ones blinded: doctors researchers can accidentally bias study. doctor knows patient given real treatment, might inadvertently give patient attention care patient knows placebo. guard bias, found measurable effect instances, modern studies employ double-blind setup doctors researchers interact patients , just like patients, unaware receiving treatment.33Exercise:\nLook back stent study first lesson researchers testing whether stents effective reducing strokes -risk patients. experiment? study blinded? double-blinded?34","code":""},{"path":"STUDY.html","id":"homework-problems-3","chapter":"4 Studies","heading":"4.3 Homework Problems","text":"Propose sampling strategy. large college class 160 students. 160 students attend lectures together, students divided 4 groups, 40 students, lab sections administered different teaching assistants. professor wants conduct survey satisfied students course, believes lab section student might affect student’s overall satisfaction course.type study ?Suggest sampling strategy carrying study.Flawed reasoning. Identify flaw reasoning following scenarios. Explain individuals study done differently wanted make strong conclusions.Students elementary school given questionnaire required return parents completed . One questions asked , find work schedule makes difficult spend time kids school? parents replied, 85% said . Based results, school officials conclude great majority parents difficulty spending time kids school.survey conducted simple random sample 1,000 women recently gave birth, asking whether smoked pregnancy. follow-survey asking children respiratory problems conducted 3 years later, however, 567 women reached address. researcher reports 567 women representative mothers.Sampling strategies. statistics student curious relationship amount time students spend social networking sites performance school decides conduct survey. Four research strategies collecting data described . , name sampling method proposed bias might expect.randomly samples 40 students study’s population, gives survey, asks fill bring back next day.gives survey friends, makes sure one fills survey.posts link online survey Facebook wall asks friends fill survey.stands outside QRC asks every third person walks door fill survey.Vitamin supplements. order assess effectiveness taking large doses vitamin C reducing duration common cold, researchers recruited 400 healthy volunteers staff students university. quarter patients assigned placebo, rest evenly divided 1g Vitamin C, 3g Vitamin C, 3g Vitamin C plus additives taken onset cold following two days. tablets identical appearance packaging. nurses handed prescribed pills patients knew patient received treatment, researchers assessing patients sick . significant differences observed measure cold duration severity four medication groups, placebo group shortest duration symptoms.experiment observational study? ?explanatory response variables study?patients blinded treatment?study double-blind?Participants ultimately able choose whether use pills prescribed . might expect adhere take pills. introduce confounding variable study? Explain reasoning.Exercise mental health. researcher interested effects exercise mental health proposes following study: Use stratified random sampling ensure representative proportions 18-30, 31-40 41-55 year olds population. Next, randomly assign half subjects age group exercise twice week, instruct rest exercise. Conduct mental health exam beginning end study, compare results.type study ?treatment control groups study?study make use blocking? , blocking variable?study make use blinding?Comment whether results study can used establish causal relationship exercise mental health, indicate whether conclusions can generalized population large.Suppose given task determining proposed study get funding. reservations study proposal?","code":""},{"path":"NUMDATA.html","id":"NUMDATA","chapter":"5 Numerical Data","heading":"5 Numerical Data","text":"","code":""},{"path":"NUMDATA.html","id":"objectives-4","chapter":"5 Numerical Data","heading":"5.1 Objectives","text":"Define use properly context new terminology.Generate R summary statistics numeric variable including breaking cases.Generate R appropriate graphical summaries numerical variables.able interpret explain output graphically numerically.","code":""},{"path":"NUMDATA.html","id":"numerical-data","chapter":"5 Numerical Data","heading":"5.2 Numerical Data","text":"lesson introduces techniques exploring summarizing numerical variables, email50 mlb data sets openintro package subset county_complete usdata provide rich opportunities examples. Recall outcomes numerical variables numbers reasonable perform basic arithmetic operations. example, pop2010 variable, represents populations counties 2010, numerical since can sensibly discuss difference ratio populations two counties. hand, area codes zip codes numerical.","code":""},{"path":"NUMDATA.html","id":"scatterplots-for-paired-data","chapter":"5 Numerical Data","heading":"5.2.1 Scatterplots for paired data","text":"scatterplot provides case--case view data two numerical variables. Figure 5.1, present scatterplot used examine federal spending poverty related county data set.\nFigure 5.1: scatterplot showing fed_spend poverty. Owsley County Kentucky, poverty rate 41.5% federal spending $21.50 per capita, highlighted.\nAnother scatterplot shown Figure 5.2, comparing number line breaks line_breaks number characters num_char emails email50 data set. scatterplot, point represents single case. Since 50 cases email50, 50 points Figure 5.2.\nFigure 5.2: scatterplot line_breaks versus num_char email50 data.\nput number characters perspective, paragraph 357 characters. Looking Figure 5.2, seems emails incredibly long! Upon investigation, actually find long emails use HTML format, means characters emails used format email rather provide text.Exercise:\nscatterplots reveal data, might useful?35Example:\nConsider new data set 54 cars two variables: vehicle price weight.36 scatterplot vehicle price versus weight shown Figure 5.3. can said relationship variables?\nFigure 5.3: scatterplot price versus weight 54 cars.\nrelationship evidently nonlinear, highlighted dashed line. different previous scatterplots ’ve seen show relationships linear.Exercise:\nDescribe two variables horseshoe shaped association scatterplot.37","code":""},{"path":"NUMDATA.html","id":"dot-plots-and-the-mean","chapter":"5 Numerical Data","heading":"5.2.2 Dot plots and the mean","text":"Sometimes two variables one many: one variable may interest. cases, dot plot provides basic displays. dot plot one-variable scatterplot; example using number characters 50 emails shown Figure 5.4.\nFigure 5.4: dot plot num_char email50 data set.\nmean, sometimes called average, common way measure center distribution data. find mean number characters 50 emails, add character counts divide number emails. computational convenience, number characters listed thousands rounded first decimal.\\[\\bar{x} = \\frac{21.7 + 7.0 + \\cdots + 15.8}{50} = 11.6\\]sample mean often labeled \\(\\bar{x}\\), bar letter, letter \\(x\\) used generic placeholder variable interest, num_char.Mean\nsample mean numerical variable sum observations divided number observations, Equation (5.1).\\[\\begin{equation}\n  \\bar{x} = \\frac{x_1+x_2+\\cdots+x_n}{n}\n  \\tag{5.1}\n\\end{equation}\\]\\(x_1, x_2, \\dots, x_n\\) represent \\(n\\) observed values.Exercise:\nExamine two equations . \\(x_1\\) correspond ? \\(x_2\\)? Can infer general meaning \\(x_i\\) might represent?38Exercise:\n\\(n\\) sample emails?39The email50 data set sample larger population emails received January March. compute mean population way sample mean. However, difference notation: population mean special label: \\(\\mu\\). symbol \\(\\mu\\) Greek letter mu represents average observations population. Sometimes subscript, \\(_x\\), used represent variable population mean refers , e.g. \\(\\mu_x\\).Example:\naverage number characters across emails can estimated using sample data. Based sample 50 emails, reasonable estimate \\(\\mu_x\\), mean number characters emails email data set? (Recall email50 sample email.)sample mean, 11.6, may provide reasonable estimate \\(\\mu_x\\). number perfect, provides point estimate population mean. Later text, develop tools characterize accuracy point estimates, find point estimates based larger samples tend accurate based smaller samples.Example:\nmight like compute average income per person US. , might first think take mean per capita incomes 3,143 counties county data set. better approach?county data set special county actually represents many individual people. simply average across income variable, treating counties 5,000 5,000,000 residents equally calculations. Instead, compute total income county, add counties’ totals, divide number people counties. completed steps county data, find per capita income US $27,348.43. computed simple mean per capita income across counties, result just $22,504.70!previous example used called weighted mean, key topic probability section. look ahead, probability mass function gives population proportions value thus find population mean \\(\\mu\\), use weighted mean.","code":""},{"path":"NUMDATA.html","id":"histograms-and-shape","chapter":"5 Numerical Data","heading":"5.2.3 Histograms and shape","text":"Dot plots show exact value observation. useful small data sets, can become hard read larger samples. Rather showing value observation, think value belonging bin. example, email50 data set, create table counts number cases character counts 0 5,000, number cases 5,000 10,000, . Observations fall boundary bin (e.g. 5,000) allocated lower bin. tabulation shown .binned counts plotted bars Figure 5.5 called histogram.\nFigure 5.5: histogram num_char. distribution strongly skewed right.\nHistograms provide view data density. Higher bars represent data relatively dense. instance, many emails 0 10,000 characters emails 10,000 20,000 characters data set. bars make easy see density data changes relative number characters.Histograms especially convenient describing shape data distribution. Figure 5.5 shows emails relatively small number characters, fewer emails large number characters. data trail right way longer right tail, shape said right skewed.40Data sets reverse characteristic – long, thin tail left – said left skewed. also say distribution long left tail. Data sets show roughly equal trailing directions called symmetric.Long tails identify skew\ndata trail one direction, distribution long tail. distribution long left tail, left skewed. distribution long right tail, right skewed.Exercise:\nTake look dot plot , Figure 5.4. Can see skew data? easier see skew histogram dot plots?41Exercise:\nBesides mean, can see dot plot see histogram?42","code":"## \n##   (0,5]  (5,10] (10,15] (15,20] (20,25] (25,30] (30,35] (35,40] (40,45] (45,50] \n##      19      12       6       2       3       5       0       0       2       0 \n## (50,55] (55,60] (60,65] \n##       0       0       1"},{"path":"NUMDATA.html","id":"making-our-own-histogram","chapter":"5 Numerical Data","heading":"5.2.3.1 Making our own histogram","text":"Let’s take time make simple histogram. use ggformula package wrapper ggplot package.two questions:want R ? andWhat must give R ?want R make histogram. ggformula plots form gf_XXXX use gf_histogram. find options information type:start just give formulas data R.Exercise:\nLook help menu gf_histogram change x-axis label, change bin width 5, left bin start 0.code exerciseIn addition looking whether distribution skewed symmetric, histograms can used identify modes. mode represented prominent peak distribution.43 one prominent peak histogram num_char.Figure 5.6 show histograms one, two, three prominent peaks. distributions called unimodal, bimodal, multimodal, respectively. distribution 2 prominent peaks called multimodal. Notice one prominent peak unimodal distribution second less prominent peak counted since differs neighboring bins observations.\nFigure 5.6: Histograms demonstrate unimodal, bimodal, multimodal data.\nExercise:\nHeight measurements young students adult teachers K-3 elementary school taken. many modes anticipate height data set?44Looking modes\nLooking modes isn’t finding clear correct answer number modes distribution, prominent rigorously defined notes. important part examination better understand data might structured.","code":"?gf_histogram\ngf_histogram(~num_char,data=email50,color=\"black\",fill=\"cyan\")email50 %>%\n   gf_histogram(~num_char,binwidth = 5,boundary=0,\n   xlab=\"The Number of Characters (in thousands)\"\n   ,color=\"black\",fill=\"cyan\") %>%\n   gf_theme(theme_classic())"},{"path":"NUMDATA.html","id":"variance-and-standard-deviation","chapter":"5 Numerical Data","heading":"5.2.4 Variance and standard deviation","text":"mean used describe center data set, variability data also important. , introduce two measures variability: variance standard deviation. useful data analysis, even though formulas bit tedious calculate hand. standard deviation easier two conceptually understand, roughly describes far away typical observation mean. Equation (5.2) equation sample variance. demonstrate data notation easier understand.\\[\\begin{equation}\n  s^2 = \\sum_{=1}^{n}\\frac{(x_i-\\bar{x})^2}{n-1}=\\frac{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2 + (x_3-\\bar{x})^2 + \\cdots + (x_n-\\bar{x})^2}{n-1}\n  \\tag{5.2}\n\\end{equation}\\]\\(x_1, x_2, \\dots, x_n\\) represent \\(n\\) observed values.call distance observation mean deviation. deviations \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), \\(50^{th}\\) observations num_char variable. computational convenience, number characters listed thousands rounded first decimal.\\[\n\\begin{aligned}\nx_1^{}-\\bar{x} &= 21.7 - 11.6 = 10.1 \\hspace{5mm}\\text{ } \\\\\nx_2^{}-\\bar{x} &= 7.0 - 11.6 = -4.6 \\\\\nx_3^{}-\\bar{x} &= 0.6 - 11.6 = -11.0 \\\\\n            &\\ \\vdots \\\\\nx_{50}^{}-\\bar{x} &= 15.8 - 11.6 = 4.2\n\\end{aligned}\n\\]square deviations take average, result equal sample variance, denoted \\(s_{}^2\\):\n\\[\n\\begin{aligned}\ns_{}^2 &= \\frac{10.1_{}^2 + (-4.6)_{}^2 + (-11.0)_{}^2 + \\cdots + 4.2_{}^2}{50-1} \\\\\n    &= \\frac{102.01 + 21.16 + 121.00 + \\cdots + 17.64}{49} \\\\\n    &= 172.44\n\\end{aligned}\n\\]divide \\(n-1\\), rather dividing \\(n\\), computing variance; need worry mathematical nuance yet. Notice squaring deviations two things. First, makes large values much larger, seen comparing \\(10.1^2\\), \\((-4.6)^2\\), \\((-11.0)^2\\), \\(4.2^2\\). Second, gets rid negative signs.sample standard deviation \\(s\\) square root variance:\n\\[s=\\sqrt{172.44} = 13.13\\]\nsample standard deviation number characters email 13.13 thousand. subscript \\(_x\\) may added variance standard deviation, .e. \\(s_x^2\\) \\(s_x^{}\\), reminder variance standard deviation observations represented \\(x_1^{}\\), \\(x_2^{}\\), …, \\(x_n^{}\\). \\(_{x}\\) subscript usually omitted clear data variance standard deviation referencing.Variance standard deviation\nvariance roughly average squared distance mean. standard deviation square root variance describes close data mean.Formulas methods used compute variance standard deviation population similar used sample.45 However, like mean, population values special symbols: \\(\\sigma_{}^2\\) variance \\(\\sigma\\) standard deviation. symbol \\(\\sigma\\) Greek letter sigma.Tip: standard deviation describes variability\nFocus conceptual meaning standard deviation descriptor variability rather formulas. Usually 70% data within one standard deviation mean 95% within two standard deviations. However, seen, percentages strict rules.\nFigure 5.7: first three different population distributions mean, 0, standard deviation, 1.\n\nFigure 5.8: second plot mean 0 standard deviation 1.\n\nFigure 5.9: final plot mean 0 standard deviation 1.\nExercise:\nEarlier concept shape distribution introduced. good description shape distribution include modality whether distribution symmetric skewed one side. Using three figures, Figures 5.7, 5.8, 5.9 example, explain description important.46Example:\nDescribe distribution num_char variable using histogram Figure 5.5. description incorporate center, variability, shape distribution, also placed context: number characters emails. Also note especially unusual cases.47In practice, variance standard deviation sometimes used means end, end able accurately estimate uncertainty associated sample statistic. example, later course use variance standard deviation assess close sample mean population mean.","code":""},{"path":"NUMDATA.html","id":"box-plots-quartiles-and-the-median","chapter":"5 Numerical Data","heading":"5.2.5 Box plots, quartiles, and the median","text":"box plot summarizes data set using five statistics also plotting unusual observations. Figure 5.10 provides vertical dot plot alongside box plot num_char variable email50 data set.\nFigure 5.10: vertical dot plot next labeled box plot number characters 50 emails. median (6,890), splits data bottom 50% top 50%, marked dot plot horizontal dashes open circles, respectively.\nfirst step building box plot drawing dark line denoting median, splits data half. Figure 5.10 shows 50% data falling median (red dashes) 50% falling median (blue open circles). 50 character counts data set (even number) data perfectly split two groups 25. take median case average two observations closest \\(50^{th}\\) percentile: \\((\\text{6,768} + \\text{7,012}) / 2 = \\text{6,890}\\). odd number observations, exactly one observation splits data two halves, case observation median (average needed).Median: number middle\ndata ordered smallest largest, median observation right middle. even number observations, two values middle, median taken average.second step building box plot drawing rectangle represent middle 50% data. total length box, shown vertically Figure 5.10, called interquartile range (IQR, short). , like standard deviation, measure variability data. variable data, larger standard deviation IQR. two boundaries box called first quartile (\\(25^{th}\\) percentile, .e. 25% data fall value) third quartile (\\(75^{th}\\) percentile), often labeled \\(Q_1\\) \\(Q_3\\), respectively.Interquartile range (IQR)\nIQR length box box plot. computed \n\\[ IQR = Q_3 - Q_1 \\]\n\\(Q_1\\) \\(Q_3\\) \\(25^{th}\\) \\(75^{th}\\) percentiles.Exercise:\npercent data fall \\(Q_1\\) median? percent median \\(Q_3\\)?48Extending box, whiskers attempt capture data outside box, however, reach never allowed \\(1.5\\times IQR\\).49 capture everything within reach. Figure 5.10, upper whisker extend last three points, beyond \\(Q_3 + 1.5\\times IQR\\), extends last point limit. lower whisker stops lowest value, 33, since additional data reach; lower whisker’s limit shown figure plot extend \\(Q_1 - 1.5\\times IQR\\). sense, box like body box plot whiskers like arms trying reach rest data.observation lies beyond whiskers labeled dot. purpose labeling points – instead just extending whiskers minimum maximum observed values – help identify observations appear unusually distant rest data. Unusually distant observations called outliers. case, reasonable classify emails character counts 41,623, 42,793, 64,401 outliers since numerically distant data.Outliers extreme\noutlier observation extreme relative rest data.important look outliers\nExamination data possible outliers serves many useful purposes, including\n1. Identifying strong skew distribution.\n2. Identifying data collection entry errors. instance, re-examined email purported 64,401 characters ensure value accurate.\n3. Providing insight interesting properties data.Exercise:\nobservation value 64,401, outlier, found accurate observation. observation suggest nature character counts emails?50Exercise:\nUsing Figure 5.10, estimate following values num_char email50 data set:\n() \\(Q_1\\),\n(b) \\(Q_3\\), \n(c) IQR.51Of course R can calculate summary statistics us. First calculations individually one function call. Remember ask want R needs.","code":"\nmean(~num_char,data=email50)## [1] 11.59822\nsd(~num_char,data=email50)## [1] 13.12526\nquantile(~num_char,data=email50)##       0%      25%      50%      75%     100% \n##  0.05700  2.53550  6.88950 15.41075 64.40100\niqr(~num_char,data=email50)## [1] 12.87525\nfavstats(~num_char,data=email50)##    min     Q1 median       Q3    max     mean       sd  n missing\n##  0.057 2.5355 6.8895 15.41075 64.401 11.59822 13.12526 50       0"},{"path":"NUMDATA.html","id":"robust-statistics","chapter":"5 Numerical Data","heading":"5.2.6 Robust statistics","text":"sample statistics num_char data set affected observation value 64,401? happened email wasn’t observed? happen summary statistics observation 64,401 even larger, say 150,000? scenarios plotted alongside original data Figure 5.11, sample statistics computed R.\nFigure 5.11: Box plots original character count data two modified data sets.\ncode used generate table isNotice using formula notation, able calculate summary statistics group.Exercise:\n() affected extreme observations, mean median? data summary may helpful.\n(b) standard deviation IQR affected extreme observations?52The median IQR called robust estimates extreme observations little effect values. mean standard deviation much affected changes extreme observations.Example:\nmedian IQR change much three scenarios . might case?53Exercise:\ndistribution vehicle prices tends right skewed, luxury sports cars lingering right tail. searching new car cared price, interested mean median price vehicles sold, assuming market regular car?54","code":"##       group   min     Q1 median       Q3     max     mean       sd  n missing\n## 1   Dropped 0.057 2.4540 6.7680 14.15600  42.793 10.52061 10.79768 49       0\n## 2 Increased 0.057 2.5355 6.8895 15.41075 150.000 13.31020 22.43436 50       0\n## 3  Original 0.057 2.5355 6.8895 15.41075  64.401 11.59822 13.12526 50       0\np1 <- email50$num_char\np2 <- p1[-which.max(p1)]\np3 <- p1\np3[which.max(p1)] <- 150\n\nrobust <- data.frame(value= c(p1,p2,p3),group=c(rep(\"Original\",50),rep(\"Dropped\",49),rep(\"Increased\",50)))\n\nfavstats(value~group,data=robust)"},{"path":"NUMDATA.html","id":"transforming-data","chapter":"5 Numerical Data","heading":"5.2.7 Transforming data","text":"data strongly skewed, sometimes transform easier model. Consider histogram salaries Major League Baseball players’ salaries 2010, shown Figure 5.12.\nFigure 5.12: Histogram MLB player salaries 2010, millions dollars.\nExample:\nhistogram MLB player salaries useful can see data extremely skewed centered (gauged median) $1 million. isn’t useful plot?55There standard transformations often applied much data cluster near zero (relative larger values data set) observations positive. transformation rescaling data using function. instance, plot natural logarithm56 player salaries results new histogram Figure 5.13. Transformed data sometimes easier work applying statistical models transformed data much less skewed outliers usually less extreme.\nFigure 5.13: Histogram log-transformed MLB player salaries 2010.\nTransformations can also applied one variables scatterplot. scatterplot line_breaks num_char variables shown Figure 5.2 . can see positive association variables many observations clustered near zero. Later text, might want use straight line model data. However, ’ll find data current state modeled well. Figure 5.14 shows scatterplot line_breaks num_char variables transformed using log (base \\(e\\)) transformation. positive association plot, transformed data show steadier trend, easier model untransformed data.\nFigure 5.14: scatterplot line_breaks versus num_char email50 data variable log-transformed.\nTransformations logarithm can useful, . instance, square root (\\(\\sqrt{\\text{original observation}}\\)) inverse (\\(\\frac{1}{\\text{original observation}}\\)) used statisticians. Common goals transforming data see data structure differently, reduce skew, assist modeling, straighten nonlinear relationship scatterplot.","code":""},{"path":"NUMDATA.html","id":"homework-problems-4","chapter":"5 Numerical Data","heading":"5.3 Homework Problems","text":"Create Rmd file work including headers, file creation data, explanation work. Make sure plots title axes labeled.Mammals exploratoryData collected 39 species mammals distributed 13 orders. data openintro package mammalsUsing help, report units variable brain_Wt.Using inspect many variables numeric?type variable danger?Create histogram total_sleep describe distribution.Create boxplot life_span describe distribution.Report mean median life span mammal.Calculate summary statistics life_span broken danger. standard deviation life span danger outcome 5?Mammals life spansContinue using mammals data set.Create side--side boxplots life_span broken exposure. Note: change exposure factor(). Report findings.happened median third quartile exposure group 4?Create faceted histograms. shortcomings plot?Create new variable exposed factor level Low exposure 1 2 High otherwise.Repeat part c new variable. Explain see plot.Mammals life spans continuedCreate scatterplot life span versus length gestation.type association apparent life span length gestation?type association expect see axes plot reversed, .e. plotted length gestation versus life span?Create new scatterplot suggested c.life span length gestation independent? Explain reasoning.","code":""},{"path":"CATDATA.html","id":"CATDATA","chapter":"6 Categorical Data","heading":"6 Categorical Data","text":"","code":""},{"path":"CATDATA.html","id":"objectives-5","chapter":"6 Categorical Data","heading":"6.1 Objectives","text":"Define use properly context new terminology.Generate R tables categorical variable(s).Generate R appropriate graphical summaries categorical numerical variables.able interpret explain output graphically numerically.","code":""},{"path":"CATDATA.html","id":"categorical-data","chapter":"6 Categorical Data","heading":"6.2 Categorical data","text":"Like numerical data, categorical data can also organized analyzed. section introduces tables basic tools categorical data. Remember beginning block material, case study categorical data seen ideas lesson.email50 data set represents sample larger email data set called email. larger data set contains information 3,921 emails. section use email data set examine whether presence numbers, small large, email provides useful value classifying email spam spam.","code":""},{"path":"CATDATA.html","id":"contingency-tables-and-bar-plots","chapter":"6 Categorical Data","heading":"6.2.1 Contingency tables and bar plots","text":"email data set two variables: spam number want summarize. Let’s use inspect() get information insight two variables. can also type ?email learn data. First load openintro library.Notice use pipe operator adds ease reading code. select() function allows us narrow variables two interest. inspect() gives us information variables. read top line; start data set email, input select() select variables , use inspect() summarize variables.indicated number categorical variable describes whether email contains numbers, small numbers (values 1 million), least one big number (value 1 million ). variable spam numeric variable 1 indicates email spam. treat categorical want change factor first build table summarizes data two variables, see Table 6.1. table called contingency table. value table represents number times particular combination variable outcomes occurred. show code generate contingency table.\nTable 6.1: contingency table email data.\nvalue 149 corresponds number emails data set spam number listed email. Row column totals also included. row totals provide total counts across row (e.g. \\(149 + 168 + 50 = 367\\)), column totals total counts column. row column totals known marginal counts values table, 149, joint counts.Let’s turn spam factor update email data object. use mutate() .Now checking data .Let’s generate table .table single variable called frequency table. table frequency table number variable.replaced counts percentages proportions, table called relative frequency table.bar plot common way display single categorical variable. Figure 6.1 shows bar plot number variable.\nFigure 6.1: Bar chart number variable.\nNext counts converted proportions (e.g. \\(549/3921=0.140\\) none) Figure 6.2.\nFigure 6.2: Bar chart number variable proportion.\n, let’s clean plot style use report.","code":"\nlibrary(openintro)\nemail %>%\n  select(spam,number) %>%\n  inspect()## \n## categorical variables:  \n##     name  class levels    n missing\n## 1   spam factor      2 3921       0\n## 2 number factor      3 3921       0\n##                                    distribution\n## 1 0 (90.6%), 1 (9.4%)                          \n## 2 small (72.1%), none (14%) ...\ntally(~spam+number,data=email,margins = TRUE)##        number\n## spam    none small  big Total\n##   0      400  2659  495  3554\n##   1      149   168   50   367\n##   Total  549  2827  545  3921\nemail <- email %>%\n  mutate(spam = factor(email$spam,levels=c(1,0),labels=c(\"spam\",\"not spam\")))\nemail %>%\n  select(spam,number) %>%\n  inspect()## \n## categorical variables:  \n##     name  class levels    n missing\n## 1   spam factor      2 3921       0\n## 2 number factor      3 3921       0\n##                                    distribution\n## 1 not spam (90.6%), spam (9.4%)                \n## 2 small (72.1%), none (14%) ...\ntally(~spam+number,data=email,margins = TRUE)##           number\n## spam       none small  big Total\n##   spam      149   168   50   367\n##   not spam  400  2659  495  3554\n##   Total     549  2827  545  3921\ntally(~number,data=email)## number\n##  none small   big \n##   549  2827   545\ntally(~number,data=email,format='proportion')## number\n##      none     small       big \n## 0.1400153 0.7209895 0.1389952\nround(tally(~number,data=email,format='percent'),2)## number\n##  none small   big \n##  14.0  72.1  13.9\nemail %>%\n  gf_bar(~number) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Size of Number\",y=\"Count\")\nemail %>%\n  gf_props(~number) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Size of Number\",y=\"Proportion\")\nemail %>%\n  gf_props(~number,title=\"The proportions of emails with a number in it\",\n           subtitle=\"From 2012\",xlab=\"Type of number in the email\",\n           ylab=\"Proportion of emails\") %>%\n  gf_theme(theme_bw())"},{"path":"CATDATA.html","id":"column-proportions","chapter":"6 Categorical Data","heading":"6.2.2 Column proportions","text":"table shows column proportions. column proportions computed counts divided column totals. value 149 intersection spam none replaced \\(149/549=0.271\\), .e. 149 divided column total, 549. 0.271 represent? corresponds proportion emails sample numbers spam. conditioning, restricting, emails number. rate spam much higher emails small numbers (5.9%) big numbers (9.2%). spam rates vary three levels number (none, small, big), provides evidence spam number variables associated.tally() function always condition variable right hand side tilde, ~, calculating proportions thus generate column proportions. general table() function R allow either column row proportions.Exercise:\nCreate table column proportions variable spam column variable.Exercise:\ntable just created, 0.748 represent?57Example:\nData scientists use statistics filter spam incoming email messages. noting specific characteristics email, data scientist may able classify emails spam spam high accuracy. One characteristics whether email contains numbers, small numbers, big numbers. Another characteristic whether email HTML content. contingency table spam format variables needed.\n1 Make format categorical factor variable.levels “text” “HTML”.58\n2 Create contingency table email data set format columns spam rows.deciding variable use column, data scientist interested proportion spam changes within email format. corresponds column proportions based format: proportion spam plain text emails proportion spam HTML emails.generating column proportions, can see higher fraction plain text emails spam (\\(209/1195 = 17.5\\%\\)) compared HTML emails (\\(158/2726 = 5.8\\%\\)). information insufficient classify email spam spam, 80% plain text emails spam. Yet, carefully combine information many characteristics, number variables, stand reasonable chance able classify email spam spam.constructing table, need think variable want column row. formula format way makes us think response predictor variables. However cases, clear variable column row analyst must decide point made table. settling one form table, important consider audience message receive table.Exercise:\nCreate two tables number spam column, two tables change variable column. useful someone hoping identify spam emails using number variable?59","code":"\ntally(spam~number,data=email,margins = TRUE,format='proportion')##           number\n## spam             none      small        big\n##   spam     0.27140255 0.05942695 0.09174312\n##   not spam 0.72859745 0.94057305 0.90825688\n##   Total    1.00000000 1.00000000 1.00000000\ntally(number~spam,data=email,margins = TRUE,format='proportion')##        spam\n## number       spam  not spam\n##   none  0.4059946 0.1125492\n##   small 0.4577657 0.7481711\n##   big   0.1362398 0.1392797\n##   Total 1.0000000 1.0000000\nemail <- email %>%\n  mutate(format = factor(email$format,levels=c(1,0),labels=c(\"HTML\",\"text\")))\ntally(spam~format,data=email,margins = TRUE,format=\"proportion\")##           format\n## spam             HTML       text\n##   spam     0.05796038 0.17489540\n##   not spam 0.94203962 0.82510460\n##   Total    1.00000000 1.00000000\ntally(spam~number,email,format='proportion',margin=TRUE)##           number\n## spam             none      small        big\n##   spam     0.27140255 0.05942695 0.09174312\n##   not spam 0.72859745 0.94057305 0.90825688\n##   Total    1.00000000 1.00000000 1.00000000\ntally(number~spam,email,format='proportion',margin=TRUE)##        spam\n## number       spam  not spam\n##   none  0.4059946 0.1125492\n##   small 0.4577657 0.7481711\n##   big   0.1362398 0.1392797\n##   Total 1.0000000 1.0000000"},{"path":"CATDATA.html","id":"segmented-bar-and-mosaic-plots","chapter":"6 Categorical Data","heading":"6.2.3 Segmented bar and mosaic plots","text":"Contingency tables using column proportions especially useful examining two categorical variables related. Segmented bar mosaic plots provide way visualize information tables.segmented bar plot graphical display contingency table information. example, segmented bar plot representing table number column shown Figure 6.3, first created bar plot using number variable separated group levels spam.\nFigure 6.3: Segmented bar plot numbers found emails, counts broken spam.\ncolumn proportions table translated standardized segmented bar plot Figure 6.4, helpful visualization fraction spam emails level number.\nFigure 6.4: Standardized version Figure 6.3.\nExample:\nExamine segmented bar plots. useful?Figure 6.3 contains information, Figure 6.4 presents information clearly. second plot makes clear emails number relatively high rate spam email – 27%! hand, less 10% email small big numbers spam.Since proportion spam changes across groups Figure 6.4, can conclude variables dependent, something also able discern using table proportions. none big groups relatively observations compared small group, association difficult see Figure 6.3.cases, segmented bar plot standardized useful communicating important information. settling particular segmented bar plot, create standardized non-standardized forms decide effective communicating features data.mosaic plot graphical display contingency table information similar bar plot one variable segmented bar plot using two variables. seems strange, mosaic plots part mosaic package. must load another set packages called vcd vcdExtra. Mosaic plot displays help visualize pattern associations among variables two-way larger tables. Mosaic plots controversial since rely perception area. Human vision good distinguishing areas.introduce mosaic plots another way visualize contingency tables. Figure 6.5 shows mosaic plot number variable. row represents level number, row heights correspond proportion emails number type. instance, fewer emails numbers emails small numbers, none outcome row shorter height. general, mosaic plots use box areas represent number observations. Since one variable, widths constant. Thus area simply related row height making visual easy read.\nFigure 6.5: Mosaic plot emails grouped number variable.\none-variable mosaic plot can divided pieces Figure 6.6 using spam variable. first variable formula used determine row height. , row split proportionally according fraction emails number category, heights similar Figure 6.5. Next row split horizontally according proportion emails spam number group. example, second row, representing emails small numbers, divided emails spam (left) spam (right). area rectangles proportional proportions table cell count divided total count. First generate table represent mosaic plot.\nFigure 6.6: Mosaic plot number first variable.\nplots hard use visual comparison area. example, area small number spam emails different none number spam emails? rectangles different shapes table can tell areas close.important use mosaic plot determine association variables may present. bottom first column represents spam emails big numbers, bottom row second column represents regular emails big numbers. can use plot see spam number variables associated since rows divided different vertical locations others, technique used checking association standardized version segmented bar plot.similar way, mosaic plot representing column proportions spam column constructed. completely understand mosaic plot shown Figure 6.7 let’s first find proportions spam.row heights split 90-10. Next let’s find proportions number within value spam. spam row, none 41%, small 46%, big 13%.However, insightful application consider fraction spam category number variable, prefer Figure 6.6.\nFigure 6.7: Mosaic plot spam first variable\n","code":"\nemail %>%\n  gf_bar(~number,fill=~spam) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Size of Number\",y=\"Count\")\nemail %>%\n  gf_props(~number,fill=~spam,position='fill') %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Size of Number\",y=\"Proportion\")\nlibrary(vcd)\nmosaic(~number,data=email)\ntally(~number+spam,data=email,format='proportion')##        spam\n## number        spam   not spam\n##   none  0.03800051 0.10201479\n##   small 0.04284621 0.67814333\n##   big   0.01275185 0.12624331\nmosaic(~number+spam,data=email)\ntally(~spam,data=email,format=\"proportion\")## spam\n##       spam   not spam \n## 0.09359857 0.90640143\ntally(number~spam,data=email,margins = TRUE,format=\"proportion\")##        spam\n## number       spam  not spam\n##   none  0.4059946 0.1125492\n##   small 0.4577657 0.7481711\n##   big   0.1362398 0.1392797\n##   Total 1.0000000 1.0000000\nmosaic(~spam+number,data=email)"},{"path":"CATDATA.html","id":"the-only-pie-chart-you-will-see-in-this-course-hopefully","chapter":"6 Categorical Data","heading":"6.2.4 The only pie chart you will see in this course, hopefully","text":"pie charts well known, typically useful charts data analysis. pie chart shown Figure 6.8. generally difficult compare group sizes pie chart bar plot, especially categories nearly identical counts proportions. case none big categories, difference slight may unable distinguish difference group sizes.\nFigure 6.8: pie chart number email data set.\nPie charts popular Air Force due ease generating Excel PowerPoint. However, values slice often printed top chart making chart irrelevant. recommend minimum use pie charts work.","code":"\npie(table(email$number), col=COL[c(3,1,2)], radius=0.75)"},{"path":"CATDATA.html","id":"comparing-numerical-data-across-groups","chapter":"6 Categorical Data","heading":"6.2.5 Comparing numerical data across groups","text":"interesting investigations can considered examining numerical data across groups. case one variable categorical numerical. methods required aren’t really new. required make numerical plot group. two convenient methods introduced: side--side box plots density plots.take look subset county_complete data set compare median household income counties gained population 2000 2010 versus counties gain. might like make causal connection , remember observational data interpretation unjustified.section give us chance perform data wrangling. using tidyverse verbs process. Data wrangling important part analysis work typically takes significant portion analysis work.code generate data need.First, reminder, let’s look data.want R ? want select variables pop2000, pop2010, med_income.R need? needs data object, variable names.use select() inspect() functions.Notice three counties missing population values, reported NA. Let’s remove find counties increased population creating new variable.2,041 counties population increased 2000 2010, 1,098 counties gain, 1 county net zero, loss. Let’s just look counties gain loss side--side boxplot. , use filter() select two groups make variable pop_gain categorical variable, data wrangling.side--side box plot traditional tool comparing across groups. example shown Figure 6.9 two box plots, one group drawn scale.\nFigure 6.9: Side--side box plot median household income, counties split whether population gain loss 2000 2010.\nAnother useful plotting method uses density plots compare numerical data across groups. histogram bins data highly dependent number boundary bins. density plot also estimates distribution numerical variable estimating density data points small window around data point. overall curve sum small density estimate. density plot can thought smooth version histogram. Several options go density estimate width window type smoothing function. ideas beyond point just use default options. Figure 6.10 plot two density curves.\nFigure 6.10: Density plots median household income counties population gain versus population loss\nExercise:\nUse box plots density plots compare incomes counties across two groups. notice approximate center group? notice variability groups? shape relatively consistent groups? many prominent modes group?60Exercise:\ncomponents plot Figures 8 9 find useful?61","code":"\nlibrary(usdata)\ncounty_tidy <- county_complete %>% \n  select(name, state, pop2000, pop2010, fed_spend=fed_spending_2009, poverty=poverty_2010, \n         homeownership = homeownership_2010, multi_unit = housing_multi_unit_2010, \n         income = per_capita_income_2010, med_income = median_household_income_2010) %>%\n  mutate(fed_spend=fed_spend/pop2010)\ncounty_tidy %>%\n  select(pop2000,pop2010,med_income) %>%\n  inspect()## \n## quantitative variables:  \n##            name   class   min       Q1 median    Q3     max     mean        sd\n## ...1    pop2000 numeric    67 11223.50  24621 61775 9519338 89649.99 292547.67\n## ...2    pop2010 numeric    82 11114.50  25872 66780 9818605 98262.04 312946.70\n## ...3 med_income numeric 19351 36956.25  42450 49144  115574 44274.12  11547.49\n##         n missing\n## ...1 3139       3\n## ...2 3142       0\n## ...3 3142       0\ncc_reduced <- county_tidy %>%\n  drop_na(pop2000) %>%\n  select(pop2000,pop2010,med_income) %>%\n  mutate(pop_gain = sign(pop2010-pop2000))\ntally(~pop_gain,data=cc_reduced)## pop_gain\n##   -1    0    1 \n## 1097    1 2041\ncc_reduced <- cc_reduced %>%\n  filter(pop_gain != 0) %>%\n  mutate(pop_gain = factor(pop_gain,levels=c(-1,1),labels=c(\"Loss\",\"Gain\")))\ninspect(cc_reduced)## \n## categorical variables:  \n##       name  class levels    n missing\n## 1 pop_gain factor      2 3138       0\n##                                    distribution\n## 1 Gain (65%), Loss (35%)                       \n## \n## quantitative variables:  \n##            name   class   min       Q1  median      Q3     max     mean\n## ...1    pop2000 numeric    67 11217.25 24608.0 61783.5 9519338 89669.37\n## ...2    pop2010 numeric    82 11127.00 25872.0 66972.0 9818605 98359.23\n## ...3 med_income numeric 19351 36950.00 42443.5 49120.0  115574 44253.24\n##             sd    n missing\n## ...1 292592.28 3138       0\n## ...2 313133.28 3138       0\n## ...3  11528.95 3138       0\ncc_reduced %>%\n  gf_boxplot(med_income~pop_gain,\n             subtitle=\"The income data were collected between 2006 and 2010.\",\n             xlab=\"Population change from 2000 to 2010\",\n             ylab=\"Median Household Income\") %>%\n  gf_theme(theme_bw())\ncc_reduced %>%\n  gf_dens(~med_income,color=~pop_gain,lwd=1) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Median household income\",y=\"\",col=\"Population \\nChange\")"},{"path":"CATDATA.html","id":"homework-problems-5","chapter":"6 Categorical Data","heading":"6.3 Homework Problems","text":"Create Rmd file work including headers, file creation data, explanation work. Make sure plots title axes labeled.Views immigration910 randomly sampled registered voters Tampa, FL asked thought workers illegally entered US () allowed keep jobs apply US citizenship, (ii) allowed keep jobs temporary guest workers allowed apply US citizenship, (iii) lose jobs leave country.data openintro package immigration data object.many levels political ?Create table using tally.percent Tampa, FL voters identify conservatives?percent Tampa, FL voters favor citizenship option?percent Tampa, FL voters identify conservatives favor citizenship option?percent Tampa, FL voters identify conservatives also favor citizenship option? percent moderates liberal share view?Create stacked bar chart.Using plot, political ideology views immigration appear independent? Explain reasoning.Views DREAM Act survey Exercise 1 also asked respondents support DREAM Act, proposed law provide path citizenship people brought illegally US children.data openintro package dream data object.Create mosaic plot.Based mosaic plot, views DREAM Act political ideology independent?Heart transplantsThe Stanford University Heart Transplant Study conducted determine whether experimental heart transplant program increased lifespan. patient entering program designated official heart transplant candidate, meaning gravely ill likely benefit new heart. patients got transplant . variable transplant indicates group patients ; patients treatment group got transplant control group . Another variable called survived used indicate whether patient alive end study.data openintro package called heart_transplant.Create mosaic plot.Based mosaic plot, survival independent whether patient got transplant? Explain reasoning.Using variable survtime, create side--side boxplots control treatment groups.box plots suggest efficacy (effectiveness) transplants?","code":""},{"path":"CS2.html","id":"CS2","chapter":"7 Case Study","heading":"7 Case Study","text":"","code":""},{"path":"CS2.html","id":"objectives-6","chapter":"7 Case Study","heading":"7.1 Objectives","text":"Use R simulate probabilistic model.Use basic counting methods.","code":""},{"path":"CS2.html","id":"introduction-to-probability-models","chapter":"7 Case Study","heading":"7.2 Introduction to probability models","text":"second block material focus probability models. take two approaches, one mathematical computational. cases can use methods problem others computational approach feasible. mathematical approach probability modeling allows us insight problem ability understand process. Simulation much greater ability generalize can time intensive run often requires writing custom functions.case study extensive may seem overwhelming, worry discuss ideas many lessons coming block.","code":""},{"path":"CS2.html","id":"probability-models","chapter":"7 Case Study","heading":"7.3 Probability models","text":"Probability models important tool data analysts. used explain variation outcomes explained variables. use ideas Statistical Modeling Block help us make decisions statistical models.Often probability models used answer question form “chance …..?” means typically experiment trial multiple outcomes possible idea frequency outcomes. use frequency measure probability particular outcome.block focus just probability models. apply probability model need toSelect experiment possible outcomes.probability values outcomes may include parameters determine probabilities.Understand assumptions behind model.","code":""},{"path":"CS2.html","id":"case-study-1","chapter":"7 Case Study","heading":"7.4 Case study","text":"famous example probability question attack case study. question want answer “room \\(n\\) people chance least two people birthday?”Exercise:\ntypical classroom USAFA 18 students . think chance least two students birthday?62","code":""},{"path":"CS2.html","id":"break-down-the-question","chapter":"7 Case Study","heading":"7.4.1 Break down the question","text":"first action take understand asked.experiment trial?mean birthday?leap years?frequency births? days less likely others?Exercise:\nDiscuss questions others think relevant.63The best first step make simple model, often ones mathematical solution. problem means answer questions.room 18 people look birthdays. either two birthdays matching ; thus two outcomes.don’t care year, day month. Thus two people born May 16th match.ignore leap years.assume person equal probability born 365 days year.least two means multiple matches day several different days multiple people matching birthdays.","code":""},{"path":"CS2.html","id":"simulate-computational","chapter":"7 Case Study","heading":"7.4.2 Simulate (computational)","text":"Now idea structure problem, next need think simulate single classroom. 18 students classroom 365 days year birthday. need sample birthdays 18 students. code days year?easy solution just label days 1 365. function seq() us.Next need pick one days using sample function. Note set seed get repeatable results, required.first person born 228th day year.Since R works vectors, don’t write loop select 18 days, just sample() us.want R ? Sample numbers 1 365 replacement, means number can picked .Notice sample least one match, although difficult look list see match. Let’s sort make easier us see.next step find way R code detect match.Exercise:\nidea(s) can use determine match exists?sort data look differences sequential values check set differences contains zero. seems computationally expensive. Instead use function unique() gives vector unique values object. function length() gives number elements vector.Since 17 unique values vector size 18, match. Now let’s put together generate another classroom size 18.next problem needs solved repeat classrooms keep track match. several functions use include replicate() use () mosaic package returns data frame can use tidyverse verbs wrangle data.() function allows us repeat operation many times. following templatewhere {stuff } typically single R command, may something complicated.Load libraries.Let’s repeat larger number simulated classroom, remember asking :want R ?R need ?within 2 decimal places mathematical solution develop shortly.many classrooms need simulate get accurate estimate probability match? statistical modeling question depends much variability can accept. discuss ideas later semester. now, can run code multiple times see estimate varies. computational power cheap, can increase number simulations.","code":"\ndays <- seq(1,365)\nset.seed(2022)\nsample(days,1)## [1] 228\nclass <- sample(days,size=18,replace = TRUE)\nclass##  [1] 206 311 331 196 262 191 206 123 233 270 248   7 349 112   1 307 288 354\nsort(class)##  [1]   1   7 112 123 191 196 206 206 233 248 262 270 288 307 311 331 349 354\nlength(unique(class))## [1] 17\nlength(unique(sample(days,size=18,replace = TRUE)))## [1] 16do(n) * {stuff to do}              # pseudo-code\nlibrary(mosaic)\nlibrary(tidyverse)\ndo(5)*length(unique(sample(days,size=18,replace = TRUE)))##   length\n## 1     18\n## 2     17\n## 3     17\n## 4     17\n## 5     18\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %>%\n  mutate(match=if_else(length==18,0,1)) %>%\n  summarise(prob=mean(match))##   prob\n## 1 0.36\n(do(10000)*length(unique(sample(days,size=18,replace = TRUE)))) %>%\n  mutate(match=if_else(length==18,0,1)) %>%\n  summarise(prob=mean(match))##     prob\n## 1 0.3442"},{"path":"CS2.html","id":"plotting","chapter":"7 Case Study","heading":"7.4.3 Plotting","text":"way, method used create data allows us summarize number unique birthdays using table bar chart. Let’s now. Note since first argument tally() data pipe operator work without extra effort. must tell R data previous argument pipeline thus use symbol . denote .Figure 7.1 plot number unique birthdays sample.\nFigure 7.1: Bar chart number unique birthdays sample.\nExercise:\nmean length unique birthdays 16, terms matches?64","code":"\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %>%\n  tally(~length,data=.)## length\n##  14  15  16  17  18 \n##   1   7  52 253 687\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %>%\n  gf_bar(~length) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Number of unique birthdays\",y=\"Count\")"},{"path":"CS2.html","id":"mathematical-solution","chapter":"7 Case Study","heading":"7.4.4 Mathematical solution","text":"solve problem mathematically, step logic one step time. One key ideas see many times idea multiplication rule. idea foundation permutation combinations counting methods frequently used probability calculations.first step take understand idea 2 people birthday. 18 people, great deal possibilities 2 birthdays. exactly 2 people birthday. 18 people birthday, 3 people birthday another 2 people birthday different 3. Accounting possibilities large counting process. Instead, take approach finding probability one matching birthday. probability least 2 people matching birthday 1 minus probability one matching birthday. known complementary probability. simpler example think rolling single die. probability rolling 6 equivalent 1 minus probability rolling 6.first need think different ways get 18 birthdays. going denominator probability calculation. First let’s just look 2 people. first person 365 different days birthday. second person also 365 different birthdays. birthday first person 365 birthdays second. Thus 2 people \\(365^2\\) possible sets birthdays. example multiplication rule. 18 people \\(365^{18}\\) sets birthdays. large number. , denominator calculating probability.numerator number sets birthdays matches. , let’s consider 2 people. first person can birthday day year, 365 possibilities. Since don’t want match, second person can 364 possibilities birthday. Thus \\(365 \\times 364\\) possibilities two people different birthdays.Exercise:\nnumber possibilities 18 people one birthday.answer 18 people \\(365 \\times 364 \\times 363 ... \\times 349 \\times 348\\). looks like truncated factorial. Remember factorial, written \\(n!\\) explanation point, product successive positive integers. example \\(3!\\) \\(3 \\times 2 \\times 1\\) 6. write multiplication numerator \\[\\frac{365!}{(365-n)!}\\] learn, multiplication rule numerator known permutation.ready put together. 18 people, probability 2 people birthday 1 minus probability one birthday, \\[1 - \\frac{\\frac{365!}{(365-18)!}}{365^{18}}\\] \\[1 - \\frac{\\frac{365!}{347!}}{365^{18}}\\]R function called factorial() factorials get large fast overflow memory. Try factorial(365) R see happens.returning infinity number large buffer. often case using computational method, must clever approach. Instead using factorials can make use Rs ability work vectors. provide R vector values, prod() perform product elements.","code":"\nfactorial(365)## [1] Inf\n365*364## [1] 132860\nprod(365:364)## [1] 132860\n1- prod(365:348)/(365^18)## [1] 0.3469114"},{"path":"CS2.html","id":"general-solution","chapter":"7 Case Study","heading":"7.4.5 General solution","text":"now mathematics understand problem. can easily generalize number people. , write function R. everything R, save function object. general format creating function isFor problem call function birthday_prob(). parameter need number people room, n. Let’s write function.Notice assigned function name birthday_prob, told R expect one argument function, calling n, provide R code find probability. set default value n case one provided prevent error function run. learn writing functions next semester.Test code know answer.Now can determine probability size room. may heard takes 23 people room 50% probability least 2 people matching birthdays.Let’s create plot probability versus number people room. , need apply function vector values. function sapply() work can also use Vectorize() alter existing function. choose latter option.First notice happens input vector function.uses first value. several ways solve problem. can use map() function purrr package. idea mapping function vector important data science. used scenarios lot data. case idea map-reduce used make analysis amenable parallel computing.also just vectorize function.Now notice happens.good go. Let’s create line plot, Figure 7.2.\nFigure 7.2: probability least 2 people mathcing birthdays\nexpected curve look like? , authors, expect . sigmodial shape large increase middle range flatten tails.","code":"my_function <- function(parameters){\n  code for function\n}\nbirthday_prob <- function(n=20){\n  1- prod(365:(365-(n-1)))/(365^n)\n}\nbirthday_prob(18)## [1] 0.3469114\nbirthday_prob(23)## [1] 0.5072972\nbirthday_prob(1:20)## Warning in 365:(365 - (n - 1)): numerical expression has 20 elements: only the\n## first used##  [1] 0.0000000 0.9972603 0.9999925 1.0000000 1.0000000 1.0000000 1.0000000\n##  [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n## [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\nmap_dbl(1:20,birthday_prob)##  [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484\n##  [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789\n## [13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418\n## [19] 0.379118526 0.411438384\nbirthday_prob <- Vectorize(birthday_prob)\nbirthday_prob(1:20)##  [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484\n##  [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789\n## [13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418\n## [19] 0.379118526 0.411438384\ngf_line(birthday_prob(1:100)~ seq(1,100),\n        xlab=\"Number of People\",\n        ylab=\"Probability of Match\",\n        title=\"Probability of at least 2 people with matching birthdays\") %>%\n  gf_theme(theme_bw())"},{"path":"CS2.html","id":"data-science-approach","chapter":"7 Case Study","heading":"7.4.6 Data science approach","text":"final approach take one based data, data science approach. mosaicData package data set called Births contains number births US 1969 1988. data allow us estimate number births day year. allows us eliminate reliance assumption day equally likely. Let’s first inspect() data object.argued randomly pick one year use . Let’s see happens just used 1969. Figure 7.3 scatter plot number births 1969 day year.\nFigure 7.3: number births day year 1969\nExercise:\npatterns see Figure 7.3? might explain ?definitely bands appearing data day week; less birthdays weekend. also seasonality birthdays summer fall. also probably impact holidays.Quickly, let’s look impact day week using color day week. Figure 7.4 makes clear weekends less number births compared work week.\nFigure 7.4: number births day year 1969 broken day week\nusing one year, data might give poor results since holidays fall certain days week weekends also impacted. Note also still problem leap years.years 1972, 1976, 1980, 1984, 1988 leap years. point, make analysis easier, drop years.Notice filter() used %% argument. logical argument checking year one values. ! front negates sense requiring year one values.`almost ready simulate. need get count births day year non-leap years.Let’s look plot number births versus day year. combined years Figure 7.5.\nFigure 7.5: Number births day year years.\ncurve seasonal cycling expect. smaller scale cycling unexpected. Maybe dropping leap years, getting days appearing time interval frequently weekends. leave investigate phenomenon.use counts weights sampling process. Days births higher probability selected. Days Christmas Christmas Eve lower probability selected. Let’s save weights object use sample() function.pull() function pulls vectors values data frame format vector format sample() needs.Now let’s simulate problem. probability match change slightly, maybe go slightly?, much since days probability number occurrences.solve problem varying frequency birth days using mathematics, least far know.Cool stuff, let’s get learning probability models next chapters.","code":"\ninspect(Births)## \n## categorical variables:  \n##   name   class levels    n missing\n## 1 wday ordered      7 7305       0\n##                                    distribution\n## 1 Wed (14.3%), Thu (14.3%), Fri (14.3%) ...    \n## \n## Date variables:  \n##   name class      first       last min_diff max_diff    n missing\n## 1 date  Date 1969-01-01 1988-12-31   1 days   1 days 7305       0\n## \n## quantitative variables:  \n##              name   class  min   Q1 median    Q3   max        mean          sd\n## ...1       births integer 6675 8792   9622 10510 12851 9648.940178 1127.315229\n## ...2         year integer 1969 1974   1979  1984  1988 1978.501027    5.766735\n## ...3        month integer    1    4      7    10    12    6.522930    3.448939\n## ...4  day_of_year integer    1   93    184   275   366  183.753593  105.621885\n## ...5 day_of_month integer    1    8     16    23    31   15.729637    8.800694\n## ...6  day_of_week integer    1    2      4     6     7    4.000274    1.999795\n##         n missing\n## ...1 7305       0\n## ...2 7305       0\n## ...3 7305       0\n## ...4 7305       0\n## ...5 7305       0\n## ...6 7305       0\nBirths %>%\n  filter(year == 1969) %>%\n  gf_point(births~day_of_year) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Day of the Year\",y=\"Number of Births\")\nBirths %>%\n  filter(year == 1969) %>%\n  gf_point(births~day_of_year,color=~factor(day_of_week)) %>%\n  gf_labs(x=\"Day of the Year\",col=\"Day of Week\") %>%\n  gf_theme(theme_bw())\nBirths %>%\n  group_by(year) %>%\n  summarise(n=n())## # A tibble: 20 x 2\n##     year     n\n##    <int> <int>\n##  1  1969   365\n##  2  1970   365\n##  3  1971   365\n##  4  1972   366\n##  5  1973   365\n##  6  1974   365\n##  7  1975   365\n##  8  1976   366\n##  9  1977   365\n## 10  1978   365\n## 11  1979   365\n## 12  1980   366\n## 13  1981   365\n## 14  1982   365\n## 15  1983   365\n## 16  1984   366\n## 17  1985   365\n## 18  1986   365\n## 19  1987   365\n## 20  1988   366\nBirths %>%\n  filter(!(year %in% c(1972,1976,1980,1984,1988))) %>%\n  group_by(year) %>%\n  summarise(n=n())## # A tibble: 15 x 2\n##     year     n\n##    <int> <int>\n##  1  1969   365\n##  2  1970   365\n##  3  1971   365\n##  4  1973   365\n##  5  1974   365\n##  6  1975   365\n##  7  1977   365\n##  8  1978   365\n##  9  1979   365\n## 10  1981   365\n## 11  1982   365\n## 12  1983   365\n## 13  1985   365\n## 14  1986   365\n## 15  1987   365\nbirth_data <- Births %>%\n  filter(!(year %in% c(1972,1976,1980,1984,1988))) %>%\n  group_by(day_of_year) %>%\n  summarise(n=sum(births)) \nhead(birth_data)## # A tibble: 6 x 2\n##   day_of_year      n\n##         <int>  <int>\n## 1           1 120635\n## 2           2 129042\n## 3           3 135901\n## 4           4 136298\n## 5           5 137319\n## 6           6 140044\nbirth_data %>%\n  gf_point(n~day_of_year,\n          xlab=\"Day of the year\",\n          ylab=\"Number of births\") %>%\n  gf_theme(theme_bw())\nbirth_data_weights <- birth_data %>%\n  select(n) %>%\n  pull()\nset.seed(20)\n(do(1000)*length(unique(sample(days,size=18,replace = TRUE,prob=birth_data_weights)))) %>%\n  mutate(match=if_else(length==18,0,1)) %>%\n  summarise(prob=mean(match))##    prob\n## 1 0.352"},{"path":"CS2.html","id":"homework-problems-6","chapter":"7 Case Study","heading":"7.5 Homework Problems","text":"Exactly 2 people birthday - Simulation. Complete similar analysis case exactly 2 people room 23 people birthday. exercise use computational simulation.Create new R Markdown file create report. Yes, know use file want practice generating report.Simulate 23 people class day year equally likely. Find cases exactly 2 people birthday, alter code Notes changing 18 23.Plot frequency occurrences bar chart.Estimate probability exactly two people birthday.Exactly 2 people birthday - Mathematical. Repeat problem 1 mathematically. big hint, need use choose() function. idea 23 people need choose 2 match. thus need multiply, multiplication rule , choose(23,2). trouble, work total 3 people room first.Find formula determine exact probability exactly 2 people room 23 birthday.Generalize solution number n people room create function.Vectorize function.Plot probability exactly 2 people birthday versus number people room.Comment shape curve explain .","code":""},{"path":"PROBRULES.html","id":"PROBRULES","chapter":"8 Probability Rules","heading":"8 Probability Rules","text":"","code":""},{"path":"PROBRULES.html","id":"objectives-7","chapter":"8 Probability Rules","heading":"8.1 Objectives","text":"Define use properly context new terminology related probability include limited : outcome, event, sample space, probability.Apply basic probability counting rules find probabilities.Describe basic axioms probability.Use R calculate simulate probabilities events.","code":""},{"path":"PROBRULES.html","id":"probability-vs-statistics","chapter":"8 Probability Rules","heading":"8.2 Probability vs Statistics","text":"review, remember course divided four general blocks: data collection/summary, probability models, inference statistical modeling/prediction. second block, probability, study stochastic (random) processes properties. Specifically, explore random experiments. name suggests, random experiment experiment whose outcome predictable exact certainty. statistical models develop last two blocks course, use variables explain variance outcome interest. remaining variance modeled probability models.Even though outcome determined chance, mean know nothing random experiment. favorite simple example coin flip. flip coin, possible outcomes heads tails. don’t know sure outcome occur, doesn’t mean don’t know anything experiment. assume coin fair, know outcome equally likely. Also, know flip coin 100 times (independently), likely, highest frequency event, see around 50 heads, unlikely see 10 heads fewer.important distinguish probability inference modeling. probability, consider known random experiment, including knowing parameters, answer questions expect see random experiment. statistics (inference modeling), consider data (results mysterious random experiment) infer underlying process. example, suppose coin unsure whether coin fair unfair, parameter unknown. flipped 20 times landed heads 14 times. Inferential statistics help us answer questions underlying process (coin unfair?).block (9 lessons ) devoted study random experiments. First, explore simple experiments, counting rule problems, conditional probability. Next, introduce concept random variable properties random variables. Following , cover common distributions discrete continuous random variables. end block multivariate probability (joint distributions covariance).","code":""},{"path":"PROBRULES.html","id":"basic-probability-terms","chapter":"8 Probability Rules","heading":"8.3 Basic probability terms","text":"start work definitions examples.","code":""},{"path":"PROBRULES.html","id":"sample-space","chapter":"8 Probability Rules","heading":"8.3.1 Sample space","text":"Suppose random experiment. sample space experiment, \\(S\\), set possible results experiment. example, case coin flip, write \\(S=\\{H,T\\}\\). element sample space considered outcome. event set outcomes, subset sample space.Example:\nLet’s let R flip coin us record number heads tails. R flip coin twice. sample space, example outcome, example event.load mosaic package function rflip() simulate flipping coin.sample space \\(S=\\{HH, TH, HT, TT\\}\\), example outcome \\(HH\\) see output R, finally example event number heads, case takes values 0, 1, 2. Another example event “least one heads”. case event \\(\\{HH,TH, HT\\}\\). Also notice \\(TH\\) different \\(HT\\) outcome; different outcomes flipping coin twice.Example Event:\nSuppose arrive rental car counter show list available vehicles, one picked random. sample space experiment \n\\[\nS=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}.\n\\]vehicle represents possible outcome experiment. Let \\(\\) event blue vehicle selected. event contains outcomes blue sedan blue SUV.","code":"\nlibrary(mosaic)\nset.seed(18)\nrflip(2)## \n## Flipping 2 coins [ Prob(Heads) = 0.5 ] ...\n## \n## H H\n## \n## Number of Heads: 2 [Proportion Heads: 1]"},{"path":"PROBRULES.html","id":"union-and-intersection","chapter":"8 Probability Rules","heading":"8.3.2 Union and intersection","text":"Suppose two events \\(\\) \\(B\\).\\(\\) considered subset \\(B\\) outcomes \\(\\) also contained \\(B\\). denoted \\(\\subset B\\).\\(\\) considered subset \\(B\\) outcomes \\(\\) also contained \\(B\\). denoted \\(\\subset B\\).intersection \\(\\) \\(B\\) outcomes contained \\(\\) \\(B\\). denoted \\(\\cap B\\).intersection \\(\\) \\(B\\) outcomes contained \\(\\) \\(B\\). denoted \\(\\cap B\\).union \\(\\) \\(B\\) outcomes contained either \\(\\) \\(B\\), . denoted \\(\\cup B\\).union \\(\\) \\(B\\) outcomes contained either \\(\\) \\(B\\), . denoted \\(\\cup B\\).complement \\(\\) outcomes contained \\(\\). denoted \\(^C\\) \\('\\).complement \\(\\) outcomes contained \\(\\). denoted \\(^C\\) \\('\\).Note: treating events sets definitions basic set operations.sometimes helpful reading probability notation think Union Intersection .Example:\nConsider rental car example . Let \\(\\) event blue vehicle selected, let \\(B\\) event black vehicle selected, let \\(C\\) event SUV selected.First, let’s list outcomes event. \\(= \\{\\mbox{blue sedan},\\mbox{blue SUV}\\}\\), \\(B=\\{\\mbox{black SUV}\\}\\), \\(C= \\{\\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\).Since outcomes \\(B\\) contained \\(C\\), know \\(B\\) subset \\(C\\), \\(B\\subset C\\). Also, since \\(\\) \\(B\\) outcomes common, \\(\\cap B = \\emptyset\\). Note \\(\\emptyset = \\{ \\}\\) empty set contains elements. , \\(\\cup C = \\{\\mbox{blue sedan}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\).","code":""},{"path":"PROBRULES.html","id":"probability","chapter":"8 Probability Rules","heading":"8.4 Probability","text":"Probability number assigned event outcome describes likely occur. probability model assigns probability element sample space. makes probability model just values assigned element idea model contains information outcomes explanatory variables involved.probability model can thought function maps outcomes, events, real number interval \\([0,1]\\).basic axioms probability know, although list complete. Let \\(S\\) sample space random experiment let \\(\\) event \\(\\subset S\\).\\(\\mbox{P}() \\geq 0\\).\\(\\mbox{P}() \\geq 0\\).\\(\\mbox{P}(S) = 1\\).\\(\\mbox{P}(S) = 1\\).two axioms essentially say probability must positive, probability outcomes must sum 1.","code":""},{"path":"PROBRULES.html","id":"probability-properties","chapter":"8 Probability Rules","heading":"8.4.1 Probability properties","text":"Let \\(\\) \\(B\\) events random experiment. can proven fairly easily.\\(\\mbox{P}(\\emptyset)=0\\)\\(\\mbox{P}(\\emptyset)=0\\)\\(\\mbox{P}(')=1-\\mbox{P}()\\) used case study.\\(\\mbox{P}(')=1-\\mbox{P}()\\) used case study.\\(\\subset B\\), \\(\\mbox{P}()\\leq \\mbox{P}(B)\\).\\(\\subset B\\), \\(\\mbox{P}()\\leq \\mbox{P}(B)\\).\\(\\mbox{P}(\\cup B) = \\mbox{P}()+\\mbox{P}(B)-\\mbox{P}(\\cap B)\\). property can generalized two events. intersection subtracted outcomes events \\(\\) \\(B\\) get counted twice first sum.\\(\\mbox{P}(\\cup B) = \\mbox{P}()+\\mbox{P}(B)-\\mbox{P}(\\cap B)\\). property can generalized two events. intersection subtracted outcomes events \\(\\) \\(B\\) get counted twice first sum.Law Total Probability: Let \\(B_1, B_2,...,B_n\\) mutually exclusive, means disjoint outcomes common, exhaustive, means union events labeled \\(B\\) sample space. ThenLaw Total Probability: Let \\(B_1, B_2,...,B_n\\) mutually exclusive, means disjoint outcomes common, exhaustive, means union events labeled \\(B\\) sample space. \\[\n\\mbox{P}()=\\mbox{P}(\\cap B_1)+\\mbox{P}(\\cap B_2)+...+\\mbox{P}(\\cap B_n)\n\\]specific application law appears Bayes’ Rule (follow). says \\(\\mbox{P}()=\\mbox{P}(\\cap B)+\\mbox{P}(\\cap B')\\). Essentially, points \\(\\) can partitioned two parts: 1) everything \\(\\) \\(B\\) 2) everything \\(\\) \\(B\\).Example:\nConsider rolling six sided die. Let event \\(\\) number showing less 5. Let event \\(B\\) number even. \\[\\mbox{P}()=\\mbox{P}(\\cap B) + \\mbox{P}(\\cap B')\\]\\[\n\\mbox{P}(< 5)=\\mbox{P}(<5 \\cap Even)+\\mbox{P}(<5 \\cap Odd)\n\\]DeMorgan’s Laws:\n\\[\n\\mbox{P}((\\cup B)')=\\mbox{P}(' \\cap B')\n\\]\n\\[\n\\mbox{P}((\\cap B)')=\\mbox{P}(' \\cup B')\n\\]","code":""},{"path":"PROBRULES.html","id":"equally-likely-scenarios","chapter":"8 Probability Rules","heading":"8.4.2 Equally likely scenarios","text":"random experiments, outcomes can defined individual outcome equally likely. case, probability becomes counting problem. Let \\(\\) event experiment outcome equally likely.\n\\[\n\\mbox{P}()=\\frac{\\mbox{# outcomes }}{\\mbox{# outcomes S}}\n\\]Example:\nSuppose family three children, child either boy (B) girl (G). Assume likelihood boys girls equal independent, idea probability gender second child change based gender first child. sample space can written :\n\\[\nS=\\{\\mbox{BBB},\\mbox{BBG},\\mbox{BGB},\\mbox{BGG},\\mbox{GBB},\\mbox{GBG},\\mbox{GGB},\\mbox{GGG}\\}\n\\]\nprobability family exactly 2 girls?happens three ways: BGG, GBG, GGB. Thus, probability exactly 2 girls 3/8 0.375.","code":""},{"path":"PROBRULES.html","id":"using-r-equally-likely-scenarios","chapter":"8 Probability Rules","heading":"8.4.3 Using R (Equally likely scenarios)","text":"previous example example “Equally Likely” scenario, sample space random experiment contains list outcomes equally likely. cases, can sometimes use R list possible outcomes count determine probability. can also use R simulate.Example:\nUse R simulate family three children child probability boy girl.Instead writing function, can use rflip() mosaic package. let \\(H\\) stand girl.First simulate one family.case got 1 girl. Next use () function repeat simulation.Next can visualize distribution number girls, heads, Figure 8.1.\nFigure 8.1: Number girls family size 3.\nFinally can estimate probability exactly 2 girls. need tidyverse library.slightly different code.bad estimate exact probability.Let’s now use example cards simulate probabilities well learning counting. file Cards.csv contains data cards 52 card deck. Let’s read summarize.can see 4 suits, 13 ranks, value face card.Example:\nSuppose draw one card standard deck. Let \\(\\) event draw Club. Let \\(B\\) event draw 10 face card (Jack, Queen, King Ace). can use R define events find probabilities.Let’s find Clubs.just counting, find probability drawing Club \\(\\frac{13}{52}\\) 0.25.can simulation, kill gets idea simulation across.Remember, ask want R R need ?Now let’s count number outcomes \\(B\\).just counting, find probability drawing 10 greater \\(\\frac{20}{52}\\) 0.3846154.Exercise:\nUsing simulation estimate probability 10 higher.Notice code robust change number simulations. change 10000, change denominator summarize() function. can change using mutate() instead filter().Notice mutate() function, creating new logical variable called face. variable takes values TRUE FALSE. next line use summarize() command function mean(). R function requires numeric input takes logical variable converts TRUE 1 FALSE 0. Thus mean() find proportion TRUE values report probability.Next, let’s find card 10 greater club.find probability drawing 10 greater club \\(\\frac{5}{52}\\) 0.0961538.Exercise:\nSimulate drawing one card estimate probability club 10 greater.","code":"\nset.seed(73)\nrflip(3)## \n## Flipping 3 coins [ Prob(Heads) = 0.5 ] ...\n## \n## T T H\n## \n## Number of Heads: 1 [Proportion Heads: 0.333333333333333]\nresults <- do(10000)*rflip(3)\nhead(results)##   n heads tails      prop\n## 1 3     1     2 0.3333333\n## 2 3     3     0 1.0000000\n## 3 3     3     0 1.0000000\n## 4 3     3     0 1.0000000\n## 5 3     1     2 0.3333333\n## 6 3     1     2 0.3333333\nresults %>%\n  gf_bar(~heads) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"NUmber of girls\",y=\"Count\")\nlibrary(tidyverse)\nresults %>%\n  filter(heads==2) %>%\n  summarize(prob=n()/10000)##     prob\n## 1 0.3782\nresults %>%\n  count(heads) %>%\n  mutate(prop=n/sum(n))##   heads    n   prop\n## 1     0 1241 0.1241\n## 2     1 3786 0.3786\n## 3     2 3782 0.3782\n## 4     3 1191 0.1191\nCards <- read_csv(\"data/Cards.csv\")\ninspect(Cards)## \n## categorical variables:  \n##   name     class levels  n missing\n## 1 rank character     13 52       0\n## 2 suit character      4 52       0\n##                                    distribution\n## 1 10 (7.7%), 2 (7.7%), 3 (7.7%) ...            \n## 2 Club (25%), Diamond (25%) ...                \n## \n## quantitative variables:  \n##       name   class        min         Q1     median         Q3        max\n## ...1 probs numeric 0.01923077 0.01923077 0.01923077 0.01923077 0.01923077\n##            mean sd  n missing\n## ...1 0.01923077  0 52       0\nhead(Cards)## # A tibble: 6 x 3\n##   rank  suit   probs\n##   <chr> <chr>  <dbl>\n## 1 2     Club  0.0192\n## 2 3     Club  0.0192\n## 3 4     Club  0.0192\n## 4 5     Club  0.0192\n## 5 6     Club  0.0192\n## 6 7     Club  0.0192\nCards %>%\n  filter(suit == \"Club\") %>%\n  select(rank,suit)## # A tibble: 13 x 2\n##    rank  suit \n##    <chr> <chr>\n##  1 2     Club \n##  2 3     Club \n##  3 4     Club \n##  4 5     Club \n##  5 6     Club \n##  6 7     Club \n##  7 8     Club \n##  8 9     Club \n##  9 10    Club \n## 10 J     Club \n## 11 Q     Club \n## 12 K     Club \n## 13 A     Club\nresults <- do(10000)*sample(Cards,1)\nhead(results)## # A tibble: 6 x 6\n##   rank  suit   probs orig.id  .row .index\n##   <chr> <chr>  <dbl> <chr>   <int>  <dbl>\n## 1 9     Spade 0.0192 47          1      1\n## 2 5     Club  0.0192 4           1      2\n## 3 5     Spade 0.0192 43          1      3\n## 4 7     Heart 0.0192 32          1      4\n## 5 4     Club  0.0192 3           1      5\n## 6 A     Spade 0.0192 52          1      6\nresults %>%\n  filter(suit == \"Club\") %>%\n  summarize(prob=n()/10000)## # A tibble: 1 x 1\n##    prob\n##   <dbl>\n## 1 0.243\nresults %>%\n  count(suit) %>%\n  mutate(prob=n/sum(n))## # A tibble: 4 x 3\n##   suit        n  prob\n##   <chr>   <int> <dbl>\n## 1 Club     2432 0.243\n## 2 Diamond  2558 0.256\n## 3 Heart    2417 0.242\n## 4 Spade    2593 0.259\nCards %>%\n  filter(rank %in% c(10, \"J\", \"Q\", \"K\", \"A\")) %>%\n  select(rank,suit)## # A tibble: 20 x 2\n##    rank  suit   \n##    <chr> <chr>  \n##  1 10    Club   \n##  2 J     Club   \n##  3 Q     Club   \n##  4 K     Club   \n##  5 A     Club   \n##  6 10    Diamond\n##  7 J     Diamond\n##  8 Q     Diamond\n##  9 K     Diamond\n## 10 A     Diamond\n## 11 10    Heart  \n## 12 J     Heart  \n## 13 Q     Heart  \n## 14 K     Heart  \n## 15 A     Heart  \n## 16 10    Spade  \n## 17 J     Spade  \n## 18 Q     Spade  \n## 19 K     Spade  \n## 20 A     Spade\nresults <- do(10000)*sample(Cards,1)\nhead(results)## # A tibble: 6 x 6\n##   rank  suit   probs orig.id  .row .index\n##   <chr> <chr>  <dbl> <chr>   <int>  <dbl>\n## 1 10    Heart 0.0192 35          1      1\n## 2 6     Heart 0.0192 31          1      2\n## 3 8     Spade 0.0192 46          1      3\n## 4 J     Heart 0.0192 36          1      4\n## 5 Q     Spade 0.0192 50          1      5\n## 6 10    Club  0.0192 9           1      6\nresults %>%\n  filter(rank %in% c(10, \"J\", \"Q\", \"K\", \"A\")) %>%\n  summarize(prob=n()/10000)## # A tibble: 1 x 1\n##    prob\n##   <dbl>\n## 1 0.389\nresults %>%\n  mutate(face=rank %in% c(10, \"J\", \"Q\", \"K\", \"A\"))%>%\n  summarize(prob=mean(face))## # A tibble: 1 x 1\n##    prob\n##   <dbl>\n## 1 0.389\nCards %>%\n  filter(rank %in% c(10, \"J\", \"Q\", \"K\", \"A\"),suit==\"Club\") %>%\n  select(rank,suit)## # A tibble: 5 x 2\n##   rank  suit \n##   <chr> <chr>\n## 1 10    Club \n## 2 J     Club \n## 3 Q     Club \n## 4 K     Club \n## 5 A     Club\nresults %>%\n  mutate(face=(rank %in% c(10, \"J\", \"Q\", \"K\", \"A\"))&(suit==\"Club\"))%>%\n  summarize(prob=mean(face))## # A tibble: 1 x 1\n##     prob\n##    <dbl>\n## 1 0.0963"},{"path":"PROBRULES.html","id":"note","chapter":"8 Probability Rules","heading":"8.4.4 Note","text":"using R count number outcomes event. helped us determine probabilities. limited problems simple ones. cards example, interesting us explore complex events drawing 5 cards standard deck. draw 5 cards equally likely, order find probability flush (5 cards suit), simply list possible flushes compare sample space. large number possible outcomes, becomes difficult. Thus need explore counting rules detail help us solve complex problems. course limit discussion three basic cases. know entire courses discrete math counting rules, still limited methods type problems can solve course.","code":""},{"path":"PROBRULES.html","id":"counting-rules","chapter":"8 Probability Rules","heading":"8.5 Counting rules","text":"three types counting problems consider. case, multiplication rule used changes whether element allowed reused, replacement, whether order selection matters. latter question difficult. case demonstrated example.","code":""},{"path":"PROBRULES.html","id":"multiplication-rule-1-order-matters-sample-with-replacement","chapter":"8 Probability Rules","heading":"8.5.1 Multiplication rule 1: Order matters, sample with replacement","text":"multiplication rule center three methods. first case using idea order matters items can reused. Let’s use example help.Example:\nlicense plate consists three numeric digits (0-9) followed three single letters (-Z). many possible license plates exist?can divide problem two sections. numeric section, selecting 3 objects 10, replacement. means number can used . Order clearly matters license plate starting “432” distinct license plate starting “234”. \\(10^3 = 1000\\) ways select first three digits; 10 first, 10 second, 10 third. multiply add?65In alphabet section, selecting 3 objects 26, order matters. Thus, \\(26^3=17576\\) ways select last three letters plate. Combined, \\(10^3 \\times 26^3 = 17576000\\) ways select license plates. Visually,\n\\[\n\\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} = 17,576,000\n\\]Next going use new counting method find probability.Exercise:\nprobability license plate starts number “8” “0” ends letter “B”?order find probability, simply need determine number ways select license plate starting “8” “0” ending letter “B”. can visually represent event:\n\\[\n\\underbrace{\\underline{\\quad 2 \\quad }}_\\text{8 0} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 1 \\quad }}_\\text{B} = 135,200\n\\]Dividing number total number possible license plates yields probability event occurring.probability obtaining license plate starting “8” “0” ending “B” 0.0077. Simulating difficult need special functions check first number last letter. gets text mining important subject data science unfortunately don’t much time course topic.","code":"\ndenom<-10*10*10*26*26*26\nnum<-2*10*10*26*26*1\nnum/denom## [1] 0.007692308"},{"path":"PROBRULES.html","id":"multiplication-rule-2-permutation-order-matters-sampling-without-replacement","chapter":"8 Probability Rules","heading":"8.5.2 Multiplication rule 2 (Permutation): Order Matters, Sampling Without Replacement","text":"Consider random experiment sample group size \\(n\\), without replacement, outcome experiment depends order outcomes. number ways select \\(k\\) objects given \\(n(n-1)(n-2)...(n-k+1)\\). known permutation sometimes written \n\\[\n{}_nP_{k} = \\frac{n!}{(n-k)!}\n\\]Recall \\(n!\\) read \\(n\\) factorial represents number ways arrange \\(n\\) objects.Example:\nTwenty-five friends participate Halloween costume party. Three prizes given party: creative costume, scariest costume, funniest costume. one can win one prize. many possible ways can prizes distributed?\\(k=3\\) prizes assigned \\(n=25\\) people. someone selected prize, removed pool eligibles. words, sampling without replacement. Also, order matters. example, Tom, Mike, Jane, win creative, scariest funniest costume, respectively, different outcome Mike won creative, Jane won scariest Tom won funniest. Thus, number ways prizes can distributed given \\({}_{25}P_3 = \\frac{25!}{22!} = 13,800\\). visually pleasing way express :\n\\[\n\\underbrace{\\underline{\\quad 25 \\quad }}_\\text{creative} \\times \\underbrace{\\underline{\\quad 24 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{funniest} = 13,800\n\\]Notice sometime difficult determine order matters problem, example name prize hint indeed order matters.Let’s use idea permutation calculate probability.Exercise:\nAssume 25 participants equally likely win one three prizes. probability Tom doesn’t win ?Just like previous probability calculation, simply need count number ways Tom doesn’t win prize. words, need count number ways prizes distributed without Tom. , remove Tom group 25 eligible participants. number ways Tom doesn’t get prize \\({}_{24}P_3 = \\frac{24!}{21!}=12,144\\). visually:\n\\[\n\\underbrace{\\underline{\\quad 24 \\quad }}_\\text{creative} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 22 \\quad }}_\\text{funniest} = 12,144\n\\]probability Tom doesn’t get prize simply second number divided first:","code":"\ndenom<-factorial(25)/factorial(25-3)\n# Or, denom<-25*24*23\nnum<-24*23*22\nnum/denom## [1] 0.88"},{"path":"PROBRULES.html","id":"multiplication-rule-3-combination-order-does-not-matter-sampling-without-replacement","chapter":"8 Probability Rules","heading":"8.5.3 Multiplication rule 3 (Combination): Order Does Not Matter, Sampling Without Replacement","text":"Consider random experiment sample group size \\(n\\), without replacement, outcome experiment depend order outcomes. number ways select \\(k\\) objects given \\(\\frac{n!} {(n-k)!k!}\\). known combination written :\n\\[\n\\binom{n}{k} = \\frac{n!}{(n-k)!k!}\n\\]read “\\(n\\) choose \\(k\\)”. Take moment compare combinations permutations, discussed Rule 2. difference two rules combination, order longer matters. combination equivalent permutation divided \\(k!\\), number ways arrange \\(k\\) objects selected.Example:\nSuppose draw 5 cards standard deck (52 cards, jokers). many possible 5 card hands ?example, order matter. don’t care receive 3 jacks 2 queens 2 queens 3 jacks. Either way, ’s collection 5 cards. Also, drawing without replacement. card selected, selected . Thus, number ways select 5 cards given :\n\\[\n\\binom{52}{5} = \\frac{52!}{(52-5)!5!} = 2,598,960\n\\]Example:\ndrawing 5 cards, probability drawing “flush” (5 cards suit)?Let’s determine many ways draw flush. four suits (clubs, hearts, diamonds spades). suit 13 cards. like pick 5 13 cards 0 remaining 39. Let’s consider just one suits (clubs):\n\\[\n\\mbox{P}(\\mbox{5 clubs})=\\frac{\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}}\n\\]second part numerator (\\(\\binom{39}{0}\\)) isn’t necessary, since simply represents number ways select 0 objects group (1 way), helps clearly lay events. brings point \\(0!\\) equals. definition 1. allows us use \\(0!\\) work.Now, expand four suits multiplying 4, \\(\\binom{4}{1}\\) since selecting 1 suit 4:\n\\[\n\\mbox{P}(\\mbox{flush})=\\frac{\\binom{4}{1}\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}}\n\\]probability 0.0020 drawing flush draw 5 cards standard deck cards.Exercise:\ndrawing 5 cards, probability drawing “full house” (3 cards rank 2 rank)?problem uses several ideas lesson. need pick rank three kind. pick 3 cards 4 possible. Next pick rank pair remaining 12 ranks. Finally pick 2 cards rank 4 possible.\\[\n\\mbox{P}(\\mbox{full house})=\\frac{\\binom{13}{1}\\binom{4}{3}\\binom{12}{1}\\binom{4}{2}}{\\binom{52}{5}}\n\\]use \\(\\binom{13}{2}\\) instead \\(\\binom{13}{1}\\binom{12}{1}\\)?66We just determined full house lower probability occurring flush. gambling, flush valued less full house.","code":"\nnum<-4*choose(13,5)*1\ndenom<-choose(52,5)\nnum/denom## [1] 0.001980792\nnum<-choose(13,1)*choose(4,3)*choose(12,1)*choose(4,2)\ndenom<-choose(52,5)\nnum/denom## [1] 0.001440576"},{"path":"PROBRULES.html","id":"homework-problems-7","chapter":"8 Probability Rules","heading":"8.6 Homework Problems","text":"Let \\(\\), \\(B\\) \\(C\\) events \\(\\mbox{P}()=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\). Also, know \\(\\mbox{P}(\\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(\\cap C)=0.1\\), \\(\\mbox{P}(\\cap B \\cap C)=0.05\\). Find following:\\(\\mbox{P}(\\cup B)\\)\\(\\mbox{P}(\\cup B \\cup C)\\)\\(\\mbox{P}(B'\\cap C')\\)\\(\\mbox{P}(\\cup (B\\cap C))\\)\\(\\mbox{P}((\\cup B \\cup C)\\cap (\\cap B \\cap C)')\\)Consider example family reading. probability family least one boy?Consider example family reading. probability family least one boy?Birthday Problem Revisited.Birthday Problem Revisited.Suppose \\(n=20\\) students classroom. birthday, instructor, April 3rd. probability least one student shares birthday? Assume 365 days year assume birthdays equally likely.R, find probability least one person shares birthday value \\(n\\) 1 300. Plot probabilities \\(n\\) \\(x\\)-axis probability \\(y\\)-axis. value \\(n\\) probability least 50%?Thinking cards . Answer following questions:Define two events mutually exclusive.Define two events independent.Define event complement.Consider license plate example reading.probability license plate contains exactly one “B”?probability license plate contains least one “B”?Consider party example reading.Suppose 8 people showed party dressed zombies. probability three awards won people dressed zombies?probability zombies win “creative” “funniest” “scariest”?Consider cards example reading.many ways can obtain “two pairs” (2 one number, 2 another, final different)?probability drawing “four kind” (four cards value)?Advanced Question: Consider rolling 5 dice. probability pour resulting full house?","code":""},{"path":"CONDPROB.html","id":"CONDPROB","chapter":"9 Conditional Probability","heading":"9 Conditional Probability","text":"","code":""},{"path":"CONDPROB.html","id":"objectives-8","chapter":"9 Conditional Probability","heading":"9.1 Objectives","text":"Define conditional probability distinguish joint probability.Find conditional probability using definition.Using conditional probability, determine whether two events independent.Apply Bayes’ Rule mathematically via simulation.","code":""},{"path":"CONDPROB.html","id":"conditional-probability","chapter":"9 Conditional Probability","heading":"9.2 Conditional Probability","text":"far, ’ve covered basic axioms probability, properties events (set theory) counting rules. Another important concept, perhaps one important, conditional probability. Often, know certain event sequence events occurred interested probability another event.Example:\nSuppose arrive rental car counter show list available vehicles, one picked random. sample space experiment \n\\[\nS=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}.\n\\]probability blue vehicle selected, given sedan selected?Since know sedan selected, sample space reduced just “red sedan” “blue sedan”. probability selecting blue vehicle sample space simply 1/2.set notation, let \\(\\) event blue vehicle selected. Let \\(B\\) event sedan selected. looking \\(\\mbox{P}(\\mbox{ given } B)\\), also written \\(\\mbox{P}(|B)\\). definition,\n\\[\n\\mbox{P}(|B)=\\frac{\\mbox{P}(\\cap B)}{\\mbox{P}(B)}\n\\]important distinguish event \\(|B\\) \\(\\cap B\\). common misunderstanding probability. \\(\\cap B\\) event outcome selected random total sample space, outcome contained \\(\\) \\(B\\). hand, \\(|B\\) assumes \\(B\\) occurred, outcome drawn remaining sample space, outcome contained \\(\\).Another common misunderstanding involves direction conditional probability. Specifically, \\(|B\\) event \\(B|\\). example, consider medical test disease. probability someone tests positive given disease different probability someone disease given tested positive. explore example Bayes’ Rule section.","code":""},{"path":"CONDPROB.html","id":"independence","chapter":"9 Conditional Probability","heading":"9.3 Independence","text":"Two events, \\(\\) \\(B\\), said independent probability one occurring change whether occurred. looked last lesson now another way looking using conditional probabilities. example, let’s say probability randomly selected student seen latest superhero movie 0.55. randomly select student see /wearing black backpack? probability change? Likely , since movie attendance probably related choice backpack color. two events independent.Mathematically, \\(\\) \\(B\\) considered independent \n\\[\n\\mbox{P}(|B)=\\mbox{P}()\n\\]Result: \\(\\) \\(B\\) independent \n\\[\n\\mbox{P}(\\cap B)=\\mbox{P}()\\mbox{P}(B)\n\\]follows definition conditional probability :\n\\[\n\\mbox{P}(|B)=\\frac{\\mbox{P}(\\cap B)}{\\mbox{P}(B)}=\\mbox{P}()\n\\]Thus, \\(\\mbox{P}(\\cap B)=\\mbox{P}()\\mbox{P}(B)\\).Example:\nConsider example . Recall events \\(\\) \\(B\\). Let \\(\\) event blue vehicle selected. Let \\(B\\) event sedan selected. \\(\\) \\(B\\) independent?. First, recall \\(\\mbox{P}(|B)=0.5\\). probability selecting blue vehicle (\\(\\mbox{P}()\\)) \\(2/7\\) (number blue vehicles sample space divided 7, total number vehicles \\(S\\)). value different 0.5; thus, \\(\\) \\(B\\) independent.also use result determine whether \\(\\) \\(B\\) independent. Note \\(\\mbox{P}()= 2/7\\). Also, know \\(\\mbox{P}(B)=2/7\\). , \\(\\mbox{P}()\\mbox{P}(B)=4/49\\). , \\(\\mbox{P}(\\cap B) = 1/7\\), since just one blue sedan sample space. \\(4/49\\) equal \\(1/7\\); thus, \\(\\) \\(B\\) independent.","code":""},{"path":"CONDPROB.html","id":"bayes-rule","chapter":"9 Conditional Probability","heading":"9.4 Bayes’ Rule","text":"mentioned introduction section, \\(\\mbox{P}(|B)\\) quantity \\(\\mbox{P}(B|)\\). However, given information \\(|B\\) \\(B\\), can use Bayes’ Rule find \\(\\mbox{P}(B|)\\). Let \\(B_1, B_2, ..., B_n\\) mutually exclusive exhaustive events let \\(\\mbox{P}()>0\\). ,\n\\[\n\\mbox{P}(B_k|)=\\frac{\\mbox{P}(|B_k)\\mbox{P}(B_k)}{\\sum_{=1}^n \\mbox{P}(|B_i)\\mbox{P}(B_i)}\n\\]Let’s use example dig comes .Example:\nSuppose doctor developed blood test certain rare disease (one every 10,000 people disease). careful extensive evaluation blood test, doctor determined test’s sensitivity specificity.Sensitivity probability detecting disease actually . Note conditional probability.Specificity probability correctly identifying “disease” . , another conditional probability.See Figure 9.1 visual representation terms others related termed confusion matrix.\nFigure 9.1: table true results test results hypothetical disease. terminology included table. ideas important evaluating machine learning classification models.\nfact, test sensitivity 100% specificity 99.9%. Now suppose patient walks , doctor administers blood test, returns positive. probability patient actually disease?classic example probability misunderstood. Upon reading question, might guess answer question quite high. , nearly perfect test. exploring problem depth, find different result.","code":""},{"path":"CONDPROB.html","id":"approach-using-whole-numbers","chapter":"9 Conditional Probability","heading":"9.4.1 Approach using whole numbers","text":"Without going directly formulaic expression , let’s consider collection 100,000 randomly selected people. know?Based prevalence disease (one every 10,000 people disease), know 10 disease.Based prevalence disease (one every 10,000 people disease), know 10 disease.test perfectly sensitive. Thus, 10 people disease, test positive.test perfectly sensitive. Thus, 10 people disease, test positive.test specificity 99.9%. 99,990 don’t disease, \\(0.999*99990\\approx 99890\\) test negative. remaining 100 test positive.test specificity 99.9%. 99,990 don’t disease, \\(0.999*99990\\approx 99890\\) test negative. remaining 100 test positive.Thus, 100,000 randomly selected people, 110 test positive. 110, 10 actually disease. Thus, probability someone disease given ’ve tested positive actually around \\(10/110 = 0.0909\\).","code":""},{"path":"CONDPROB.html","id":"mathematical-approach","chapter":"9 Conditional Probability","heading":"9.4.2 Mathematical approach","text":"Now let’s put context Bayes’ Rule stated . First, let’s define events. Let \\(D\\) event someone disease. Thus, \\(D'\\) event someone disease. Similarly, let \\(T\\) event someone tested positive. already know?\n\\[\n\\mbox{P}(D) = 0.0001 \\hspace{1cm} \\mbox{P}(D')=0.9999\n\\]\n\\[\n\\mbox{P}(T|D)= 1 \\hspace{1cm} \\mbox{P}(T'|D)=0\n\\]\n\\[\n\\mbox{P}(T'|D')=0.999 \\hspace{1cm} \\mbox{P}(T|D') = 0.001\n\\]looking \\(\\mbox{P}(D|T)\\), probability someone disease, given /tested positive. definition conditional probability,\n\\[\n\\mbox{P}(D|T)=\\frac{\\mbox{P}(D \\cap T)}{\\mbox{P}(T)}\n\\]numerator can rewritten, utilizing definition conditional probability: \\(\\mbox{P}(D\\cap T)=\\mbox{P}(T|D)\\mbox{P}(D)\\).denominator can rewritten using Law Total Probability (discussed ) definition conditional probability: \\(\\mbox{P}(T)=\\mbox{P}(T\\cap D) + \\mbox{P}(T \\cap D') = \\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')\\). , putting together,\n\\[\n\\mbox{P}(D|T)=\\frac{\\mbox{P}(T|D)\\mbox{P}(D)}{\\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D')\\mbox{P}(D')}\n\\]Now stated problem context quantities know:\n\\[\n\\mbox{P}(D|T)=\\frac{1\\cdot 0.0001}{1\\cdot 0.0001 + 0.001\\cdot 0.9999} = 0.0909\n\\]Note original statement Bayes’ Rule, considered \\(n\\) partitions, \\(B_1, B_2,...,B_n\\). example, two: \\(D\\) \\(D'\\).","code":""},{"path":"CONDPROB.html","id":"simulation","chapter":"9 Conditional Probability","heading":"9.4.3 Simulation","text":"simulation, can think flipping coin. First let’s assume pulling 1,000,000 people population. probability one person disease 0.0001. use rflip() get 1,000,000 people designate disease disease.case 100 people disease. Now let’s find positive test results. 100 disease, test positive. without disease, 0.001 probability testing positive.Now 959 tested positive. Thus probability disease given positive test result approximately:","code":"\nset.seed(43)\nresults <- rflip(1000000,0.0001,summarize = TRUE)\nresults##       n heads  tails  prob\n## 1 1e+06   100 999900 1e-04\nrflip(as.numeric(results['tails']),prob=.001,summarize = TRUE)##        n heads  tails  prob\n## 1 999900   959 998941 0.001\n100/(100+959)## [1] 0.09442871"},{"path":"CONDPROB.html","id":"homework-problems-8","chapter":"9 Conditional Probability","heading":"9.5 Homework Problems","text":"Consider: \\(\\), \\(B\\) \\(C\\) events \\(\\mbox{P}()=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\), \\(\\mbox{P}(\\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(\\cap C)=0.1\\), \\(\\mbox{P}(\\cap B \\cap C)=0.05\\).\\(\\) \\(B\\) independent?\\(B\\) \\(C\\) independent?Suppose biased coin (probability flip heads 0.6). flip coin twice. Assume coin memoryless (flips independent one another).probability second flip results heads?probability second flip results heads, given first also resulted heads?probability flips result heads?probability exactly one coin flip results heads?Now assume flip coin five times. probability result 5 heads?probability result exactly 2 heads (5 flips)?Suppose three assistants working company: Moe, Larry Curly. three assist filing process. one filing assistant needed time. Moe assists 60% time, Larry assists 30% time Curly assists remaining 10% time. Occasionally, make errors (misfiles); Moe misfile rate 0.01, Larry misfile rate 0.025, Curly rate 0.05. Suppose misfile discovered, unknown schedule occurred. likely committed misfile? Calculate probabilities three assistants.Suppose three assistants working company: Moe, Larry Curly. three assist filing process. one filing assistant needed time. Moe assists 60% time, Larry assists 30% time Curly assists remaining 10% time. Occasionally, make errors (misfiles); Moe misfile rate 0.01, Larry misfile rate 0.025, Curly rate 0.05. Suppose misfile discovered, unknown schedule occurred. likely committed misfile? Calculate probabilities three assistants.playing game two coins. One coin fair comes heads 80% time. One coin flipped 3 times result three heads, probability coin flipped fair coin? need make assumption probability either coin selected.playing game two coins. One coin fair comes heads 80% time. One coin flipped 3 times result three heads, probability coin flipped fair coin? need make assumption probability either coin selected.Use Bayes formula solve problem.Use simulation solve problem.","code":""},{"path":"RANDVAR.html","id":"RANDVAR","chapter":"10 Random Variables","heading":"10 Random Variables","text":"","code":""},{"path":"RANDVAR.html","id":"objectives-9","chapter":"10 Random Variables","heading":"10.1 Objectives","text":"Define use properly context new terminology.Given discrete random variable, obtain pmf cdf, use obtain probabilities events.Simulate random variable discrete distribution.Find moments discrete random variable.Find expected value linear transformation random variable.","code":""},{"path":"RANDVAR.html","id":"random-variables","chapter":"10 Random Variables","heading":"10.2 Random variables","text":"already discussed random experiments. also discussed \\(S\\), sample space experiment. random variable essentially maps events sample space real number line. formal definition: random variable \\(X\\) function \\(X: S\\rightarrow \\mathbb{R}\\) assigns exactly one number outcome experiment.Example:\nSuppose flip coin three times. sample space, \\(S\\), experiment \n\\[\nS=\\{\\mbox{HHH}, \\mbox{HHT}, \\mbox{HTH}, \\mbox{HTT}, \\mbox{THH}, \\mbox{THT}, \\mbox{TTH}, \\mbox{TTT}\\}\n\\]Let random variable \\(X\\) number heads three coin flips. Whenever introduced new random variable, take moment think possible values can \\(X\\) take? tossing coin 3 times, can get heads, one head, two heads three heads. random variable \\(X\\) assigns outcome experiment one values. Visually:\n\\[\nS=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\}\n\\]sample space \\(X\\), sometimes referred support, list numerical values \\(X\\) can take.\n\\[\nS_X=\\{0,1,2,3\\}\n\\]sample space \\(X\\) countable list numbers, consider \\(X\\) discrete random variable (later).","code":""},{"path":"RANDVAR.html","id":"how-does-this-help","chapter":"10 Random Variables","heading":"10.2.1 How does this help?","text":"Sticking example, can now frame problem interest context random variable \\(X\\). example, suppose wanted know probability least two heads. Without random variable, write :\n\\[\n\\mbox{P}(\\mbox{least two heads})= \\mbox{P}(\\{\\mbox{HHH},\\mbox{HHT},\\mbox{HTH},\\mbox{THH}\\})\n\\]context random variable, simply becomes \\(\\mbox{P}(X\\geq 2)\\). may seem important case like , imagine flipping coin 50 times wanted know probability obtaining least 30 heads. unfeasible write possible ways obtain least 30 heads. much easier write \\(\\mbox{P}(X\\geq 30)\\) explore distribution \\(X\\).Essentially, random variable often helps us reduce complex random experiment simple variable easy characterize.","code":""},{"path":"RANDVAR.html","id":"discrete-vs-continuous","chapter":"10 Random Variables","heading":"10.2.2 Discrete vs Continuous","text":"discrete random variable sample space consists countable set values. \\(X\\) example discrete random variable. Note “countable” necessarily mean “finite”. example, random variable Poisson distribution (topic later lesson) sample space \\(\\{0,1,2,...\\}\\). sample space unbounded, considered countably infinite, thus random variable considered discrete.continuous random variable sample space continuous interval. example, let \\(Y\\) random variable corresponding height randomly selected individual. \\(Y\\) continuous random variable person measure 68.1 inches, 68.2 inches, perhaps value . Note measure height, precision limited measuring device, technically “discretizing” height. However, even cases, typically consider height continuous random variable.mixed random variable exactly sounds like. sample space discrete continuous. thing occur? Consider experiment person rolls standard six-sided die. lands anything one, result die roll recorded. lands one, person spins wheel, angle degrees resulting spin, divided 360, recorded. random variable \\(Z\\) number recorded experiment, sample space \\(Z\\) \\([0,1] \\cup \\{2,3,4,5,6\\}\\). spending much time mixed random variables. However occur practice, consider job analyzing bomb error data. bomb hits within certain radius, error 0. Otherwise measured radial direction. data mixed.","code":""},{"path":"RANDVAR.html","id":"discrete-distribution-functions","chapter":"10 Random Variables","heading":"10.2.3 Discrete distribution functions","text":"defined random variable, need way describe behavior use probabilities purpose.Distribution functions describe behavior random variables. can use functions determine probability random variable takes value range values. discrete random variables, two distribution functions interest: probability mass function (pmf) cumulative distribution function (cdf).","code":""},{"path":"RANDVAR.html","id":"probability-mass-function","chapter":"10 Random Variables","heading":"10.2.4 Probability mass function","text":"Let \\(X\\) discrete random variable. probability mass function (pmf) \\(X\\), given \\(f_X(x)\\), function assigns probability possible outcome \\(X\\).\n\\[\nf_X(x)=\\mbox{P}(X=x)\n\\]Note pmf function. Functions input output. input pmf real number. output pmf probability random variable takes inputted value. pmf must follow axioms probability described Probability Rules lesson. Primarily,\\(x \\\\mathbb{R}\\), \\(0 \\leq f_X(x) \\leq 1\\).\\(x \\\\mathbb{R}\\), \\(0 \\leq f_X(x) \\leq 1\\).\\(\\sum_x f_X(x) = 1\\), \\(x\\) index sum simply denotes summing across entire domain support \\(X\\).\\(\\sum_x f_X(x) = 1\\), \\(x\\) index sum simply denotes summing across entire domain support \\(X\\).Example:\nRecall example . flip coin three times let \\(X\\) number heads three coin flips. know \\(X\\) can take values 0, 1, 2 3. probability take three values? example, listed possible outcomes experiment denoted value \\(X\\) corresponds outcome.\n\\[\nS=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\}\n\\]eight outcomes equally likely (probability \\(\\frac{1}{8}\\)). Thus, building pmf \\(X\\) becomes matter counting number outcomes associated possible value \\(X\\):\n\\[\nf_X(x)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{1}{8}, & x=0 \\\\\n\\frac{3}{8}, & x=1 \\\\\n\\frac{3}{8}, & x=2 \\\\\n\\frac{1}{8}, & x=3 \\\\\n0, & \\mbox{otherwise} \\end{array} \\right .\n\\]Note function specifies probability \\(X\\) takes four values sample space (0, 1, 2, 3). Also, specifies probability \\(X\\) takes value 0.Graphically, pmf terribly interesting. pmf 0 values \\(X\\) except 0, 1, 2 3, Figure 10.1.\nFigure 10.1: Probability Mass Function \\(X\\) Coin Flip Example\nExample:\ncan use pmf answer questions experiment. example, consider context. probability flip least one heads? can write context \\(X\\):\n\\[\n\\mbox{P}(\\mbox{least one heads})=\\mbox{P}(X\\geq 1)=\\mbox{P}(X=1)+\\mbox{P}(X=2)+\\mbox{P}(X=3)=\\frac{3}{8} + \\frac{3}{8}+\\frac{1}{8}=\\frac{7}{8}\n\\]Alternatively, can recognize \\(\\mbox{P}(X\\geq 1)=1-\\mbox{P}(X=0)=1-\\frac{1}{8}=\\frac{7}{8}\\).","code":""},{"path":"RANDVAR.html","id":"cumulative-distribution-function","chapter":"10 Random Variables","heading":"10.2.5 Cumulative distribution function","text":"Let \\(X\\) discrete random variable. cumulative distribution function (cdf) \\(X\\), given \\(F_X(x)\\), function assigns value \\(X\\) probability \\(X\\) takes value lower:\n\\[\nF_X(x)=\\mbox{P}(X\\leq x)\n\\], note cdf function input output. input cdf real number. output cdf probability random variable takes inputted value less.know pmf, can obtain cdf:\n\\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\sum_{y\\leq x} f_X(y)\n\\]Like pmf, cdf must 0 1. Also, since pmf always non-negative, cdf must non-decreasing.Example:\nObtain plot cdf \\(X\\) previous example.\n\\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, & x <0 \\\\\n\\frac{1}{8}, & 0\\leq x < 1 \\\\\n\\frac{4}{8}, & 1\\leq x < 2 \\\\\n\\frac{7}{8}, & 2\\leq x < 3 \\\\\n1, & x\\geq 3 \\end{array}\\right .\n\\]Visually, cdf discrete random variable stairstep appearance. example, cdf takes value 0 \\(X=0\\), point cdf increases 1/8. stays value \\(X=1\\), . beyond \\(X=3\\), cdf equal 1, Figure 10.2.\nFigure 10.2: Cumulative Distribution Function \\(X\\) Coin Flip Example\n","code":""},{"path":"RANDVAR.html","id":"simulating-random-variables","chapter":"10 Random Variables","heading":"10.2.6 Simulating random variables","text":"can simulate values random variable using cdf, use similar idea continuous random variables. Since range cdf interval \\([0,1]\\) generate random number interval use inverse function find value random variable. pseudo code :\n1) Generate random number, \\(U\\).\n2) Find index \\(k\\) \\(\\sum_{j=1}^{k-1}f_X(x_{j}) \\leq U < \\sum_{j=1}^{k}f_X(x_{j})\\) \\(F_x(k-1) \\leq U < F_{x}(k)\\).Example:\nSimulate random variable number heads flipping coin three times.First create pmf.get cdf cumulative sum.Next, generate random number 0 1.Finally, find value random variable. step separately first can understand code.Let’s make function.Now let’s generate 10000 values random variable.bad approximation.","code":"\npmf <- c(1/8,3/8,3/8,1/8)\nvalues <- c(0,1,2,3)\npmf## [1] 0.125 0.375 0.375 0.125\ncdf <- cumsum(pmf)\ncdf## [1] 0.125 0.500 0.875 1.000\nset.seed(1153)\nran_num <- runif(1)\nran_num## [1] 0.7381891\nran_num < cdf## [1] FALSE FALSE  TRUE  TRUE\nwhich(ran_num < cdf)## [1] 3 4\nwhich(ran_num < cdf)[1]## [1] 3\nvalues[which(ran_num < cdf)[1]]## [1] 2\nsimple_rv <- function(values,cdf){\nran_num <- runif(1)\nreturn(values[which(ran_num < cdf)[1]])\n}\nresults <- do(10000)*simple_rv(values,cdf)\ninspect(results)## \n## quantitative variables:  \n##           name   class min Q1 median Q3 max   mean       sd     n missing\n## ...1 simple_rv numeric   0  1      2  2   3 1.5048 0.860727 10000       0\ntally(~simple_rv,data=results,format=\"proportion\")## simple_rv\n##      0      1      2      3 \n## 0.1207 0.3785 0.3761 0.1247"},{"path":"RANDVAR.html","id":"moments","chapter":"10 Random Variables","heading":"10.3 Moments","text":"Distribution functions excellent characterizations random variables. pmf cdf tell exactly often random variables takes particular values. However, distribution functions often lot information. Sometimes, may want describe random variable \\(X\\) single value small set values. example, may want know average measure center \\(X\\). also may want know measure spread \\(X\\). Moments values summarize random variables single numbers. Since dealing population, moments population values summary statistics used first block material.","code":""},{"path":"RANDVAR.html","id":"expectation","chapter":"10 Random Variables","heading":"10.3.1 Expectation","text":"point, define term expectation. Let \\(g(X)\\) function discrete random variable \\(X\\). expected value \\(g(X)\\) given :\n\\[\n\\mbox{E}(g(X))=\\sum_x g(x) \\cdot f_X(x)\n\\]","code":""},{"path":"RANDVAR.html","id":"mean","chapter":"10 Random Variables","heading":"10.3.2 Mean","text":"common moments used describe random variables mean variance. mean (often referred expected value \\(X\\)), simply average value random variable. denoted \\(\\mu_X\\) \\(\\mbox{E}(X)\\). discrete case, mean found :\n\\[\n\\mu_X=\\mbox{E}(X)=\\sum_x x \\cdot f_X(x)\n\\]mean also known first moment \\(X\\) around origin. weighted sum weight probability. outcome equally likely, expected value just average values random variable since weight reciprocal number values.Example:\nFind expected value (mean) \\(X\\): number heads three flips fair coin.\n\\[\n\\mbox{E}(X)=\\sum_x x\\cdot f_X(x) = 0*\\frac{1}{8} + 1*\\frac{3}{8} + 2*\\frac{3}{8} + 3*\\frac{1}{8}=1.5\n\\]using \\(\\mu\\) population parameter.simulation , can find mean estimate expected value. really statistic since simulation data population thus variance sample sample.","code":"\nmean(~simple_rv,data=results)## [1] 1.5048"},{"path":"RANDVAR.html","id":"variance","chapter":"10 Random Variables","heading":"10.3.3 Variance","text":"Variance measure spread random variable. variance \\(X\\) denoted \\(\\sigma^2_X\\) \\(\\mbox{Var}(X)\\). equivalent average squared deviation mean:\n\\[\n\\sigma^2_X=\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\n\\]discrete case, can evaluated :\n\\[\n\\mbox{E}[(X-\\mu_X)^2]=\\sum_x (x-\\mu_X)^2f_X(x)\n\\]Variance also known second moment \\(X\\) around mean.square root \\(\\mbox{Var}(X)\\) denoted \\(\\sigma_X\\), standard deviation \\(X\\). standard deviation often reported measured units \\(X\\), variance measured squared units thus harder interpret.Example:\nFind variance \\(X\\): number heads three flips fair coin.\\[\n\\mbox{Var}(X)=\\sum_x (x-\\mu_X)^2 \\cdot f_X(x)\n\\]\\[\n= (0-1.5)^2 \\times \\frac{1}{8} + (1-1.5)^2 \\times \\frac{3}{8}+(2-1.5)^2 \\times \\frac{3}{8} + (3-1.5)^2\\times \\frac{1}{8}\n\\]\nR :variance \\(X\\) 0.75.can find variance simulation R uses sample variance population variance. need multiply \\(\\frac{n-1}{n}\\)","code":"\n(0-1.5)^2*1/8 + (1-1.5)^2*3/8 + (2-1.5)^2*3/8 + (3-1.5)^2*1/8## [1] 0.75\nvar(~simple_rv,data=results)*(10000-1)/10000## [1] 0.740777"},{"path":"RANDVAR.html","id":"mean-and-variance-of-linear-transformations","chapter":"10 Random Variables","heading":"10.3.4 Mean and variance of Linear Transformations","text":"Lemma: Let \\(X\\) discrete random variable, let \\(\\) \\(b\\) constants. :\n\\[\n\\mbox{E}(aX+b)=\\mbox{E}(X)+b\n\\]\n\n\\[\n\\mbox{Var}(aX+b)=^2\\mbox{Var}(X)\n\\]proof left homework problem.","code":""},{"path":"RANDVAR.html","id":"homework-problems-9","chapter":"10 Random Variables","heading":"10.4 Homework Problems","text":"Suppose flipping fair coin, result single coin flip either heads tails. Let \\(X\\) random variable representing number flips first heads.\\(X\\) discrete continuous? domain, support, \\(X\\)?values expect \\(X\\) take? think average \\(X\\)? Don’t actually formal math, just think flipping regular coin, long take get first heads.Advanced: R, generate 10,000 observations \\(X\\). empirical, simulation, pmf? average value \\(X\\) based simulation? Create bar chart proportions. Note: Unlike example Notes, don’t pmf, simulate experiment using R find number flips first heads.Note: many ways . description one approach. assumes extremely unlikely go past 1000 flips.First, let’s sample replacement vector c(“H”,“T”), 1000 times replacement, use sample().First, let’s sample replacement vector c(“H”,“T”), 1000 times replacement, use sample().reading, use () logical argument find first occurrence heads.reading, use () logical argument find first occurrence heads.Find theoretical distribution, use math come closed solution pmf. Repeat Problem 1,except part d, different random variable, \\(Y\\): number coin flips fifth heads. Suppose data analyst large international airport. boss, head airport, dismayed airport received negative attention press inefficiencies sluggishness. staff meeting, boss gives week build report addressing “timeliness” airport. boss big hurry gives information guidance task.Prior building report, need conduct analysis. aid , create list least three random variables help address timeliness airport. random variables,Determine whether discrete continuous.Report domain.experimental unit?Explain random variable useful addressing timeliness airport.provide one example:Let \\(D\\) difference flight’s actual departure scheduled departure. continuous random variable, since time can measured fractions minutes. flight can early late, domain real number. experimental unit individual (non-canceled) flight. useful random variable average value \\(D\\) describe whether flights take time. also find often \\(D\\) exceeds 0 (implying late departure) often \\(D\\) exceeds 30 minutes, indicate “late” departure.Consider experiment rolling two fair six-sided dice. Let random variable \\(Y\\) absolute difference two numbers appear upon rolling dice.domain/support \\(Y\\)?values expect \\(Y\\) take? think average \\(Y\\)? Don’t actually formal math, just think experiment.Find probability mass function cumulative distribution function \\(Y\\).Find expected value variance \\(Y\\).Advanced: R, obtain 10,000 realizations \\(Y\\). words, simulate roll two fair dice, record absolute difference repeat 10,000 times. Construct frequency table results (percentage time get difference 0? difference 1? etc.) Find mean variance simulated sample \\(Y\\). close answers part d? Prove Lemma Notes: Let \\(X\\) discrete random variable, let \\(\\) \\(b\\) constants. Show \\(\\mbox{E}(aX + b)=\\mbox{E}(X)+b\\). saw \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show \\(\\mbox{Var}(X)\\) also equal \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\).","code":""},{"path":"CONRANDVAR.html","id":"CONRANDVAR","chapter":"11 Continuous Random Variables","heading":"11 Continuous Random Variables","text":"","code":""},{"path":"CONRANDVAR.html","id":"objectives-10","chapter":"11 Continuous Random Variables","heading":"11.1 Objectives","text":"Define properly use new terms include probability density function (pdf) cumulative distribution function (cdf) continuous random variables.Given continuous random variable, find probabilities using pdf /cdf.Find mean variance continuous random variable.","code":""},{"path":"CONRANDVAR.html","id":"continuous-random-variables","chapter":"11 Continuous Random Variables","heading":"11.2 Continuous random variables","text":"last lesson, introduced random variables, explored discrete random variables. lesson, move continuous random variables, properties, distribution functions, differ discrete random variables.Recall continuous random variable domain continuous interval (possibly group intervals). example, let \\(Y\\) random variable corresponding height randomly selected individual. measurement necessitate “discretizing” height degree, technically, height continuous random variable since person measure 67.3 inches 67.4 inches anything .","code":""},{"path":"CONRANDVAR.html","id":"continuous-distribution-functions","chapter":"11 Continuous Random Variables","heading":"11.2.1 Continuous distribution functions","text":"describe randomness continuous random variables? case discrete random variables, probability mass function (pmf) cumulative distribution function (cdf) used describe randomness. However, recall pmf function returns probability random variable takes inputted value. Due nature continuous random variables, probability continuous random variable takes one individual value technically 0. Thus, pmf apply continuous random variable.Rather, describe randomness continuous random variables probability density function (pdf) cumulative distribution function (cdf). Note cdf interpretation application discrete case.","code":""},{"path":"CONRANDVAR.html","id":"probability-density-function","chapter":"11 Continuous Random Variables","heading":"11.2.2 Probability density function","text":"Let \\(X\\) continuous random variable. probability density function (pdf) \\(X\\), given \\(f_X(x)\\) function describes behavior \\(X\\). important note continuous case, \\(f_X(x)\\neq \\mbox{P}(X=x)\\), probability \\(X\\) taking one individual value 0.pdf function. input pdf real number. output known density. pdf three main properties:\\(f_X(x)\\geq 0\\)\\(f_X(x)\\geq 0\\)\\(\\int_{S_X} f_X(x)\\mathop{}\\!\\mathrm{d}x = 1\\)\\(\\int_{S_X} f_X(x)\\mathop{}\\!\\mathrm{d}x = 1\\)\\(\\mbox{P}(X\\)=\\int_{x\\} f_X(x)\\mathop{}\\!\\mathrm{d}x\\) another way write \\(\\mbox{P}(\\leq X \\leq b)=\\int_{}^{b} f_X(x)\\mathop{}\\!\\mathrm{d}x\\)\\(\\mbox{P}(X\\)=\\int_{x\\} f_X(x)\\mathop{}\\!\\mathrm{d}x\\) another way write \\(\\mbox{P}(\\leq X \\leq b)=\\int_{}^{b} f_X(x)\\mathop{}\\!\\mathrm{d}x\\)Properties 2) 3) imply area underneath pdf represents probability. pdf non-negative function, negative values.","code":""},{"path":"CONRANDVAR.html","id":"cumulative-distribution-function-1","chapter":"11 Continuous Random Variables","heading":"11.2.3 Cumulative distribution function","text":"cumulative distribution function (cdf) continuous random variable interpretation discrete random variable. function. input cdf real number, output probability random variable takes value less equal inputted value. denoted \\(F\\) given :\n\\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\int_{-\\infty}^x f_x(t) \\mathop{}\\!\\mathrm{d}t\n\\]Example:\nLet \\(X\\) continuous random variable \\(f_X(x)=2x\\) \\(0 \\leq x \\leq 1\\). Verify \\(f\\) valid pdf. Find cdf \\(X\\). Also, find following probabilities: \\(\\mbox{P}(X<0.5)\\), \\(\\mbox{P}(X>0.5)\\), \\(\\mbox{P}(0.1\\leq X < 0.75)\\). Finally, find median \\(X\\).verify \\(f\\) valid pdf, simply note \\(f_X(x) \\geq 0\\) range \\(0 \\leq x \\leq 1\\). Also, note \\(\\int_0^1 2x \\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^1 = 1\\).Using R, findOr can use mosaicCalc package find anti-derivative. package installed, can use Packages tab RStudio type install.packages(\"mosaicCalc\") command prompt. Load library.\nFigure 11.1: pdf \\(X\\)\ncdf \\(X\\) found \n\\[\n\\int_0^x 2t \\mathop{}\\!\\mathrm{d}t = t^2\\bigg|_0^x = x^2\n\\]\nantiD found calculations .,\n\\[\nF_X(x)=\\left\\{ \\begin{array}{ll} 0, & x<0 \\\\ x^2, & 0\\leq x \\leq 1 \\\\ 1, & x>1 \\end{array}\\right.\n\\]plot cdf \\(X\\) shown Figure 11.2.\nFigure 11.2: cdf \\(X\\)\nProbabilities found either integrating pdf using cdf:\\(\\mbox{P}(X < 0.5)=\\mbox{P}(X\\leq 0.5)=F_X(0.5)=0.5^2=0.25\\). See Figure 11.3.\nFigure 11.3: Probability represented shaded area\n\\(\\mbox{P}(X > 0.5) = 1-\\mbox{P}(X\\leq 0.5)=1-0.25 = 0.75\\) See Figure 11.4.\nFigure 11.4: Probability represented shaded area\n\\(\\mbox{P}(0.1\\leq X < 0.75) = \\int_{0.1}^{0.75}2x\\mathop{}\\!\\mathrm{d}x = 0.75^2 - 0.1^2 = 0.5525\\) See Figure 11.5.Alternatively, \\(\\mbox{P}(0.1\\leq X < 0.75) = \\mbox{P}(X < 0.75) -\\mbox{P}(x \\leq 0.1) = F(0.75)-F(0.1)=0.75^2-0.1^2 =0.5525\\)Notice continuous random variable, loose use = sign. continuous random variable \\(\\mbox{P}(X=x)=0\\). get sloppy working discrete random variables.\nFigure 11.5: Probability represented shaded area\nmedian \\(X\\) value \\(x\\) \\(\\mbox{P}(X\\leq x)=0.5\\), area single point 0. simply solve \\(x^2=0.5\\) \\(x\\). Thus, median \\(X\\) \\(\\sqrt{0.5}=0.707\\).using R","code":"\nintegrate(function(x)2*x,0,1)## 1 with absolute error < 1.1e-14\nlibrary(mosaicCalc)\n(Fx<-antiD(2*x~x))## function (x, C = 0) \n## 1 * x^2 + C\nFx(1)-Fx(0)## [1] 1\nintegrate(function(x)2*x,.1,.75)## 0.5525 with absolute error < 6.1e-15\nFx(0.75)-Fx(0.1)## [1] 0.5525\nuniroot(function(x)(Fx(x)-.5),c(0,1))$root## [1] 0.7071067"},{"path":"CONRANDVAR.html","id":"simulation-1","chapter":"11 Continuous Random Variables","heading":"11.2.4 Simulation","text":"case discrete random variable, can simulate continuous random variable inverse cdf. range cdf \\([0,1]\\), generate random number interval apply inverse cdf obtain random variable. similar manner, continuous random variable, use following pseudo code:\n1. Generate random number interval \\([0,1]\\), \\(U\\).\n2. Find random variable \\(X\\) \\(F_{X}^{-1}(U)\\).\nR example, looks like following.Figure 11.6 density plot simulated density function.\nFigure 11.6: Density plot simulated random variable.\n","code":"\nsqrt(runif(1))## [1] 0.6137365\nresults <- do(10000)*sqrt(runif(1))\ninspect(results)## \n## quantitative variables:  \n##      name   class         min        Q1    median        Q3       max      mean\n## ...1 sqrt numeric 0.005321359 0.4977011 0.7084257 0.8656665 0.9999873 0.6669452\n##             sd     n missing\n## ...1 0.2358056 10000       0\nresults %>%\n  gf_density(~sqrt,xlab=\"X\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"X\",y=\"\")"},{"path":"CONRANDVAR.html","id":"moments-1","chapter":"11 Continuous Random Variables","heading":"11.3 Moments","text":"discrete random variables, moments can calculated summarize characteristics center spread. discrete case, expectation found multiplying possible value associated probability summing across domain (\\(\\mbox{E}(X)=\\sum_x x\\cdot f_X(x)\\)). continuous case, domain \\(X\\) consists infinite set values. calculus days, recall sum across infinite domain represented integral.Let \\(g(X)\\) function \\(X\\). expectation \\(g(X)\\) found :\n\\[\n\\mbox{E}(g(X)) = \\int_{S_X} g(x)f_X(x)\\mathop{}\\!\\mathrm{d}x\n\\]","code":""},{"path":"CONRANDVAR.html","id":"mean-and-variance","chapter":"11 Continuous Random Variables","heading":"11.3.1 Mean and variance","text":"Let \\(X\\) continuous random variable. mean \\(X\\), \\(\\mu_X\\), simply \\(\\mbox{E}(X)\\). Thus,\n\\[\n\\mbox{E}(X)=\\int_{S_X}x\\cdot f_X(x)\\mathop{}\\!\\mathrm{d}x\n\\]discrete case, variance \\(X\\) expected squared difference mean, \\(\\mbox{E}[(X-\\mu_X)^2]\\). Thus,\n\\[\n\\sigma^2_X = \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]= \\int_{S_X} (x-\\mu_X)^2\\cdot f_X(x) \\mathop{}\\!\\mathrm{d}x\n\\]Recall homework problem 6 last chapter. problem, showed \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\). Thus,\n\\[\n\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_{S_X} x^2\\cdot f_X(x)\\mathop{}\\!\\mathrm{d}x - \\mu_X^2\n\\]Example:\nConsider random variable \\(X\\) . Find mean variance \\(X\\).\n\\[\n\\mu_X= \\mbox{E}(X)=\\int_0^1 x\\cdot 2x\\mathop{}\\!\\mathrm{d}x = \\frac{2x^3}{3}\\bigg|_0^1 = \\frac{2}{3}=0.667\n\\]Side note: Since mean \\(X\\) smaller median \\(X\\), say \\(X\\) skewed left, negatively skewed.Using R.using antiD()Using simulation.\\[\n\\sigma^2_X = \\mbox{Var}(X)= \\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_0^1 x^2\\cdot 2x\\mathop{}\\!\\mathrm{d}x - \\left(\\frac{2}{3}\\right)^2 = \\frac{2x^4}{4}\\bigg|_0^1-\\frac{4}{9}=\\frac{1}{2}-\\frac{4}{9}=\\frac{1}{18}=0.056\n\\]orAnd finally, standard deviation \\(X\\) \\(\\sigma_X = \\sqrt{\\sigma^2_X}=\\sqrt{1/18}=0.236\\).","code":"\nintegrate(function(x)x*2*x,0,1)## 0.6666667 with absolute error < 7.4e-15\nEx<-antiD(2*x^2~x)\nEx(1)-Ex(0)## [1] 0.6666667\nmean(~sqrt,data=results)## [1] 0.6669452\nintegrate(function(x)x^2*2*x,0,1)$value-(2/3)^2## [1] 0.05555556\nVx<-antiD(x^2*2*x~x)\nVx(1)-Vx(0)-(2/3)^2## [1] 0.05555556\nvar(~sqrt,data=results)*9999/10000## [1] 0.05559873"},{"path":"CONRANDVAR.html","id":"homework-problems-10","chapter":"11 Continuous Random Variables","heading":"11.4 Homework Problems","text":"Let \\(X\\) continuous random variable domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\).Assume \\(f(x)\\) valid pdf. Find value \\(k\\).Plot pdf \\(X\\).Find plot cdf \\(X\\).Find \\(\\mbox{P}(X<1)\\).Find \\(\\mbox{P}(1.5<X\\leq 2.5)\\).Find 80th percentile \\(X\\) (value \\(x\\) 80% distribution left value).Find value \\(x\\) \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\).Find mean variance \\(X\\).Simulate 10000 values distribution plot density.Let \\(X\\) continuous random variable. Prove cdf \\(X\\), \\(F_X(x)\\) non-decreasing function. (Hint: show \\(< b\\), \\(F_X() \\leq F_X(b)\\).)","code":""},{"path":"DISCRETENAMED.html","id":"DISCRETENAMED","chapter":"12 Named Discrete Distributions","heading":"12 Named Discrete Distributions","text":"","code":""},{"path":"DISCRETENAMED.html","id":"objectives-11","chapter":"12 Named Discrete Distributions","heading":"12.1 Objectives","text":"Recognize setup use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) include parameters, assumptions, moments.Use R calculate probabilities quantiles involving random variables common discrete distributions.","code":""},{"path":"DISCRETENAMED.html","id":"named-distributions","chapter":"12 Named Discrete Distributions","heading":"12.2 Named distributions","text":"previous two lessons, introduced concept random variables, distribution functions, expectations. cases, nature experiment may yield random variables common distributions. cases, can rely easy--use distribution functions built-R functions order calculate probabilities quantiles.","code":""},{"path":"DISCRETENAMED.html","id":"discrete-uniform-distribution","chapter":"12 Named Discrete Distributions","heading":"12.2.1 Discrete uniform distribution","text":"first distribution discuss discrete uniform distribution. commonly used distribution, especially compared continuous counterpart. discrete random variable discrete uniform distribution probability evenly allocated value sample space. variable distribution parameters \\(\\) \\(b\\) representing minimum maximum sample space, respectively. (default, sample space assumed consist integers , means always case.)Example:\nRolling fair die example discrete uniform. side die equal probability.Let \\(X\\) discrete random variable uniform distribution. sample space consecutive integers, distribution denoted \\(X\\sim\\textsf{DUnif}(,b)\\). pmf \\(X\\) given :\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{b-+1}, & x \\\\{, +1,...,b\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]die:\\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6-1+1} = \\frac{1}{6}, & x \\\\{1, 2,...,6\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]expected value \\(X\\) found :\n\\[\n\\mbox{E}(X)=\\sum_{x=}^b x\\cdot\\frac{1}{b-+1}= \\frac{1}{b-+1} \\cdot \\sum_{x=}^b x=\\frac{1}{b-+1}\\cdot\\frac{b-+1}{2}\\cdot (+b) = \\frac{+b}{2}\n\\]sum consecutive integers common result discrete math, research information.variance \\(X\\) found : (derivation included)\n\\[\n\\mbox{Var}(X)=\\mbox{E}[(X-\\mbox{E}(X))^2]=\\frac{(b-+1)^2-1}{12}\n\\]Summarizing die:Let \\(X\\) result single roll fair die. report distribution \\(X\\), pmf, \\(\\mbox{E}(X)\\) \\(\\mbox{Var}(X)\\).sample space \\(X\\) \\(S_X=\\{1,2,3,4,5,6\\}\\). Since outcomes equally likely, \\(X\\) follows discrete uniform distribution \\(=1\\) \\(b=6\\). Thus,\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6}, & x \\\\{1,2,3,4,5,6\\} \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]Finally, \\(\\mbox{E}(X)=\\frac{1+6}{2}=3.5\\). Also, \\(\\mbox{Var}(X)=\\frac{(6-1+1)^2-1}{12}=\\frac{35}{12}=2.917\\).","code":""},{"path":"DISCRETENAMED.html","id":"simulating","chapter":"12 Named Discrete Distributions","heading":"12.2.2 Simulating","text":"simulate discrete uniform, use sample().Example:\nsimulate rolling die 4 times, use sample().Let’s roll 10,000 times findAgain reminder, multiply \\(\\frac{(10000-1)}{10000}\\) function var() calculating sample variance using \\(n-1\\) denominator need population variance.","code":"\nset.seed(61)\nsample(1:6,4,replace=TRUE)## [1] 4 2 2 1\nresults<-do(10000)*sample(1:6,1,replace=TRUE)\ntally(~sample,data=results,format=\"percent\")## sample\n##     1     2     3     4     5     6 \n## 16.40 16.46 16.83 17.15 16.92 16.24\nmean(~sample,data=results)## [1] 3.5045\nvar(~sample,data=results)*(10000-1)/10000## [1] 2.87598"},{"path":"DISCRETENAMED.html","id":"binomial-distribution","chapter":"12 Named Discrete Distributions","heading":"12.2.3 Binomial distribution","text":"binomial distribution extremely common, appears many situations. fact, already discussed several examples binomial distribution heavily involved.Consider experiment involving repeated independent trials binary process (two outcomes), trial, constant probability “success” (one outcomes arbitrary). random variable \\(X\\) represents number successes \\(n\\) independent trials, \\(X\\) said follow binomial distribution parameters \\(n\\) \\(p\\) (probability success trial).pmf \\(X\\) given :\n\\[\nf_X(x)=\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}\n\\]\\(x \\\\{0,1,...,n\\}\\) 0 otherwise.Let’s take moment dissect pmf. looking probability obtaining \\(x\\) successes \\(n\\) trials. \\(p^x\\) represents probability \\(x\\) successes, using multiplication rule independence assumption. term \\((1-p)^{n-x}\\) represents probability remainder trials failures. Finally, \\(n\\choose x\\) term represents number ways obtain \\(x\\) successes \\(n\\) trials. example, three ways obtain 1 success 3 trials (one success followed two failures; one success, one failure one success; two failures followed success).expected value binomially distributed random variable given \\(\\mbox{E}(X)=np\\) variance given \\(\\mbox{Var}(X)=np(1-p)\\).Example:\nLet \\(X\\) number heads 20 independent flips fair coin. Note binomial trials independent probability success, case heads, constant, two outcomes. Find distribution, mean variance \\(X\\). Find \\(\\mbox{P}(X=8)\\). Find \\(\\mbox{P}(X\\leq 8)\\).\\(X\\) binomial distribution \\(n=20\\) \\(p=0.5\\). pmf given :\n\\[\nf_X(x)=\\mbox{P}(X=x)={20 \\choose x}0.5^x (1-0.5)^{20-x}\n\\]Also, \\(\\mbox{E}(X)=20*0.5=10\\) \\(\\mbox{Var}(X)=20*0.5*0.5=5\\).find \\(\\mbox{P}(X=8)\\), can simply use pmf:\n\\[\n\\mbox{P}(X=8)=f_X(8)={20\\choose 8}0.5^8 (1-0.5)^{12}\n\\]find \\(\\mbox{P}(X\\leq 8)\\), need find cumulative probability:\n\\[\n\\mbox{P}(X\\leq 8)=\\sum_{x=0}^8 {20\\choose 8}0.5^x (1-0.5)^{20-x}\n\\]","code":"\nchoose(20,8)*0.5^8*(1-0.5)^12## [1] 0.1201344\nx<-0:8\nsum(choose(20,x)*0.5^x*(1-.5)^(20-x))## [1] 0.2517223"},{"path":"DISCRETENAMED.html","id":"software-functions","chapter":"12 Named Discrete Distributions","heading":"12.2.4 Software Functions","text":"One advantages using named distributions software packages built-functions compute probabilities quantiles common named distributions. course lesson, notice named distribution treated similarly R. four main functions tied distribution. binomial distribution, dbinom(), pbinom(), qbinom(), rbinom().dbinom(): function equivalent probability mass function. use find \\(\\mbox{P}(X=x)\\) \\(X\\sim \\textsf{Binom}(n,p)\\). function takes three inputs: x (value random variable), size (number trials, \\(n\\)), prob (probability success, \\(p\\)). ,\n\\[\n\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}=\\textsf{dbinom(x,n,p)}\n\\]pbinom(): function equivalent cumulative distribution function. use find \\(\\mbox{P}(X\\leq x)\\) \\(X\\sim \\textsf{Binom}(n,p)\\). function takes inputs dbinom() returns cumulative probability:\n\\[\n\\mbox{P}(X\\leq x)=\\sum_{k=0}^x{n\\choose{k}}p^k(1-p)^{n-k}=\\textsf{pbinom(x,n,p)}\n\\]qbinom(): inverse cumulative distribution function return percentile. function three inputs: p (probability), size prob. returns smallest value \\(x\\) \\(\\mbox{P}(X\\leq x) \\geq p\\).rbinom(): function used randomly generate values binomial distribution. takes three inputs: n (number values generate), size prob. returns vector containing randomly generated values.learn functions, type ? followed function console.Exercise:\nUse built-functions binomial distribution plot pmf \\(X\\) previous example. Also, use built-functions compute probabilities example.Figure 12.1\nFigure 12.1: pmf binomial random variable\n","code":"\ngf_dist(\"binom\",size=20,prob=.5) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\n###P(X=8)\ndbinom(8,20,0.5)## [1] 0.1201344\n###P(X<=8)\npbinom(8,20,0.5)## [1] 0.2517223\n## or \nsum(dbinom(0:8,20,0.5))## [1] 0.2517223"},{"path":"DISCRETENAMED.html","id":"poisson-distribution","chapter":"12 Named Discrete Distributions","heading":"12.2.5 Poisson distribution","text":"Poisson distribution common considering count arrival data. Consider random process events occur according rate time (think arrivals retail register). Often, events modeled Poisson process. Poisson process assumes consistent rate arrival memoryless arrival process (time next arrival independent time since last arrival). assume particular process Poisson process, two random variables take common named distributions. number arrivals specified amount time follows Poisson distribution. Also, amount time next arrival follows exponential distribution. defer discussion exponential distribution next lesson. random Poisson number occurrences interval fixed. discrete distribution. parameter \\(\\lambda\\) average number occurrences specific interval, note interval must specified random variable.Let \\(X\\) number arrivals length time, \\(T\\), arrivals occur according Poisson process average \\(\\lambda\\) arrivals length time \\(T\\). \\(X\\) follows Poisson distribution parameter \\(\\lambda\\):\n\\[\nX\\sim \\textsf{Poisson}(\\lambda)\n\\]pmf \\(X\\) given :\n\\[\nf_X(x)=\\mbox{P}(X=x)=\\frac{\\lambda^xe^{-\\lambda}}{x!}, \\hspace{0.5cm} x=0,1,2,...\n\\]One unique feature Poisson distribution \\(\\mbox{E}(X)=\\mbox{Var}(X)=\\lambda\\).Example:\nSuppose fleet vehicles arrive maintenance garage average rate 0.4 per day. Let’s assume vehicles arrive according Poisson process. Let \\(X\\) number vehicles arrive garage week (7 days). Notice time interval changed! random variable \\(X\\)? distribution (parameter) \\(X\\). \\(\\mbox{E}(X)\\) \\(\\mbox{Var}(X)\\)? Find \\(\\mbox{P}(X=0)\\), \\(\\mbox{P}(X\\leq 6)\\), \\(\\mbox{P}(X \\geq 2)\\), \\(\\mbox{P}(2 \\leq X \\leq 8)\\). Also, find median \\(X\\), 95th percentile \\(X\\) (value \\(x\\) \\(\\mbox{P}(X\\leq x)\\geq 0.95\\)). , plot pmf \\(X\\).Since vehicles arrive according Poisson process, probability question leads us define random variable \\(X\\) number vehicles arrive week.know \\(X\\sim \\textsf{Poisson}(\\lambda=0.4*7=2.8)\\). Thus, \\(\\mbox{E}(X)=\\mbox{Var}(X)=2.8\\).parameter average number vehicles arrive week.\\[\n\\mbox{P}(X=0)=\\frac{2.8^0 e^{-2.8}}{0!}=e^{-2.8}=0.061\n\\]Alternatively, can use built-R functions Poisson distribution:Note considering \\(\\mbox{P}(X\\geq 2)\\), recognize equivalent \\(1-\\mbox{P}(X\\leq 1)\\). can use ppois() find probability.considering \\(\\mbox{P}(2\\leq X \\leq 8)\\), need make sure formulate correctly. two possible methods:find median 95th percentiles, use qpois:Figure 12.2 plot pmf Poisson random variable.\nFigure 12.2: pmf Poisson random variable.\nFigure 12.3 cdf Poisson random variable Figure 12.2.\nFigure 12.3: cdf Poisson random variable Figure 12.2\n","code":"\n##P(X=0)\ndpois(0,2.8)## [1] 0.06081006\n##P(X<=6)\nppois(6,2.8)## [1] 0.9755894\n## or\nsum(dpois(0:6,2.8))## [1] 0.9755894\n##P(X>=2)=1-P(X<2)=1-P(X<=1)\n1-ppois(1,2.8)## [1] 0.7689218\n## or\nsum(dpois(2:1000,2.8))## [1] 0.7689218\n##P(2 <= X <= 8) = P(X <= 8)-P(X <= 1)\nppois(8,2.8)-ppois(1,2.8)## [1] 0.766489\n## or\nsum(dpois(2:8,2.8))## [1] 0.766489\nqpois(0.5,2.8)## [1] 3\nqpois(0.95,2.8)## [1] 6\ngf_dist(\"pois\",lambda=2.8) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\ngf_dist(\"pois\",lambda=2.8,kind=\"cdf\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"X\",y=\"P(X<=x)\")"},{"path":"DISCRETENAMED.html","id":"hypergeometric","chapter":"12 Named Discrete Distributions","heading":"12.2.6 Hypergeometric","text":"Consider experiment \\(k\\) objects selected larger, finite, group consisting \\(m\\) “successes” \\(n\\) “failures”. similar binomial process; , selecting successes failures. However, case, results effectively selected without replacement. random variable \\(X\\) represents number successes selected sample size \\(k\\), \\(X\\) follows hypergeometric distribution parameters \\(m\\), \\(n\\), \\(k\\). pmf \\(X\\) given :\\[\nf_X(x) = \\frac{{m \\choose{x}}{n \\choose{k-x}}}{{m+n \\choose{k}}}, \\qquad x = 0,1,...,m\n\\]Also, \\(\\mbox{E}(X)=\\frac{km}{m+n}\\) \\(\\mbox{Var}(X)=k\\frac{m}{m+n}\\frac{n}{m+n}\\frac{m+n-k}{m+n-1}\\)draw knowledge combinations, can see pmf makes sense.Example:\nSuppose bag contains 12 red chips 8 black chips. reach blindly randomly select 6 chips. probability select black chips? black chips? 2 5 black chips?First identify random variable help us problem. Let \\(X\\) number black chips selected randomly selecting 6 bag. \\(X\\sim \\textsf{HyperGeom}(8,12,6)\\). can use R find probabilities.First, plot pmf hypergeometric Figure 12.4.\nFigure 12.4: pmf hypergeometric random variable.\n","code":"\ngf_dist(\"hyper\",m=8,n=12,k=6) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\n##P(X=0)\ndhyper(0,8,12,6)## [1] 0.02383901\n##P(X=6)\ndhyper(6,8,12,6)## [1] 0.0007223942\n##P(2 <= X <=5)\nsum(dhyper(2:5,8,12,6))## [1] 0.8119711"},{"path":"DISCRETENAMED.html","id":"homework-problems-11","chapter":"12 Named Discrete Distributions","heading":"12.3 Homework Problems","text":"problems , 1) define random variable help answer question, 2) state distribution parameters random variable; 3) determine expected value variance random variable, 4) use random variable answer question.demonstrate using 1a 1b.T-6 training aircraft used UPT. Suppose training sortie, aircraft return maintenance-related failure rate 1 per 100 sorties.Find probability maintenance failures 15 sorties.\\(X\\): number maintenance failures 15 sorties.\\(X\\sim \\textsf{Bin}(n=15,p=0.01)\\)\\(\\mbox{E}(X)=15*0.01=0.15\\) \\(\\mbox{Var}(X)=15*0.01*0.99=0.1485\\).\\(\\mbox{P}(\\mbox{mainteance failures})=\\mbox{P}(X=0)={15\\choose 0}0.01^0(1-0.01)^{15}=0.99^{15}\\)probability makes sense, since expected value fairly low. , average, 0.15 failures occur every 15 trials, 0 failures common result. Graphically, pmf looks like Figure 12.5.\nFigure 12.5: pmf binomail Homework Problem 1a.\nFind probability least two maintenance failures 15 sorties.can use \\(X\\) . Now, looking \\(\\mbox{P}(X\\geq 2)\\). equivalent finding \\(1-\\mbox{P}(X\\leq 1)\\):Find probability least 30 successful (mx failures) sorties first failure.Find probability least 50 successful sorties third failure.given Saturday, suppose vehicles arrive USAFA North Gate according Poisson process rate 40 arrivals per hour.Find probability vehicles arrive 10 minutes.Find probability least 50 vehicles arrive hour.Find probability least 5 minutes pass next arrival.Suppose 12 male 7 female cadets classroom. select 5 completely random (without replacement).Find probability select female cadets.Find probability select 2 female cadets.","code":"\n0.99^15## [1] 0.8600584\n## or \ndbinom(0,15,0.01)## [1] 0.8600584\ngf_dist(\"binom\",size=15,prob=0.01) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"X\",y=\"P(X=x)\")\n## Directly\n1-(0.99^15 + 15*0.01*0.99^14)## [1] 0.009629773\n## or, using R\nsum(dbinom(2:15,15,0.01))## [1] 0.009629773\n## or\n1-sum(dbinom(0:1,15,0.01))## [1] 0.009629773\n## or\n1-pbinom(1,15,0.01)## [1] 0.009629773\n## or \npbinom(1,15,0.01,lower.tail = F)## [1] 0.009629773"},{"path":"CONTNNAMED.html","id":"CONTNNAMED","chapter":"13 Named Continuous Distributions","heading":"13 Named Continuous Distributions","text":"","code":""},{"path":"CONTNNAMED.html","id":"objectives-12","chapter":"13 Named Continuous Distributions","heading":"13.1 Objectives","text":"Recognize use common continuous distributions (Uniform, Exponential, Gamma, Normal, Weibull, Beta), identify parameters, find moments.Use R calculate probabilities quantiles involving random variables common continuous distributions.Understand relationship Poisson process Poisson & Exponential distributions.Know apply use memoryless property.","code":""},{"path":"CONTNNAMED.html","id":"continuous-distributions","chapter":"13 Named Continuous Distributions","heading":"13.2 Continuous distributions","text":"lesson explore continuous distributions. means work probability density functions use find probabilities. Thus must integrate, either numerically, graphically, mathematically. cumulative distribution function also play important role lesson.many distributions ones lesson common set learn use others future.","code":""},{"path":"CONTNNAMED.html","id":"uniform-distribution","chapter":"13 Named Continuous Distributions","heading":"13.2.1 Uniform distribution","text":"first continuous distribution discuss uniform distribution. default, refer uniform distribution, referring continuous version. referring discrete version, use full term “discrete uniform distribution.”continuous random variable uniform distribution probability density constant, uniform. parameters distribution \\(\\) \\(b\\), representing minimum maximum sample space. distribution commonly denoted \\(U(,b)\\).Let \\(X\\) continuous random variable uniform distribution. denoted \\(X\\sim \\textsf{Unif}(,b)\\). pdf \\(X\\) given :\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll} \\frac{1}{b-}, & \\leq x \\leq b \\\\ 0, & \\mbox{otherwise} \\end{array}\\right.\n\\]mean \\(X\\) \\(\\mbox{E}(X)=\\frac{+b}{2}\\) variance \\(\\mbox{Var}(X)=\\frac{(b-)^2}{12}\\). derivation mean left exercises.common uniform distribution \\(U(0,1)\\) already used several times book. , notice Figure 13.1 plot pdf constant uniform value.\nFigure 13.1: pdf Uniform random variable.\ncheck proper pdf, values must non-negative total probability must 1. R function probability density start letter d short descriptor distribution. uniform use dunif().","code":"\ngf_dist(\"unif\",title=\"Pdf of Uniform random variable\",ylab=\"f(x)\") %>%\n  gf_theme(theme_bw())\nintegrate(function(x)dunif(x),0,1)## 1 with absolute error < 1.1e-14"},{"path":"CONTNNAMED.html","id":"exponential-distribution","chapter":"13 Named Continuous Distributions","heading":"13.2.2 Exponential distribution","text":"Recall lesson named discrete distributions, discussed Poisson process. arrivals follow Poisson process, know number arrivals specified amount time follows Poisson distribution, time next arrival follows exponential distribution. Poisson distribution, number arrivals random interval fixed. exponential distribution change , interval random arrivals fixed 1. subtle point worth time make sure understand.Let \\(X\\) number arrivals time interval \\(T\\), arrivals occur according Poisson process average \\(\\lambda\\) arrivals per unit time interval. previous lesson, know \\(X\\sim \\textsf{Poisson}(\\lambda T)\\). Now let \\(Y\\) time next arrival. \\(Y\\) follows exponential distribution parameter \\(\\lambda\\) units inverse base time:\\[\nY \\sim \\textsf{Expon}(\\lambda)\n\\]Note \\(\\lambda\\): One point confusion involving parameters Poisson exponential distributions. parameter Poisson distribution (usually denoted \\(\\lambda\\)) represents average number arrivals whatever amount time specified random variable. case exponential distribution, parameter (also denoted \\(\\lambda\\)) represents average number arrivals per unit time. example, suppose arrivals follow Poisson process average 10 arrivals per day. \\(X\\), number arrivals 5 days, follows Poisson distribution parameter \\(\\lambda=50\\), since average number arrivals amount time specified \\(X\\). Meanwhile, \\(Y\\), time days next arrival, follows exponential distribution parameter \\(\\lambda=10\\) (average number arrivals per day).pdf \\(Y\\) given :\n\\[\nf_Y(y)=\\lambda e^{-\\lambda y}, \\hspace{0.3cm} y>0\n\\]mean variance \\(Y\\) : \\(\\mbox{E}(Y)=\\frac{1}{\\lambda}\\) \\(\\mbox{Var}(Y)=\\frac{1}{\\lambda^2}\\). able derive results require integration parts can lengthy algebraic exercises.Example:\nSuppose local retail store, customers arrive checkout counter according Poisson process average one arrival every three minutes. Let \\(Y\\) time (minutes) next customer arrives counter. distribution (parameter) \\(Y\\)? \\(\\mbox{E}(Y)\\) \\(\\mbox{Var}(Y)\\)? Find \\(\\mbox{P}(Y>5)\\), \\(\\mbox{P}(Y\\leq 3)\\), \\(\\mbox{P}(1 \\leq Y < 5)\\)? Also, find median 95th percentile \\(Y\\). Finally, plot pdf \\(Y\\).Since one arrival shows every three minutes, average number arrivals per unit time 1/3 arrival per minute. Thus, \\(Y\\sim \\textsf{Expon}(\\lambda=1/3)\\). means \\(\\mbox{E}(Y)=3\\) \\(\\mbox{Var}(Y)=9\\).find \\(\\mbox{P}(Y>5)\\), integrate pdf \\(Y\\):\n\\[\n\\mbox{P}(Y>5)=\\int_5^\\infty \\frac{1}{3}e^{-\\frac{1}{3}y}\\mathop{}\\!\\mathrm{d}y = \\lim_{\\+\\infty}\\int_5^\\frac{1}{3}e^{-\\frac{1}{3}y}\\mathop{}\\!\\mathrm{d}y = \\]\\[\\lim_{\\+\\infty} -e^{-\\frac{1}{3}y}\\bigg|_5^=\\lim_{\\+\\infty} -e^{-\\frac{}{3}}-(-e^{-\\frac{5}{3}})= 0 + 0.189 = 0.189\n\\]Alternatively, use R:using integrate()remaining probabilities, use R:median \\(y\\) \\(\\mbox{P}(Y\\leq y)=0.5\\). can find solving following \\(y\\):\n\\[\n\\int_0^y \\frac{1}{3}e^{-\\frac{1}{3}y}\\mathop{}\\!\\mathrm{d}y = 0.5\n\\]Alternatively, can use qexp R:\nFigure 13.2: pdf exponential random varible \\(Y\\)\nFigure 13.2 mean median, know exponential distribution skewed right.","code":"\n##Prob(Y>5)=1-Prob(Y<=5)\n1-pexp(5,1/3)## [1] 0.1888756\nintegrate(function(x)1/3*exp(-1/3*x),5,Inf)## 0.1888756 with absolute error < 8.5e-05\n##Prob(Y<=3)\npexp(3,1/3)## [1] 0.6321206\n##Prob(1<=Y<5)\npexp(5,1/3)-pexp(1,1/3)## [1] 0.5276557\n##median\nqexp(0.5,1/3)## [1] 2.079442\n##95th percentile\nqexp(0.95,1/3)## [1] 8.987197"},{"path":"CONTNNAMED.html","id":"memoryless-property","chapter":"13 Named Continuous Distributions","heading":"13.2.3 Memoryless property","text":"Poisson process known memoryless property. Essentially, means time next arrival independent time since last arrival. Thus, probability arrival within next 5 minutes regardless whether arrival just occurred arrival occurred long time.show let’s consider random variable \\(Y\\) ( time next arrival minutes) \\(Y\\sim\\textsf{Expon}(\\lambda)\\). show , given least \\(t\\) minutes since last arrival, probability wait least \\(y\\) additional minutes equal marginal probability wait \\(y\\) additional minutes.First, note cdf \\(Y\\), \\(F_Y(y)=\\mbox{P}(Y\\leq y)=1-e^{-\\lambda y}\\), able derive . ,\n\\[\n\\mbox{P}(Y\\geq y+t|Y\\geq t) = \\frac{\\mbox{P}(Y\\geq y+t \\cap Y\\geq t)}{\\mbox{P}(Y\\geq t)}=\\frac{\\mbox{P}(Y\\geq y +t)}{\\mbox{P}(Y\\geq t)} = \\frac{1-(1-e^{-(y+t)\\lambda})}{1-(1-e^{-t\\lambda})}\n\\]\n\\[\n=\\frac{e^{-\\lambda y }e^{-\\lambda t}}{e^{-\\lambda t }}=e^{-\\lambda y} = 1-(1-e^{-\\lambda y})=\\mbox{P}(Y\\geq y).\n\\blacksquare\n\\]Let’s simulate values Poisson. Poisson often used modeling customer service situations service Chipotle. However, people mistaken idea arrivals equally spaced. fact, arrivals come clusters bunches. Maybe root common expression, “Bad news comes threes”?\nFigure 13.3: Simulations Poisson random variable.\nFigure 13.3, number events box \\(X\\sim \\textsf{Poisson}(\\lambda = 5)\\). can see, boxes 5 less 5 average number arrivals. Also note spacing equal. 8 different runs just repeated simulations process. can see spacing clusters run.","code":""},{"path":"CONTNNAMED.html","id":"gamma-distribution","chapter":"13 Named Continuous Distributions","heading":"13.2.4 Gamma distribution","text":"gamma distribution generalization exponential distribution. exponential distribution, parameter \\(\\lambda\\) sometimes referred rate parameter. gamma distribution sometimes used model wait times (exponential distribution), cases without memoryless property. gamma distribution two parameters, rate shape. texts, scale (inverse rate) used alternative parameter rate.Suppose \\(X\\) random variable gamma distribution shape parameter \\(\\alpha\\) rate parameter \\(\\lambda\\):\n\\[\nX \\sim \\textsf{Gamma}(\\alpha,\\lambda)\n\\]\\(X\\) following pdf:\n\\[\nf_X(x)=\\frac{\\lambda^\\alpha}{\\Gamma (\\alpha)}x^{\\alpha-1}e^{-\\lambda x}, \\hspace{0.3cm} x>0\n\\]0 otherwise. mean variance \\(X\\) \\(\\mbox{E}(X)=\\frac{\\alpha}{\\lambda}\\) \\(\\mbox{Var}(X)=\\frac{\\alpha}{\\lambda^2}\\). Looking pdf, mean variance, one can easily see \\(\\alpha=1\\), resulting distribution equivalent \\(\\textsf{Expon}(\\lambda)\\).","code":""},{"path":"CONTNNAMED.html","id":"gamma-function","chapter":"13 Named Continuous Distributions","heading":"13.2.4.1 Gamma function","text":"may little background Gamma function (\\(\\Gamma (\\alpha)\\)). different gamma distribution. gamma function simply function defined :\n\\[\n\\Gamma (\\alpha)=\\int_0^\\infty t^{\\alpha-1}e^{-t}\\mathop{}\\!\\mathrm{d}t\n\\]important properties gamma function. Notably, \\(\\Gamma (\\alpha)=(\\alpha-1)\\Gamma (\\alpha -1)\\), \\(\\alpha\\) non-negative integer, \\(\\Gamma(\\alpha)=(\\alpha-1)!\\).Suppose \\(X \\sim \\textsf{Gamma}(\\alpha,\\lambda)\\). pdf \\(X\\) various values \\(\\alpha\\) \\(\\lambda\\) shown Figure 13.4.\nFigure 13.4: pdf Gamma various values alpha lambda\nExample:\nLet \\(X \\sim \\textsf{Gamma}(\\alpha=5,\\lambda=1)\\). Find mean variance \\(X\\). Also, compute \\(\\mbox{P}(X\\leq 2)\\) \\(\\mbox{P}(1\\leq X < 8)\\). Find median 95th percentile \\(X\\).mean variance \\(X\\) \\(\\mbox{E}(X)=5\\) \\(\\mbox{Var}(X)=5\\). find probabilities quantiles, integration difficult, ’s best use built-R functions:","code":"\n## Prob(X<=2)\npgamma(2,5,1)## [1] 0.05265302\n##Prob(1 <= X < 8)\npgamma(8,5,1)-pgamma(1,5,1)## [1] 0.8967078\n## median\nqgamma(0.5,5,1)## [1] 4.670909\n## 95th percentile\nqgamma(0.95,5,1)## [1] 9.153519"},{"path":"CONTNNAMED.html","id":"weibull-distribution","chapter":"13 Named Continuous Distributions","heading":"13.2.5 Weibull distribution","text":"Another common distribution used modeling Weibull distribution. Like gamma, Weibull distribution generalization exponential distribution meant model wait times. random variable Weibull distribution parameters \\(\\alpha\\) \\(\\beta\\). R, referred shape scale respectively. Note resources, represented \\(k\\) \\(\\lambda\\) even \\(k\\) \\(\\theta\\).\nLet \\(X \\sim \\textsf{Weibull}(\\alpha,\\beta)\\). pdf \\(X\\) given :\n\\[\nf_X(x)=\\frac{\\alpha}{\\beta} \\left(\\frac{x}{\\beta}\\right)^{\\alpha-1} e^{-\\left(\\frac{x}{\\beta}\\right)^\\alpha}, \\hspace{0.3cm} x\\geq 0\n\\]mean variance random variable Weibull distribution can found consulting R documentation. Look make sure can use .","code":""},{"path":"CONTNNAMED.html","id":"normal-distribution","chapter":"13 Named Continuous Distributions","heading":"13.2.6 Normal distribution","text":"normal distribution (also referred Gaussian) common distribution found natural processes. likely seen bell curve various contexts. bell curve often indicative underlying normal distribution. two parameters normal distribution: \\(\\mu\\) (mean \\(X\\)) \\(\\sigma\\) (standard deviation \\(X\\)).Suppose random variable \\(X\\) normal distribution parameters \\(\\mu\\) \\(\\sigma\\). pdf \\(X\\) given :\\[\nf_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\hspace{0.3cm} -\\infty < x <\\infty\n\\]plots normal distributions different parameters plotted Figure 13.5.\nFigure 13.5: pdf Normal various values mu sigma\n","code":""},{"path":"CONTNNAMED.html","id":"standard-normal","chapter":"13 Named Continuous Distributions","heading":"13.2.6.1 Standard normal","text":"random variable \\(X\\) normally distributed \\(\\mu=0\\) \\(\\sigma=1\\), \\(X\\) said follow standard normal distribution. Sometimes, standard normal pdf denoted \\(\\phi(x)\\).Note normally distributed random variable can transformed standard normal distribution. Let \\(X \\sim \\textsf{Norm}(\\mu,\\sigma)\\). ,\n\\[\nZ=\\frac{X-\\mu}{\\sigma} \\sim \\textsf{Norm}(0,1)\n\\]Partially, one can show true noting mean \\(Z\\) 0 variance (standard deviation) \\(Z\\) 1:\n\\[\n\\mbox{E}(Z)=\\mbox{E}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma}\\left(\\mbox{E}(X)-\\mu\\right)=\\frac{1}\\sigma(\\mu-\\mu)=0\n\\]\n\\[\n\\mbox{Var}(Z)=\\mbox{Var}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma^2}\\left(\\mbox{Var}(X)-0\\right)=\\frac{1}{\\sigma^2} \\sigma^2=1\n\\]Note prove \\(Z\\) follows standard normal distribution; merely shown \\(Z\\) mean 0 variance 1. discuss transformation random variables later lesson.Example:\nLet \\(X \\sim \\textsf{Norm}(\\mu=200,\\sigma=15)\\). Compute \\(\\mbox{P}(X\\leq 160)\\), \\(\\mbox{P}(180\\leq X < 230)\\), \\(\\mbox{P}(X>\\mu+\\sigma)\\). Find median 95th percentile \\(X\\).gamma distribution, find probabilities quantiles, integration difficult, ’s best use built-R functions:","code":"\n## Prob(X<=160)\npnorm(160,200,15)## [1] 0.003830381\n##Prob(180 <= X < 230)\npnorm(230,200,15)-pnorm(180,200,15)## [1] 0.8860386\n##Prob(X>mu+sig)\n1-pnorm(215,200,15)## [1] 0.1586553\n## median\nqnorm(0.5,200,15)## [1] 200\n## 95th percentile\nqnorm(0.95,200,15)## [1] 224.6728"},{"path":"CONTNNAMED.html","id":"beta-distribution","chapter":"13 Named Continuous Distributions","heading":"13.2.7 Beta distribution","text":"last common continuous distribution study beta distribution. unique application domain random variable beta distribution \\([0,1]\\). Thus typically used model proportions. beta distribution two parameters, \\(\\alpha\\) \\(\\beta\\). (R, denoted cleverly shape1 shape2.)Let \\(X \\sim \\textsf{Beta}(\\alpha,\\beta)\\). pdf \\(X\\) given :\n\\[\nf_X(x)=\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, \\hspace{0.3cm} 0\\leq x \\leq 1\n\\]Yes, old friend Gamma function. resources, \\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\) written \\(\\frac{1}{B(\\alpha,\\beta)}\\), \\(B\\) known beta function.Note \\(\\mbox{E}(X)=\\frac{\\alpha}{\\alpha+\\beta}\\) \\(\\mbox{Var}(X)=\\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\).various values \\(\\alpha\\) \\(\\beta\\), pdf beta distributed random variable shown Figure 13.6.\nFigure 13.6: pdf Beta various values alpha beta\nExercise\ndistribution \\(\\alpha=\\beta=1\\)?uniform. easy verify \\(\\Gamma(1)=1\\) \\(B(1,1)=1\\).","code":""},{"path":"CONTNNAMED.html","id":"homework-problems-12","chapter":"13 Named Continuous Distributions","heading":"13.3 Homework Problems","text":"problems 1-3 , 1) define random variable help answer question, 2) state distribution parameters random variable; 3) determine expected value variance random variable, 4) use random variable answer question.given Saturday, suppose vehicles arrive USAFA North Gate according Poisson process rate 40 arrivals per hour.Find probability vehicles arrive 10 minutes.Find probability least 5 minutes pass next arrival.Find probability next vehicle arrive 2 10 minutes now.Find probability least 7 minutes pass next arrival, given 2 minutes already passed. Compare answer part (b). example memoryless property exponential distribution.Fill blank. probability 90% next vehicle arrive within __ minutes. value known 90% percentile random variable.Use function stripplot() visualize arrival 30 vehicles using random sample appropriate exponential distribution.Suppose time computer errors F-35 follows Gamma distribution mean 20 hours variance 10.Find probability 20 hours pass without computer error.Find probability 45 hours pass without computer error, given 25 hours already passed. memoryless property apply Gamma distribution?Find \\(\\) \\(b\\): 95% probability time next computer error \\(\\) \\(b\\). (note: technically, many answers question, find \\(\\) \\(b\\) tail equal probability.)Suppose PFT scores cadet wing follow normal distribution mean 330 standard deviation 50.Find probability randomly selected cadet PFT score higher 450.Find probability randomly selected cadet PFT score within 2 standard deviations mean.Find \\(\\) \\(b\\) 90% PFT scores \\(\\) \\(b\\).Find probability randomly selected cadet PFT score higher 450 given /among top 10% cadets.Let \\(X \\sim \\textsf{Beta}(\\alpha=1,\\beta=1)\\). Show \\(X\\sim \\textsf{Unif}(0,1)\\). Hint: write beta distribution pdf \\(\\alpha=1\\) \\(\\beta=1\\).Let \\(X \\sim \\textsf{Beta}(\\alpha=1,\\beta=1)\\). Show \\(X\\sim \\textsf{Unif}(0,1)\\). Hint: write beta distribution pdf \\(\\alpha=1\\) \\(\\beta=1\\).using R calculate probabilities related gamma distribution, often use pgamma. Recall pgamma equivalent cdf gamma distribution. \\(X\\sim\\textsf{Gamma}(\\alpha,\\lambda)\\), \n\\[\n\\mbox{P}(X\\leq x)=\\textsf{pgamma(x,alpha,lambda)}\n\\]\ndgamma function exists R . plain language, explain dgamma returns. ’m looking definition found R documentation. ’m looking simple description function returns. output dgamma useful? , ?using R calculate probabilities related gamma distribution, often use pgamma. Recall pgamma equivalent cdf gamma distribution. \\(X\\sim\\textsf{Gamma}(\\alpha,\\lambda)\\), \n\\[\n\\mbox{P}(X\\leq x)=\\textsf{pgamma(x,alpha,lambda)}\n\\]\ndgamma function exists R . plain language, explain dgamma returns. ’m looking definition found R documentation. ’m looking simple description function returns. output dgamma useful? , ?Advanced. may heard 68-95-99.7 rule. helpful rule thumb says population normal distribution, 68% data within one standard deviation mean, 95% data within two standard deviations 99.7% data within three standard deviations. Create function R two inputs (mean standard deviation). return vector three elements: probability randomly selected observation normal distribution inputted mean standard deviation lies within one, two three standard deviations. Test function several values mu sd. get answer time.Advanced. may heard 68-95-99.7 rule. helpful rule thumb says population normal distribution, 68% data within one standard deviation mean, 95% data within two standard deviations 99.7% data within three standard deviations. Create function R two inputs (mean standard deviation). return vector three elements: probability randomly selected observation normal distribution inputted mean standard deviation lies within one, two three standard deviations. Test function several values mu sd. get answer time.Derive mean general uniform distribution, \\(U(,b)\\).Derive mean general uniform distribution, \\(U(,b)\\).","code":""},{"path":"MULTIDISTS.html","id":"MULTIDISTS","chapter":"14 Multivariate Distributions","heading":"14 Multivariate Distributions","text":"","code":""},{"path":"MULTIDISTS.html","id":"objectives-13","chapter":"14 Multivariate Distributions","heading":"14.1 Objectives","text":"Define (distinguish ) terms joint probability mass/density function, marginal pmf/pdf, conditional pmf/pdf.Given joint pmf/pdf, obtain marginal conditional pmfs/pdfs.Use joint, marginal conditional pmfs/pdfs obtain probabilities.","code":""},{"path":"MULTIDISTS.html","id":"multivariate-distributions","chapter":"14 Multivariate Distributions","heading":"14.2 Multivariate distributions","text":"Multivariate situations common practice. often dealing one variable. seen previous block material see multivariate distributions remainder book.basic idea want determine relationship two variables include variable(s) conditional variables.","code":""},{"path":"MULTIDISTS.html","id":"joint-probability","chapter":"14 Multivariate Distributions","heading":"14.3 Joint probability","text":"Thus far, considered situations involving one random variable. cases, might concerned behavior multiple random variables simultaneously. chapter next dedicated jointly distributed random variables.","code":""},{"path":"MULTIDISTS.html","id":"discrete-random-variables","chapter":"14 Multivariate Distributions","heading":"14.3.1 Discrete random variables","text":"discrete case, joint probability described joint probability mass function. bivariate case, suppose \\(X\\) \\(Y\\) discrete random variables. joint pmf given \\(f_{X,Y}(x,y)\\) represents \\(\\mbox{P}(X=x,Y=y) = \\mbox{P}(X=x \\cap Y=y)\\). Note: common statistical probability models use comma represent , fact select() function tidyverse .rules probability apply joint pmf. value \\(f\\) must 0 1, total probability must sum 1:\n\\[\n\\sum_{x\\S_X}\\sum_{y \\S_Y} f_{X,Y}(x,y) = 1\n\\]\nnotation means sum joint probabilities values random variables \\(X\\) \\(Y\\) get 1.given joint pmf, one can obtain marginal pmf individual variables. marginal pmf simply mass function individual random variable, summing possible values variables. bivariate case, marginal pmf \\(X\\), \\(f_X(x)\\) found :\n\\[\nf_X(x)=\\sum_{y \\S_Y}f_{X,Y}(x,y)\n\\]Notice summation, summed \\(y\\) values.Similarly,\n\\[\nf_Y(y)=\\sum_{x \\S_X}f_{X,Y}(x,y)\n\\]marginal pmf must distinguished conditional pmf. conditional pmf describes discrete random variable given random variables taken particular values. bivariate case, conditional pmf \\(X\\), given \\(Y=y\\), denoted \\(f_{X|Y=y}(x)\\) found :\n\\[\nf_{X|Y=y}(x)=\\mbox{P}(X=x|Y=y)=\\frac{\\mbox{P}(X=x,Y=y)}{\\mbox{P}(Y=y)}=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]Example:\nLet \\(X\\) \\(Y\\) discrete random variables joint pmf .\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\&\\hline0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &2 & 0.18 & 0.20 & 0.12  \n\\\\&4 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]Find marginal pmfs \\(X\\) \\(Y\\).Find marginal pmfs \\(X\\) \\(Y\\).Find \\(f_{X|Y=0}(x)\\) \\(f_{Y|X=2}(y)\\).Find \\(f_{X|Y=0}(x)\\) \\(f_{Y|X=2}(y)\\).marginal pmfs can found summing across variable. , find \\(f_X(x)\\), simply sum across rows:\\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, & x=0 \\\\\n0.18+0.20+0.12, & x=2 \\\\\n0.07+0.05+0.09, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=2 \\\\\n0.21, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]Similarly, \\(f_Y(y)\\) can found summing columns joint pmf:\n\\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.33, & y=1 \\\\\n0.32, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]find conditional pmf \\(X\\) given \\(Y=0\\), helps recognize know \\(Y=0\\), overall sample space changed. Now outcomes consider first column (corresponding \\(Y=0\\)):looking distribution \\(X\\) within circled area. , need find proportion probability assigned outcome \\(X\\). Mathematically:\n\\[\nf_{X|Y=0}(x)=\\mbox{P}(X=x|Y=0)=\\frac{\\mbox{P}(X=x,Y=0)}{\\mbox{P}(Y=0)}=\\frac{f_{X,Y}(x,0)}{f_Y(0)}\n\\], found marginal pmf \\(Y\\). know \\(f_Y(0)=0.35\\). ,\n\\[\n\\renewcommand{\\arraystretch}{1.25}\nf_{X|Y=0}(x)=\\left\\{\\begin{array}{ll} \\frac{0.10}{0.35}, & x=0 \\\\\n\\frac{0.18}{0.35}, & x=2 \\\\\n\\frac{0.07}{0.35}, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.286, & x=0 \\\\\n0.514, & x=2 \\\\\n0.200, & x=4 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]Note probabilities pmf sum 1. always wise confirm ensure make simple computational error along way.Similarly, can find \\(f_{Y|X=2}(y)\\). First recognize \\(f_X(2)=0.5\\).\n\\[\n\\renewcommand{\\arraystretch}{1.25}\nf_{Y|X=2}(x)=\\left\\{\\begin{array}{ll} \\frac{0.18}{0.50}, & y=0 \\\\\n\\frac{0.20}{0.50}, & y=1 \\\\\n\\frac{0.12}{0.50}, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.36, & y=0 \\\\\n0.40, & y=1 \\\\\n0.24, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]Together, pmfs can used find relevant probabilities. example, see homework exercises.","code":""},{"path":"MULTIDISTS.html","id":"continuous-random-variables-1","chapter":"14 Multivariate Distributions","heading":"14.3.2 Continuous random variables","text":"Many ideas discrete random variables also apply case multiple continuous random variables. Suppose \\(X\\) \\(Y\\) continuous random variables. joint probability described joint probability density function. discrete case, joint pdf represented \\(f_{X,Y}(x,y)\\). Recall pmfs return probabilities, pdfs return densities, equivalent probabilities. order obtain probability pdf, one integrate pdf across applicable subset domain.rules joint pdf analogous univariate case. \\(x\\) \\(y\\), \\(f_{X,Y}(x,y)\\geq 0\\) probability must sum one:\n\\[\n\\int_{S_X}\\int_{S_Y}f_{X,Y}(x,y)\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = 1\n\\]marginal pdf density function individual random variable, integrating others. bivariate case, marginal pdf \\(X\\), \\(f_X(x)\\), found summing, integrating, across variable:\n\\[\nf_X(x)=\\int_{S_Y}f_{X,Y}(x,y)\\mathop{}\\!\\mathrm{d}y\n\\]Similarly,\n\\[\nf_Y(y)=\\int_{S_X}f_{X,Y}(x,y)\\mathop{}\\!\\mathrm{d}x\n\\]conditional pdf \\(X\\), given \\(Y=y\\) denoted \\(f_{X|Y=y}(x)\\) found way discrete case:\n\\[\nf_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]Similarly,\n\\[\nf_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\]Note working pdf probabilities case. can’t determine probability point continuous random variable. Thus work conditional pdfs find probabilities conditional statements.Example:\nLet \\(X\\) \\(Y\\) continuous random variables joint pdf:\\[\nf_{X,Y}(x,y)=xy\n\\]\n\\(0\\leq x \\leq 2\\) \\(0 \\leq y \\leq 1\\).Verify \\(f\\) valid joint pdf.need ensure total volume pdf 1. Note double integral constant limits integration just like single integrals. just treat variable constant. book work limits integration variables , material Calc III.simple case constant limits integration, order integration matter. arbitrarily integrate \\(x\\) first, treating \\(y\\) constant. integrate respect \\(y\\).\\[\n\\int_0^1 \\int_0^2 xy \\mathop{}\\!\\mathrm{d}x \\mathop{}\\!\\mathrm{d}y = \\int_0^1 \\frac{x^2y}{2}\\bigg|_0^2 \\mathop{}\\!\\mathrm{d}y = \\int_0^1 2y\\mathop{}\\!\\mathrm{d}y = y^2\\bigg|_0^1 = 1\n\\]Using R requires new package cubature. can install RStudio package tab command line using install.packages(\"cubature\"). can use follows:Notice function adaptIntegrate returned four objects. can read help menu learn interested result contained object integral.Find \\(\\mbox{P}(X > 1, Y \\leq 0.5)\\).\n\\[\n\\mbox{P}(X>1,Y\\leq 0.5)=\\int_0^{0.5}\\int_1^2 xy \\mathop{}\\!\\mathrm{d}x \\mathop{}\\!\\mathrm{d}y = \\int_0^{0.5} \\frac{x^2 y}{2}\\bigg|_1^2 \\mathop{}\\!\\mathrm{d}y = \\int_0^{0.5}2y - \\frac{y}{2}\\mathop{}\\!\\mathrm{d}y\n\\]\n\\[\n= \\frac{3y^2}{4}\\bigg|_0^{0.5}=0.1875\n\\]Find marginal pdfs \\(X\\) \\(Y\\).\n\\[\nf_X(x)=\\int_0^1 xy \\mathop{}\\!\\mathrm{d}y = \\frac{xy^2}{2}\\bigg|_0^1=\\frac{x}{2}\n\\]\\(0 \\leq x \\leq 2\\).\\[\nf_Y(y)=\\int_0^2 xy \\mathop{}\\!\\mathrm{d}x = \\frac{x^2y}{2}\\bigg|_0^2= 2y\n\\]\\(0 \\leq y \\leq 1\\).Find conditional pdfs \\(X|Y=y\\) \\(Y|X=x\\).\n\\[\nf_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}=\\frac{xy}{2y}=\\frac{x}{2}\n\\]\\(0 \\leq x \\leq 2\\).Similarly,\n\\[\nf_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)}=\\frac{xy}{\\frac{x}{2}}=2y\n\\]\\(0 \\leq y \\leq 1\\).","code":"\n library(cubature) # load the package \"cubature\"\nf <- function(x) { (x[1] * x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 2))## $integral\n## [1] 1\n## \n## $error\n## [1] 0\n## \n## $functionEvaluations\n## [1] 17\n## \n## $returnCode\n## [1] 0\nf <- function(x) { (x[1] * x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(1, 0), upperLimit = c(2, 1/2))## $integral\n## [1] 0.1875\n## \n## $error\n## [1] 2.775558e-17\n## \n## $functionEvaluations\n## [1] 17\n## \n## $returnCode\n## [1] 0"},{"path":"MULTIDISTS.html","id":"homework-problems-13","chapter":"14 Multivariate Distributions","heading":"14.4 Homework Problems","text":"Let \\(X\\) \\(Y\\) continuous random variables joint pdf:\n\\[\nf_{X,Y}(x,y)=x + y\n\\]\\(0 \\leq x \\leq 1\\) \\(0 \\leq y \\leq 1\\).Verify \\(f\\) valid pdf.Find marginal pdfs \\(X\\) \\(Y\\).Find conditional pdfs \\(X|Y=y\\) \\(Y|X=x\\).Find following probabilities: \\(\\mbox{P}(X<0.5)\\); \\(\\mbox{P}(Y>0.8)\\); \\(\\mbox{P}(X<0.2,Y\\geq 0.75)\\); \\(\\mbox{P}(X<0.2|Y\\geq 0.75)\\); \\(\\mbox{P}(X<0.2|Y= 0.25)\\); Optional - \\(\\mbox{P}(X\\leq Y)\\). reading, saw example \\(f_X(x)=f_{X|Y=y}(x)\\) \\(f_Y(y)=f_{Y|X=x}(y)\\). common important. imply \\(X\\) \\(Y\\)?ADVANCED: Recall earlier assignment, came random variables describe timeliness airport. Suppose course 210 days, day recorded number customer complaints regarding timeliness. Also day, recorded weather (airport located somewhere without snow without substantial wind). data displayed .\\[\n\\begin{array}{cc|cc} & & &\\textbf{Weather Status}\\\\\n& & \\mbox{Clear} & \\mbox{Light Rain} & \\mbox{Rain}  \\\\\n& \\hline0 & 28 & 11 & 4  \\\\\n& 1 & 18 & 15 & 8  \\\\\n& 2 & 17 & 25 & 12  \\\\\n\\textbf{# complaints} & 3 & 13 & 15 & 16  \\\\\n& 4 & 8 & 8 & 10 \\\\\n& 5 & 0 & 1 & 1 \\\\\n\\end{array}\n\\]First, define two random variables scenario. One (# complaints) essentially already random variable. (weather status) need assign number status.Use table build empirical joint pmf two random variables.Find marginal pmfs random variable.Find probability fewer 3 complaints.Find probability fewer 3 complaints given rain. Optional like Calc III want challenge.Let \\(X\\) \\(Y\\) continuous random variables joint pmf:\n\\[\nf_{X,Y}(x,y)=1\n\\]\\(0 \\leq x \\leq 1\\) \\(0 \\leq y \\leq 2x\\).Verify \\(f\\) valid pdf.Find marginal pdfs \\(X\\) \\(Y\\).Find conditional pdfs \\(X|Y=y\\) \\(Y|X=x\\).Find following probabilities: \\(\\mbox{P}(X<0.5)\\); \\(\\mbox{P}(Y>1)\\); \\(\\mbox{P}(X<0.5,Y\\leq 0.8)\\); Optional \\(\\mbox{P}(X<0.5|Y= 0.8)\\); \\(\\mbox{P}(Y\\leq 1-X)\\). (probably help draw pictures.)","code":""},{"path":"MULTIEXP.html","id":"MULTIEXP","chapter":"15 Multivariate Expectation","heading":"15 Multivariate Expectation","text":"","code":""},{"path":"MULTIEXP.html","id":"objectives-14","chapter":"15 Multivariate Expectation","heading":"15.1 Objectives","text":"Given joint pmf/pdf, obtain means variances random variables functions random variables.Define terms covariance correlation, given joint pmf/pdf, obtain covariance correlation two random variables.Given joint pmf/pdf, determine whether random variables independent one another.Find conditional expectations.","code":""},{"path":"MULTIEXP.html","id":"expectation---moments","chapter":"15 Multivariate Expectation","heading":"15.2 Expectation - moments","text":"Computing expected values random variables joint context similar univariate case. Let \\(X\\) \\(Y\\) discrete random variables joint pmf \\(f_{X,Y}(x,y)\\). Let \\(g(X,Y)\\) function \\(X\\) \\(Y\\). :\n\\[\n\\mbox{E}[g(X,Y)]=\\sum_x\\sum_y g(x,y)f_{X,Y}(x,y)\n\\](Note \\(\\sum\\limits_{x}\\) shorthand sum across possible values \\(x\\).)case continuous random variables joint pdf \\(f_{X,Y}(x,y)\\), expectation becomes:\n\\[\n\\mbox{E}[g(X,Y)]=\\int_x\\int_y g(x,y)f_{X,Y}(x,y)\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x\n\\]","code":""},{"path":"MULTIEXP.html","id":"expectation-of-discrete-random-variables","chapter":"15 Multivariate Expectation","heading":"15.2.1 Expectation of discrete random variables","text":"Given joint pmf, one can find mean \\(X\\) using joint function finding marginal pmf first using find \\(\\mbox{E}(X)\\). end, ways . discrete case:\n\\[\n\\mbox{E}(X)=\\sum_x\\sum_y xf_{X,Y}(x,y) = \\sum_x x \\sum_y f_{X,Y}(x,y)\n\\]\\(x\\) can moved outside inner sum since inner sum respect variable \\(y\\) \\(x\\) constant respect \\(y\\). Note inner sum marginal pmf \\(X\\). ,\n\\[\n\\mbox{E}(X)=\\sum_x x \\sum_y f_{X,Y}(x,y)=\\sum_x x f_X(x)\n\\]Example:\nLet \\(X\\) \\(Y\\) discrete random variables joint pmf .\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\&\\hline0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &1 & 0.18 & 0.20 & 0.12  \n\\\\&2 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]Find \\(\\mbox{E}(X)\\)First use joint pmf directly, find marginal pmf \\(X\\) use univariate case.\\[\n\\mbox{E}(X)=\\sum_{x=0}^2 \\sum_{y=0}^2 x f_{X,Y}(x,y)=0*0.10+0*0.08+0*0.11+1*0.18+...+2*0.09 = 0.92\n\\]marginal pmf \\(X\\) \n\\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, & x=0 \\\\\n0.18+0.20+0.12, & x=1 \\\\\n0.07+0.05+0.09, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=1 \\\\\n0.21, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\], \\(\\mbox{E}(X)=0*0.29+1*0.5+2*0.21=0.92\\).","code":""},{"path":"MULTIEXP.html","id":"exercises-to-apply-what-we-learned","chapter":"15 Multivariate Expectation","heading":"15.3 Exercises to apply what we learned","text":"Exercise:\nLet \\(X\\) \\(Y\\) defined . Find \\(\\mbox{E}(Y)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), \\(\\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right)\\).","code":""},{"path":"MULTIEXP.html","id":"ey","chapter":"15 Multivariate Expectation","heading":"15.3.1 E(Y)","text":"\\(\\mbox{E}(X)\\), \\(\\mbox{E}(Y)\\) can found two ways. use marginal pmf \\(Y\\), derive:\\[\n\\mbox{E}(Y)=\\sum_{y=0}^2 y \\cdot f_Y(y)=0*0.35+1*0.33+2*0.32 = 0.97\n\\]","code":""},{"path":"MULTIEXP.html","id":"exy","chapter":"15 Multivariate Expectation","heading":"15.3.2 E(X+Y)","text":"find \\(\\mbox{E}(X+Y)\\) use joint pmf. discrete case, helps first identify possible values \\(X+Y\\) figure probabilities associated value. problem really transformation problem finding distribution \\(X+Y\\). example, \\(X+Y\\) can take values 0, 1, 2, 3, 4. value 0 happens \\(X=Y=0\\) probability outcome 0.10. value 1 occurs \\(X=0\\) \\(Y=1\\) \\(X=1\\) \\(Y=0\\). occurs probability 0.08 + 0.18. continue manner:\n\\[\n\\mbox{E}(X+Y)=\\sum_{x=0}^2\\sum_{y=0}^2 (x+y)f_{X,Y}(x,y) = 0*0.1+1*(0.18+0.08)+2*(0.11+0.07+0.20)\n\\]\n\\[\n+3*(0.12+0.05)+4*0.09 = 1.89\n\\]Note \\(\\mbox{E}(X+Y)=\\mbox{E}(X)+\\mbox{E}(Y)\\). (proof left reader.)","code":""},{"path":"MULTIEXP.html","id":"exy-1","chapter":"15 Multivariate Expectation","heading":"15.3.3 E(XY)","text":"\\[\n\\mbox{E}(XY)=\\sum_{x=0}^2\\sum_{y=0}^2 xyf_{X,Y}(x,y) = 0*(0.1+0.08+0.11+0.18+0.07)+1*0.20\n\\]\n\\[\n+2*(0.12+0.05)+4*0.09= 0.9\n\\]Note \\(\\mbox{E}(XY)\\) necessarily equal \\(\\mbox{E}(X)\\mbox{E}(Y)\\).","code":""},{"path":"MULTIEXP.html","id":"e12xy1","chapter":"15 Multivariate Expectation","heading":"15.3.4 E(1/2X+Y+1)","text":"\\[\n\\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right) = \\sum_{x=0}^2\\sum_{y=0}^2 \\frac{1}{2x+y+1}f_{X,Y}(x,y) = 1*0.1+\\frac{1}{2}*0.08+\\frac{1}{3}*(0.11+0.18)\n\\]\n\\[\n+\\frac{1}{4}*0.20+\\frac{1}{5}*(0.12+0.07)+\\frac{1}{6}*0.05+\\frac{1}{7}*0.09 = 0.3125\n\\]","code":""},{"path":"MULTIEXP.html","id":"expectation-of-continuous-random-variables","chapter":"15 Multivariate Expectation","heading":"15.3.5 Expectation of continuous random variables","text":"Let’s consider example continuous random variables summation replaced integration:Example:\nLet \\(X\\) \\(Y\\) continuous random variables joint pdf:\n\\[\nf_{X,Y}(x,y)=xy\n\\]\n\\(0\\leq x \\leq 2\\) \\(0 \\leq y \\leq 1\\).","code":""},{"path":"MULTIEXP.html","id":"exercises-to-apply-what-we-learned-1","chapter":"15 Multivariate Expectation","heading":"15.4 Exercises to apply what we learned","text":"Exercise:\nFind \\(\\mbox{E}(X)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), \\(\\mbox{Var}(XY)\\).","code":""},{"path":"MULTIEXP.html","id":"ex","chapter":"15 Multivariate Expectation","heading":"15.4.1 E(X)","text":"found marginal pdf \\(X\\) previous lesson, use now:\n\\[\n\\mbox{E}(X)=\\int_0^2 x\\frac{x}{2}\\mathop{}\\!\\mathrm{d}x = \\frac{x^3}{6}\\bigg|_0^2= \\frac{4}{3}\n\\]\nusing R","code":"\nfractions(integrate(function(x){x^2/2},0,2)$value)## [1] 4/3"},{"path":"MULTIEXP.html","id":"exy-2","chapter":"15 Multivariate Expectation","heading":"15.4.2 E(X+Y)","text":"find \\(\\mbox{E}(X+Y)\\), use joint pdf directly, use marginal pdf \\(Y\\) find \\(\\mbox{E}(Y)\\) add result \\(\\mbox{E}(X)\\). reason valid integrate \\(x\\) joint pdf, integrating respect \\(y\\) first, can treat \\(x\\) constant bring side integral. integrating joint pdf respect \\(y\\) results marginal pdf \\(X\\).’ll use joint pdf:\n\\[\n\\mbox{E}(X+Y)=\\int_0^2\\int_0^1 (x+y)xy\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x=\\int_0^2\\int_0^1 (x^2y+xy^2)\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^2 \\frac{x^2y^2}{2}+\\frac{xy^3}{3} \\bigg|_{y=0}^{y=1}\\mathop{}\\!\\mathrm{d}x\n\\]\\[\n= \\int_0^2 \\frac{x^2}{2}+\\frac{x}{3} \\mathop{}\\!\\mathrm{d}x= \\frac{x^3}{6}+\\frac{x^2}{6}\\bigg|_0^2=\\frac{8}{6}+\\frac{4}{6}=2\n\\]using R:wanted use simulation find expectation, simulate variables marginal \\(X\\) \\(Y\\) add together create new variable.cdf \\(X\\) \\(\\frac{x^2}{4}\\) simulate random variable \\(X\\) sampling random uniform taking inverse cdf. \\(Y\\) cdf \\(y^2\\) similar simulation.can see \\(E(X + Y) = E(X) + E(Y)\\).","code":"\nadaptIntegrate(function(x){(x[1]+x[2])*x[1]*x[2]},lowerLimit = c(0,0),upperLimit = c(1,2))$integral## [1] 2\nset.seed(1820)\nnew_data <- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %>%\n  mutate(z=x+y) %>%\n  summarize(Ex=mean(x),Ey=mean(y),Explusy = mean(z))##         Ex        Ey  Explusy\n## 1 1.338196 0.6695514 2.007748"},{"path":"MULTIEXP.html","id":"exy-3","chapter":"15 Multivariate Expectation","heading":"15.4.3 E(XY)","text":"Next, \\[\n\\mbox{E}(XY)=\\int_0^2\\int_0^1 xy*xy\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^2 \\frac{x^2y^3}{3}\\bigg|_0^1 \\mathop{}\\!\\mathrm{d}x = \\int_0^2 \\frac{x^2}{3}\\mathop{}\\!\\mathrm{d}x\n\\]\n\\[\n=\\frac{x^3}{9}\\bigg|_0^2 = \\frac{8}{9}\n\\]Using R:simulating, :","code":"\nfractions(adaptIntegrate(function(x){(x[1]*x[2])*x[1]*x[2]},lowerLimit = c(0,0),upperLimit = c(1,2))$integral)## [1] 8/9\nset.seed(191)\nnew_data <- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %>%\n  mutate(z=x*y) %>%\n  summarize(Ex=mean(x),Ey=mean(y),Extimesy = mean(z))##        Ex        Ey  Extimesy\n## 1 1.33096 0.6640436 0.8837552"},{"path":"MULTIEXP.html","id":"vxy","chapter":"15 Multivariate Expectation","heading":"15.4.4 V(XY)","text":"Recall variance random variable expected value squared difference mean. ,\n\\[\n\\mbox{Var}(XY)=\\mbox{E}\\left[\\left(XY-\\mbox{E}(XY)\\right)^2\\right]=\\mbox{E}\\left[\\left(XY-\\frac{8}{9}\\right)^2\\right]\n\\]\n\\[\n=\\int_0^2\\int_0^1 \\left(xy-\\frac{8}{9}\\right)^2 xy\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x =\\int_0^2\\int_0^1 \\left(x^2y^2-\\frac{16xy}{9}+\\frac{64}{81}\\right)xy\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x\n\\]Yuck!! continue determined integrate much Calculus core curriculum.\\[\n=\\int_0^2\\int_0^1 \\left(x^3y^3-\\frac{16x^2y^2}{9}+\\frac{64xy}{81}\\right)\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x\n\\]\\[\n=\\int_0^2 \\frac{x^3y^4}{4}-\\frac{16x^2y^3}{27}+\\frac{32xy^2}{81}\\bigg|_0^1 \\mathop{}\\!\\mathrm{d}x\n\\]\\[\n= \\int_0^2 \\frac{x^3}{4}-\\frac{16x^2}{27}+\\frac{32x}{81}\\mathop{}\\!\\mathrm{d}x\n\\]\\[\n= \\frac{x^4}{16}-\\frac{16x^3}{81}+\\frac{16x^2}{81}\\bigg|_0^2\n\\]\n\\[\n=\\frac{16}{16}-\\frac{128}{81}+\\frac{64}{81}=\\frac{17}{81}\n\\]Using R:Next estimate variance using simulation:much easier. Notice really just estimating expectations simulations. mathematical answers true population values simulations sample estimates. lessons discuss estimators detail.","code":"\nfractions(adaptIntegrate(function(x){(x[1]*x[2]-8/9)^2*x[1]*x[2]},lowerLimit = c(0,0),upperLimit = c(1,2))$integral)## [1] 17/81\nset.seed(816)\nnew_data <- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000)))\nnew_data %>%\n  mutate(z=(x*y-8/9)^2) %>%\n  summarize(Var = mean(z))##         Var\n## 1 0.2098769"},{"path":"MULTIEXP.html","id":"covariancecorrelation","chapter":"15 Multivariate Expectation","heading":"15.5 Covariance/Correlation","text":"discussed expected values random variables functions random variables joint context. helpful kind consistent measure describe two random variables related one another. Covariance correlation just . important understand measures linear relationship variables.Consider two random variables \\(X\\) \\(Y\\). (certainly consider two, demonstration, let’s consider two now). covariance \\(X\\) \\(Y\\) denoted \\(\\mbox{Cov}(X,Y)\\) found :\n\\[\n\\mbox{Cov}(X,Y)=\\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right]\n\\]can simplify expression make little usable:\n\\[\n\\begin{align}\n\\mbox{Cov}(X,Y) & = \\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right] \\\\\n& = \\mbox{E}\\left[XY - Y\\mbox{E}(X) - X\\mbox{E}(Y) + \\mbox{E}(X)\\mbox{E}(Y)\\right] \\\\\n& = \\mbox{E}(XY) - \\mbox{E}(Y\\mbox{E}(X)) - \\mbox{E}(X\\mbox{E}(Y)) + \\mbox{E}(X)\\mbox{E}(Y) \\\\\n& = \\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)-\\mbox{E}(X)\\mbox{E}(Y)+\\mbox{E}(X)\\mbox{E}(Y) \\\\\n& = \\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\n\\end{align}\n\\]Thus,\n\\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\n\\]expression little easier use, since ’s typically straightforward find quantities.important note variance positive quantity, covariance can positive negative. positive covariance implies value one variable increases, tends increase. statement linear relationship. Likewise, negative covariance implies value one variable increases, tends decrease.Example:\nexample positive covariance human height weight. height increase, weight tends increase. example negative covariance gas mileage car weight. car weight increases, gas mileage decreases.Remember \\(\\) \\(b\\) constants, \\(\\mbox{E}(aX+b) =\\mbox{E}(X)+b\\) \\(\\mbox{Var}(aX+b)=^2\\mbox{Var}(X)\\). Similarly, \\(\\), \\(b\\), \\(c\\), \\(d\\) constants,\n\\[\n\\mbox{Cov}(aX+b,cY+d)=ac\\mbox{Cov}(X,Y)\n\\]One disadvantage covariance dependence scales random variables involved. makes difficult compare covariances multiple sets variables. Correlation avoids problem. Correlation scaled version covariance. denoted \\(\\rho\\) found :\n\\[\n\\rho = \\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}\n\\]covariance take real number, correlation bounded -1 1. Two random variables correlation 1 said perfectly positively correlated, correlation -1 implies perfect negative correlation. Two random variables correlation (thus covariance) 0 said uncorrelated, linear relationship non-linear relationship. last point important; random variables relationship 0 covariance. However, 0 covariance implies random variables linear relationship.Let’s look plots, Figures 15.1, 15.2, 15.3, 15.4 different correlations. Remember correlation calculating section population, plots showing sample points population.\nFigure 15.1: Correlation 1\n\nFigure 15.2: Correlation .8\n\nFigure 15.3: Correlation .5\n\nFigure 15.4: Correlation 0\n","code":""},{"path":"MULTIEXP.html","id":"variance-of-sums","chapter":"15 Multivariate Expectation","heading":"15.5.1 Variance of sums","text":"Suppose \\(X\\) \\(Y\\) two random variables. ,\n\\[\n\\mbox{Var}(X+Y)=\\mbox{E}\\left[(X+Y-\\mbox{E}(X+Y))^2\\right]=\\mbox{E}[(X+Y)^2]-\\left[\\mbox{E}(X+Y)\\right]^2\n\\]last step, using alternative expression variance (\\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\)). Evaluating:\n\\[\n\\mbox{Var}(X+Y)=\\mbox{E}(X^2)+\\mbox{E}(Y^2)+2\\mbox{E}(XY)-\\mbox{E}(X)^2-\\mbox{E}(Y)^2-2\\mbox{E}(X)\\mbox{E}(Y)\n\\]Regrouping terms:\n\\[\n=\\mbox{E}(X^2)-\\mbox{E}(X)^2+\\mbox{E}(Y^2)-\\mbox{E}(Y)^2+2\\left(\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\\right)\n\\]\\[\n=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)\n\\]Example:\nLet \\(X\\) \\(Y\\) defined . Find \\(\\mbox{Cov}(X,Y)\\), \\(\\rho\\), \\(\\mbox{Var}(X+Y)\\).\\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)=0.9-0.92*0.97=0.0076\n\\]\\[\n\\rho=\\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}\n\\]Quickly, \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2= 1.34-0.92^2 =0.4936\\) \\(\\mbox{Var}(Y)=0.6691\\). ,\n\\[\n\\rho=\\frac{0.0076}{\\sqrt{0.4936*0.6691}}=0.013\n\\]low \\(\\rho\\), say \\(X\\) \\(Y\\) slightly positively correlated.\\[\n\\mbox{Var}(X+Y)=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)=0.4936+0.6691+2*0.0076=1.178\n\\]","code":""},{"path":"MULTIEXP.html","id":"independence-1","chapter":"15 Multivariate Expectation","heading":"15.6 Independence","text":"Two random variables \\(X\\) \\(Y\\) said independent joint pmf/pdf product marginal pmfs/pdfs:\n\\[\nf_{X,Y}(x,y)=f_X(x)f_Y(y)\n\\]\\(X\\) \\(Y\\) independent, \\(\\mbox{Cov}(X,Y) = 0\\). converse necessarily true, however non-linear relationship.discrete distribution, must check cell, joint probabilities, equal product marginal probability. Back joint pmf :\\[\n\\begin{array}{cc|ccc} & & & \\textbf{Y} &\n\\\\ & & 0 & 1 & 2  \n\\\\&\\hline0 & 0.10 & 0.08 & 0.11  \n\\\\\\textbf{X} &1 & 0.18 & 0.20 & 0.12  \n\\\\&2 & 0.07 & 0.05 & 0.09\n\\end{array}\n\\]marginal pmf \\(X\\) \\[\nf_X(x) = \\left\\{\\begin{array}{ll} 0.29, & x=0 \\\\\n0.50, & x=1 \\\\\n0.21, & x=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nmarginal pmf \\(Y\\) \\[\nf_Y(y) = \\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.33, & y=1 \\\\\n0.32, & y=2 \\\\\n0, & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nChecking first cell, immediately see \\(f_{X,Y}(x=0,y=0) \\neq f_{X}(x=0)\\cdot f_{Y}(y=0)\\) \\(X\\) \\(Y\\) independent.easy way determine continuous variables independent first check domain contains constants, rectangular, second joint pdf can written product function \\(X\\) function \\(Y\\) .Thus examples even though domains rectangular, \\(f(x,y)=xy\\), \\(X\\) \\(Y\\) independent \\(f(x,y)=x+y\\) .","code":""},{"path":"MULTIEXP.html","id":"conditional-expectation","chapter":"15 Multivariate Expectation","heading":"15.7 Conditional expectation","text":"important idea graph theory, network analysis, Bayesian networks, queuing theory conditional expectation. briefly introduce ideas basic understanding. imply important topic.Let’s start simple example illustrate ideas.Example:\nSam read either one chapter history book one chapter philosophy book. number misprints chapter history book Poisson mean 2 number misprints chapter philosophy book Poisson mean 5, assuming Sam equally likely choose either book, expected number misprints Sam find?Note: next chapter working transformations attack problem using method.First let’s use simulation get idea value answer use algebraic techniques definitions learned book.Simulate 5000 reads history book 5000 philosophy combine:Figure 15.5 histogram data.\nFigure 15.5: Misprints combined history philosphy books.\nbar chart Figure 15.6\nFigure 15.6: Misprints combined history philosphy books bar chart.\nnow find average.Now mathematical solution. Let \\(X\\) stand number misprints \\(Y\\) book, make \\(Y\\) random variable let’s call 0 history 1 philosophy. \\(E[X|Y]\\) expected number misprints given book. conditional expectation. example \\(E[X|Y=0]\\) expected number misprints history book.Now tricky part, without specifying value \\(Y\\), called realization, expectation function random variable depends \\(Y\\). words, don’t know book, expected number misprints depends \\(Y\\) thus random variable. take expected value random variable get\\[E[X]=E[E[X|Y]]\\]inner expectation right hand side conditional distribution outer marginal respect \\(Y\\). seems confusing, let’s go back example.\\[E[X]=E[E[X|Y]]\\]\n\\[=E[X|Y=0] \\cdot \\mbox{P}(Y=0)+E[X|Y=1] \\cdot \\mbox{P}(Y=1)\\]\n\\[=2*\\frac{1}{2}+5*\\frac{1}{2}=\\frac{7}{2}=3.5\\]\nideas going similar continuous random variables. much can conditional expectations, beyond scope book.","code":"\nset.seed(2011)\nmy_data<-data.frame(misprints=c(rpois(5000,2),rpois(5000,5)))\nhead(my_data)##   misprints\n## 1         1\n## 2         1\n## 3         2\n## 4         1\n## 5         2\n## 6         4\ndim(my_data)## [1] 10000     1\ngf_histogram(~misprints,data=my_data,breaks=seq(-0.5, 15.5, by=1)) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Number of Misprints\")\ngf_bar(~misprints,data=my_data)%>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Number of Misprints\")\nmean(~misprints,data=my_data)## [1] 3.4968"},{"path":"MULTIEXP.html","id":"homework-problems-14","chapter":"15 Multivariate Expectation","heading":"15.8 Homework Problems","text":"Let \\(X\\) \\(Y\\) continuous random variables joint pdf:\n\\[\nf_{X,Y}(x,y)=x + y\n\\]\\(0 \\leq x \\leq 1\\) \\(0 \\leq y \\leq 1\\).Find \\(\\mbox{E}(X)\\) \\(\\mbox{E}(Y)\\).Find \\(\\mbox{Var}(X)\\) \\(\\mbox{Var}(Y)\\).Find \\(\\mbox{Cov}(X,Y)\\) \\(\\rho\\). \\(X\\) \\(Y\\) independent?Find \\(\\mbox{Var}(3X+2Y)\\).Optional - difficult small Calc III idea. Let \\(X\\) \\(Y\\) continuous random variables joint pmf:\n\\[\nf_{X,Y}(x,y)=1\n\\]Optional - difficult small Calc III idea. Let \\(X\\) \\(Y\\) continuous random variables joint pmf:\n\\[\nf_{X,Y}(x,y)=1\n\\]Suppose \\(X\\) \\(Y\\) independent random variables. Show \\(\\mbox{E}(XY)=\\mbox{E}(X)\\mbox{E}(Y)\\).Suppose \\(X\\) \\(Y\\) independent random variables. Show \\(\\mbox{E}(XY)=\\mbox{E}(X)\\mbox{E}(Y)\\).playing game friend. roll fair sided die record result.playing game friend. roll fair sided die record result.Write joint probability mass function.Write joint probability mass function.Find expected value product score friend’s score.Find expected value product score friend’s score.Verify previous part using simulation.Verify previous part using simulation.Using simulation, find expected value maximum number two roles.Using simulation, find expected value maximum number two roles.miner trapped mine containing three doors. first door leads tunnel takes safety two hours travel. second door leads tunnel returns mine three hours travel. third door leads tunnel returns mine five hours. Assuming miner times equally likely choose one doors, yes bad assumption makes nice problem, expected length time miner reaches safety?miner trapped mine containing three doors. first door leads tunnel takes safety two hours travel. second door leads tunnel returns mine three hours travel. third door leads tunnel returns mine five hours. Assuming miner times equally likely choose one doors, yes bad assumption makes nice problem, expected length time miner reaches safety?ADVANCED: Let \\(X_1,X_2,...,X_n\\) independent, identically distributed random variables. (often abbreviated “..d.”). \\(X_i\\) mean \\(\\mu\\) variance \\(\\sigma^2\\) (.e., \\(\\), \\(\\mbox{E}(X_i)=\\mu\\) \\(\\mbox{Var}(X_i)=\\sigma^2\\)).ADVANCED: Let \\(X_1,X_2,...,X_n\\) independent, identically distributed random variables. (often abbreviated “..d.”). \\(X_i\\) mean \\(\\mu\\) variance \\(\\sigma^2\\) (.e., \\(\\), \\(\\mbox{E}(X_i)=\\mu\\) \\(\\mbox{Var}(X_i)=\\sigma^2\\)).Let \\(S=X_1+X_2+...+X_n=\\sum_{=1}^n X_i\\). let \\(\\bar{X}={\\sum_{=1}^n X_i \\n}\\).Find \\(\\mbox{E}(S)\\), \\(\\mbox{Var}(S)\\), \\(\\mbox{E}(\\bar{X})\\) \\(\\mbox{Var}(\\bar{X})\\).","code":""},{"path":"TRANS.html","id":"TRANS","chapter":"16 Transformations","heading":"16 Transformations","text":"","code":""},{"path":"TRANS.html","id":"objectives-15","chapter":"16 Transformations","heading":"16.1 Objectives","text":"Given discrete random variable, determine distribution transformation random variable.Given continuous random variable, use cdf method determine distribution transformation random variable.Use simulation methods find distribution transform single multivariate random variables.","code":""},{"path":"TRANS.html","id":"transformations","chapter":"16 Transformations","heading":"16.2 Transformations","text":"Throughout coverage random variables, mentioned transformations random variables. context linear transformations. discussed expected value variance linear transformations. Recall \\(\\mbox{E}(aX+b)=\\mbox{E}(X)+b\\) \\(\\mbox{Var}(aX+b)=^2\\mbox{Var}(X)\\).chapter, discuss transformations random variables general, beyond linear case.","code":""},{"path":"TRANS.html","id":"transformations-of-discrete-random-variables","chapter":"16 Transformations","heading":"16.2.1 Transformations of discrete random variables","text":"Let \\(X\\) discrete random variable let \\(g\\) function. variable \\(Y=g(X)\\) discrete random variable pmf:\n\\[\nf_Y(y)=\\mbox{P}(Y=y)=\\sum_{\\forall x: g(x)=y}\\mbox{P}(X=x)=\\sum_{\\forall x: g(x)=y}f_X(x)\n\\]example help since notation can confusing.Example:\nSuppose \\(X\\) discrete random variable pmf:\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.05, & x=-2 \\\\\n0.10, & x=-1 \\\\\n0.35, & x=0 \\\\\n0.30, & x=1 \\\\\n0.20, & x=2 \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]Find pmf \\(Y=X^2\\).helps identify support \\(Y\\), \\(f_{Y}(y)>0\\). Since support \\(X\\) \\(S_X=\\{-2,-1,0,1,2\\}\\), support \\(Y\\) \\(S_Y=\\{0,1,4\\}\\).\n\\[\nf_Y(0)=\\sum_{x^2=0}f_X(x)=f_X(0)=0.35\n\\]\\[\nf_Y(1)=\\sum_{x^2=1}f_X(x)=f_X(-1)+f_X(1)=0.1+0.3=0.4\n\\]\n\\[\nf_Y(4)=\\sum_{x^2=4}f_X(x)=f_X(-2)+f_X(2)=0.05+0.2=0.25\n\\],\n\\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.35, & y=0 \\\\\n0.4, & y=1 \\\\\n0.25, & y=4 \\\\\n0, & \\mbox{otherwise} \\end{array}\\right.\n\\]also helps confirm probabilities add one, . pmf \\(Y=X^2\\).key idea find support new random variable go back original random variable sum probabilities get mapped new support element.","code":""},{"path":"TRANS.html","id":"transformations-of-continuous-random-variables","chapter":"16 Transformations","heading":"16.2.2 Transformations of continuous random variables","text":"methodology work directly case continuous random variables. continuous case, pdf, \\(f_X(x)\\), represents density probability.","code":""},{"path":"TRANS.html","id":"the-cdf-method","chapter":"16 Transformations","heading":"16.2.3 The cdf method","text":"cdf method one several methods can used transformations continuous random variables. idea find cdf new random variable find pdf way fundamental theorem calculus.Suppose \\(X\\) continuous random variable cdf \\(F_X(x)\\). Let \\(Y=g(X)\\). can find cdf \\(Y\\) :\\[\nF_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X)\\leq y)=\\mbox{P}(X\\leq g^{-1}(y))=F_X(g^{-1}(y))\n\\]get pdf \\(Y\\) need take derivative cdf. Note \\(g^{-1}(y)\\) function inverse \\(g(y)^{-1}\\) multiplicative inverse.method requires transformation function inverse. Sometimes can break domain original random variables regions inverse transformation function exists.Example:\nLet \\(X\\sim \\textsf{Unif}(0,1)\\) let \\(Y=X^2\\). Find pdf \\(Y\\).start, let’s think distribution \\(Y\\). randomly taking numbers 0 1 squaring . Squaring positive number less 1 makes even smaller. thus suspect pdf \\(Y\\) larger density near 0 1. shape hard determine let’s math.Since \\(X\\) uniform distribution, know \\(f_X(x)\\) \\(F_X(x)=x\\) \\(0\\leq x \\leq 1\\). ,\n\\[\nF_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(X^2\\leq y)=\\mbox{P}(X\\leq \\sqrt{y})=F_X\\left(\\sqrt{y}\\right)=\\sqrt{y}\n\\]Taking derivative yields:\n\\[\nf_Y(y)=\\frac{1}{2\\sqrt{y}}\n\\]\\(0 < y \\leq 1\\) 0 otherwise. Notice can’t \\(y=0\\) since dividing zero. problem since continuous distribution. verify proper pdf determining pdf integrates 1 domain:\n\\[\n\\int_0^1 \\frac{1}{2\\sqrt{y}} \\mathop{}\\!\\mathrm{d}y = \\sqrt{y}\\bigg|_0^1 = 1\n\\]\ncan also using R first create function can take vector input.Notice since domain original random variable non-negative, squared function inverse.pdf random variable \\(Y\\) plotted Figure 16.1.\nFigure 16.1: pdf transformed random variable \\(Y\\).\ncan see density much larger approach 0.","code":"\ny_pdf <- function(y) {\n  1/(2*sqrt(y))\n}\ny_pdf<- Vectorize(y_pdf)\nintegrate(y_pdf,0,1)## 1 with absolute error < 2.9e-15\ngf_line(y_pdf(seq(0.01,1,.01))~seq(0.01,1,.01),xlab=\"Y\",ylab=expression(f(y))) %>%\n  gf_theme(theme_bw())"},{"path":"TRANS.html","id":"the-pdf-method","chapter":"16 Transformations","heading":"16.2.4 The pdf method","text":"cdf method transforming continuous random variables also yields another method called pdf method. Recall cdf method tells us \\(X\\) continuous random variable cdf \\(F_X\\), \\(Y=g(X)\\), \\[\nF_Y(y)=F_X(g^{-1}(y))\n\\]can find pdf \\(Y\\) differentiating cdf:\n\\[\nf_Y(y)=\\frac{\\mathop{}\\!\\mathrm{d}}{\\mathop{}\\!\\mathrm{d}y}F_Y(y)=\\frac{\\mathop{}\\!\\mathrm{d}}{\\mathop{}\\!\\mathrm{d}y} F_X(g^{-1}(y)) = f_X(g^{-1}(y))\\bigg| \\frac{\\mathop{}\\!\\mathrm{d}}{\\mathop{}\\!\\mathrm{d}y}  g^{-1}(y) \\bigg|\n\\], long \\(g^{-1}\\) differentiable, can use method directly obtain pdf \\(Y\\).Note texts, portion expression \\(\\frac{\\mathop{}\\!\\mathrm{d}}{\\mathop{}\\!\\mathrm{d}y} g^{-1}(y)\\) sometimes referred Jacobian. need take absolute value transformation function \\(g(x)\\) decreasing function, \\[\nF_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X) \\leq y)=\\mbox{P}(X \\geq g^{-1}(y))= 1 - F_X(g^{-1}(y))\n\\]Exercise:\nRepeat previous example using pdf method.Since \\(X\\) uniform distribution, know \\(f_X(x)=1\\) \\(0\\leq x \\leq 1\\). Also, \\(g(x)=x^2\\) \\(g^{-1}(y)=\\sqrt{y}\\), differentiable. ,\\[\nf_Y(y)=f_X(\\sqrt{y})\\bigg|\\frac{\\mathop{}\\!\\mathrm{d}}{\\mathop{}\\!\\mathrm{d}y} \\sqrt{y}\\bigg| = \\frac{1}{2\\sqrt{y}}\n\\]","code":""},{"path":"TRANS.html","id":"simulation-2","chapter":"16 Transformations","heading":"16.2.5 Simulation","text":"can also get estimate distribution simulating random variable. cdf can find inverse, just like earlier lesson, sample uniform distribution apply inverse get distribution.earlier chapter example:Let \\(X\\) continuous random variable \\(f_X(x)=2x\\) \\(0 \\leq x \\leq 1\\).Now let’s find approximation distribution \\(Y = \\ln{X}\\) using simulation.cdf \\(X\\) \\(F_X(x)=x^2\\) \\(0 \\leq x \\leq 1\\). draw uniform random variable take square root simulate random variable \\(f_X(x)\\). replicate 10,000 times. R code, done , :Remember, using square root want inverse cdf , method, inverse transformation function using mathematical method. can point confusion.Figure 16.2 density plot simulated original random variable.\nFigure 16.2: density plot original using simulation.\nNow find distribution \\(Y\\) just apply transformation.Figure 16.3 density plot transformed random variable simulation. can see support \\(Y\\) \\(-\\infty < y \\leq 0\\) density tight near zero skewed left.\nFigure 16.3: density plot transformed random variable simulation.\n","code":"\nresults <- do(10000)*sqrt(runif(1))\ninspect(results)## \n## quantitative variables:  \n##      name   class         min        Q1    median        Q3       max      mean\n## ...1 sqrt numeric 0.005076088 0.4923543 0.7048933 0.8687366 0.9999369 0.6651345\n##            sd     n missing\n## ...1 0.238571 10000       0\nresults %>%\n  gf_density(~sqrt,xlab=\"X\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(y=\"\")\ny_results <- results %>%\n  transmute(y=log(sqrt))\ny_results %>%\n  gf_density(~y,xlab=\"X\")  %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(y=\"\")\ninspect(y_results)## \n## quantitative variables:  \n##      name   class       min         Q1     median         Q3           max\n## ...1    y numeric -5.283214 -0.7085567 -0.3497089 -0.1407153 -6.312309e-05\n##            mean        sd     n missing\n## ...1 -0.5046498 0.5048173 10000       0"},{"path":"TRANS.html","id":"multivariate-transformations","chapter":"16 Transformations","heading":"16.2.6 Multivariate Transformations","text":"discrete case, joint pmf, transformation univariate random variable, process similar see learned . continuous random variables, mathematics little difficult just use simulation.’s scenario. Suppose \\(X\\) \\(Y\\) independent random variables, uniformly distributed \\([5,6]\\).\n\\[\nX\\sim \\textsf{Unif}(5,6)\\hspace{1.5cm} Y\\sim \\textsf{Unif}(5,6)\n\\]Let \\(X\\) arrival time dinner \\(Y\\) friends arrival time. picked 5 6 time evening want meet. Also assume travel independently.Define \\(Z\\) transformation \\(X\\) \\(Y\\) \\(Z=|X-Y|\\). Thus \\(Z\\) absolute value difference arrival times. units \\(Z\\) hours. like explore distribution \\(Z\\). via calc III methods simulate instead.can use R obtain simulated values \\(X\\) \\(Y\\) (thus find \\(Z\\)).First, simulate 100,000 observations uniform distribution parameters 5 6. Assign random observations variable. Next, repeat process, assigning different variable. two vectors represent simulated values \\(X\\) \\(Y\\). Finally, obtain simulated values \\(Z\\) taking absolute value difference.Exercise:Complete code looking code .Figure 16.4 plot estimated density transformation.\nFigure 16.4: density absolute value difference uniform random variables.\nhistogram Figure 16.5.\nFigure 16.5: Histogram absolute value difference random variables.\nExercise:\nNow suppose whomever arrives first wait 5 minutes leave. probability eat together?Exercise:\nlong first person wait least 50% probability eating together?Let’s write function find cdf.Now test 5 minutes make sure function correct since determined value 0.15966.Let’s plot see cdf looks like.looks like somewhere around 15 minutes, quarter hour. find better answer finding root. code follows want find cdf equals 0.5. function uniroot() solves given equations roots want put cdf minus 0.5. words, uniroot() solves \\(f(x)=0\\) x.actually 0.292 hours, 17.5 minutes. round wait 18 minutes.","code":"\nset.seed(354)\nresults <- do(100000)*abs(diff(runif(2,5,6)))\nhead(results)##          abs\n## 1 0.03171229\n## 2 0.77846706\n## 3 0.29111599\n## 4 0.06700434\n## 5 0.08663187\n## 6 0.40622840\nresults %>%\n  gf_density(~abs) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"|X-Y|\",y=\"\")\nresults %>%\n  gf_histogram(~abs)%>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"|X-Y|\",y=\"\")\ninspect(results)## \n## quantitative variables:  \n##      name   class          min       Q1    median        Q3       max     mean\n## ...1  abs numeric 1.265667e-06 0.133499 0.2916012 0.4990543 0.9979459 0.332799\n##             sd      n missing\n## ...1 0.2358863 100000       0\ndata.frame(results) %>%\n  summarise(mean(abs<=5/60))##   mean(abs <= 5/60)\n## 1           0.15966\nz_cdf <- function(x) {\n  mean(results$abs<=x)\n}\nz_cdf<- Vectorize(z_cdf)\nz_cdf(5/60)## [1] 0.15966\ngf_line(z_cdf(seq(0,1,.01))~seq(0,1,.01),xlab=\"Time Difference\",ylab=\"CDF\") %>%\n  gf_theme(theme_bw())\nuniroot(function(x)z_cdf(x)-.5,c(.25,35))$root## [1] 0.2916077"},{"path":"TRANS.html","id":"homework-problems-15","chapter":"16 Transformations","heading":"16.3 Homework Problems","text":"Let \\(X\\) random variable let \\(g\\) function. point, clear \\(\\mbox{E}[g(X)]\\) necessarily equal \\(g(\\mbox{E}[X])\\).Let \\(X\\sim \\textsf{Expon}(\\lambda=0.5)\\) \\(g(X)=X^2\\). know \\(\\mbox{E}(X)=\\frac{1}{0.5}=2\\) \\(g(\\mbox{E}(X))=\\mbox{E}(X)^2=4\\). Use R find \\(\\mbox{E}[g(X)]\\). Make use fact R rexp() built , don’t create random variable generator.Let \\(X\\sim \\textsf{Binom}(n,\\pi)\\). pmf \\(Y = X+3\\)? Make sure specify domain \\(Y\\). [Note, used \\(p\\) probability success binomial distribution past chapters references use \\(\\pi\\) instead.]Let \\(X\\sim \\textsf{Binom}(n,\\pi)\\). pmf \\(Y = X+3\\)? Make sure specify domain \\(Y\\). [Note, used \\(p\\) probability success binomial distribution past chapters references use \\(\\pi\\) instead.]Let \\(X\\sim \\textsf{Expon}(\\lambda)\\). Let \\(Y=X^2\\). Find pdf \\(Y\\).Let \\(X\\sim \\textsf{Expon}(\\lambda)\\). Let \\(Y=X^2\\). Find pdf \\(Y\\).OPTIONAL: exercise 3, found pdf \\(Y=X^2\\) \\(X\\sim \\textsf{Expon}(\\lambda)\\). Rearrange pdf show \\(Y\\sim \\textsf{Weibull}\\) find parameters distribution.OPTIONAL: exercise 3, found pdf \\(Y=X^2\\) \\(X\\sim \\textsf{Expon}(\\lambda)\\). Rearrange pdf show \\(Y\\sim \\textsf{Weibull}\\) find parameters distribution.team two. tasked complete exercise. time takes , \\(T_1\\), likewise, teammate, \\(T_2\\), complete exercise independent random variables. Exercise completion time, minutes, distributed following pdf:team two. tasked complete exercise. time takes , \\(T_1\\), likewise, teammate, \\(T_2\\), complete exercise independent random variables. Exercise completion time, minutes, distributed following pdf:\\[\nf_T(t)= \\frac{-t}{200}+\\frac{3}{20}; 10 \\leq t \\leq30\n\\]Figure 16.6 plot pdf.\nFigure 16.6: pdf \\(T\\)\nwant find probability combined time less 40 minutes, \\(\\mbox{P}(T_1 + T_2 < 40)\\). solve steps problem. going use computational method mathematics long algebra intensive. welcome try mathematical solution like provide mathematical solution.Use integrate() function confirm valid pdf.Use integrate() function confirm valid pdf.Find cdf \\(T\\) mathematically.Find cdf \\(T\\) mathematically.use cdf simulate random variables distribution, need inverse cdf means solve quadratic equation. can mathematically just use function uniroot(). first, make sure understand use uniroot().use cdf simulate random variables distribution, need inverse cdf means solve quadratic equation. can mathematically just use function uniroot(). first, make sure understand use uniroot().check, know median distribution approximately 15.857. code show 15.857 approximately median. integrating pdf 10 15.857 confirm 0.5.Use uniroot() cdf confirm 15.857 median.create function take random uniform variable interval \\([0,1]\\) return value random variable, \\(T\\), exercise time. can use function simulate exercise times create new random variable sum. Complete R code check returns median.made function \\(y\\) since using \\(x\\) cdf. two function calls , can see ?Vectorize function just created using Vectorize() function. Check vectorized entering c(.5,.75) function. get 15.85786 20.00000 output.Vectorize function just created using Vectorize() function. Check vectorized entering c(.5,.75) function. get 15.85786 20.00000 output.ready. Let’s create data frame 10000 simulation time another 10000 teammates. Remember set seed. point may hard remember done. function created takes input vector random number uniform distribution applies inverse cdf generate random sample given pdf.ready. Let’s create data frame 10000 simulation time another 10000 teammates. Remember set seed. point may hard remember done. function created takes input vector random number uniform distribution applies inverse cdf generate random sample given pdf.numerical summary data plot density plot exercise times give us confidence simulated process correctly.numerical summary data plot density plot exercise times give us confidence simulated process correctly.Create new variable sum two exercise time find probability sum less 40.Create new variable sum two exercise time find probability sum less 40.","code":"\nintegrate(function(x)-x/200+3/20,10,15.857)## 0.4999389 with absolute error < 5.6e-15T <- function(y){\n  uniroot(function(x)\"YOUR CDF HERE as a function of x\"-y,c(10,30))$root\n}"},{"path":"EST.html","id":"EST","chapter":"17 Estimation Methods","heading":"17 Estimation Methods","text":"","code":""},{"path":"EST.html","id":"objectives-16","chapter":"17 Estimation Methods","heading":"17.1 Objectives","text":"Obtain method moments estimate parameter set parameters.Given random sample distribution, obtain likelihood function.Obtain maximum likelihood estimate parameter set parameters.Determine estimator unbiased.","code":""},{"path":"EST.html","id":"transition","chapter":"17 Estimation Methods","heading":"17.2 Transition","text":"started book descriptive models data moved onto probability models. probability models, characterizing experiments random processes using theory simulation. models using model random event make decisions data. models population used make decisions samples data. example, suppose flip fair coin 10 times, record number heads. population collection possible outcomes experiment. case, population infinite, run experiment repeatedly without limit. assume, model, number heads binomial distribution, know exact distribution outcomes. example, know exactly 24.61% time, obtain 5 heads 10 flips fair coin. can also use model characterize variance, equal 5 much different 5 . However, probability models highly dependent assumptions values parameters.point book, focus statistical models. Statistical models describe one variables relationships. use models make decisions population, predict future outcomes, . Often don’t know true underlying process; sample observations perhaps context. Using inferential statistics, can draw conclusions underlying process. example, suppose given coin don’t know whether fair. , flip number times obtain sample outcomes. can use sample decide whether coin fair.sense, ’ve already explored concepts. simulation examples, drawn observations population interest used observations estimate characteristics another population segment experiment. example, explored random variable \\(Z\\), \\(Z=|X - Y|\\) \\(X\\) \\(Y\\) uniform random variables. Instead dealing distribution \\(Z\\) directly, simulated many observations \\(Z\\) used simulation describe behavior \\(Z\\).Statistical models probability models separate. statistical models find relationships, explained portion variation, use probability models remaining random variation. Figure 17.1, demonstrate relationship two types models. first part studies, use univariate data statistical models estimate parameters probability model. develop sophisticated models include multivariate models.\nFigure 17.1: graphical representation probability statistics. probability, describe expect happen know underlying process; statistics, don’t know underlying process, must infer based representative samples.\n","code":""},{"path":"EST.html","id":"estimation","chapter":"17 Estimation Methods","heading":"17.3 Estimation","text":"Recall probability models, complete information population use describe expected behavior samples population. statistics given sample population know little nothing.lesson, discuss estimation. Given sample, like estimate population parameters. several ways . discuss two methods: method moments maximum likelihood.","code":""},{"path":"EST.html","id":"method-of-moments","chapter":"17 Estimation Methods","heading":"17.4 Method of Moments","text":"Recall earlier discussed moments. can refer \\(\\mbox{E}(X) = \\mu\\) first moment mean. , can refer \\(\\mbox{E}(X^k)\\) \\(k\\)th central moment \\(\\mbox{E}[(X-\\mu)^k]\\) \\(k\\) moment around mean. second moment around mean also known variance. important point POPULATION moments typically function parameters probability model.Suppose \\(X_1,X_2,...,X_n\\) sequence independent, identically distributed random variables distribution parameters \\(\\boldsymbol{\\theta}\\). provided random sample data, know population moments. However, can obtain sample moments. \\(k\\)th central sample moment denoted \\(\\hat{\\mu}_k\\) given \n\\[\n\\hat{\\mu}_k = \\frac{1}{n}\\sum_{=1}^n x_i^k\n\\]\\(k\\)th sample moment around mean denoted \\(\\hat{\\mu}'_k\\) given \n\\[\n\\hat{\\mu}'_k=\\frac{1}{n} \\sum_{=1}^n (x_i-\\bar{x})^k\n\\]value \\(\\hat{\\mu}\\) read “mu-hat”. hat denotes value estimate.can use sample moments estimate population moments since population moments usually functions distribution’s parameters, \\(\\boldsymbol{\\theta}\\). Thus, can solve parameters obtain method moments estimates \\(\\boldsymbol{\\theta}\\).technical, let’s look example.Example:\nSuppose \\(x_1,x_2,...,x_n\\) ..d., independent identically distributed, sample uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), don’t know \\(\\theta\\). , data consists positive random numbers don’t know upper bound. Find method moments estimator \\(\\theta\\), upper bound.know \\(X\\sim \\textsf{Unif}(,b)\\), \\(\\mbox{E}(X)=\\frac{+b}{2}\\). , case, \\(\\mbox{E}(X)={\\theta \\2}\\). first population moment. can estimate first sample moment, just sample mean:\n\\[\n\\hat{\\mu}_1=\\frac{1}{n}\\sum_{=1}^n x_i = \\bar{x}\n\\]best guess first population moment (\\(\\theta/2\\)) first sample moment (\\(\\bar{x}\\)). common sense perspective, hoping sample moment close value population moment, can set equal solve unknown population parameter. essentially simulations probability models. Solving \\(\\theta\\) yields method moments estimator \\(\\theta\\):\n\\[\n\\hat{\\theta}_{MoM}=2\\bar{x}\n\\]Note used second moments mean well. less intuitive still applicable. case know \\(X\\sim \\textsf{Unif}(,b)\\), \\(\\mbox{Var}(X)=\\frac{(b - )^2}{12}\\). , case, \\(\\mbox{Var}(X)=\\frac{\\theta ^2}{ 12}\\). use second sample moment mean \\(\\hat{\\mu}'_2=\\frac{1}{n} \\sum_{=1}^n (x_i-\\bar{x})^2\\) quite sample variance. fact, sample variance related second sample moment mean \\(\\hat{\\mu}'_2 = s^2 \\frac{n}{n-1}\\). Setting population moment sample moment equal solving get\\[\n\\hat{\\theta}_{MoM}=\\sqrt{\\frac{12n}{n-1}}s\n\\]decide better need criteria comparison. beyond scope book, common criteria unbiased minimum variance.method moments can used estimate one parameter well. simply incorporate higher order moments.Example:\nSuppose take ..d. sample normal distribution parameters \\(\\mu\\) \\(\\sigma\\). Find method moments estimates \\(\\mu\\) \\(\\sigma\\).First, remember know two population moments normal distribution:\n\\[\n\\mbox{E}(X)=\\mu \\hspace{1cm} \\mbox{Var}(X)=\\mbox{E}[(X-\\mu)^2]=\\sigma^2\n\\]Setting equal sample moments yields:\n\\[\n\\hat{\\mu}_{MoM}=\\bar{x} \\hspace{1cm} \\hat{\\sigma}_{MoM} = \\sqrt{\\frac{1}{n}\\sum_{=1}^n (x_i-\\bar{x})^2}\n\\], notice estimate \\(\\sigma\\) different sample standard deviation discussed earlier book. reason property estimators called unbiased. Notice treat data points random variables estimators random variables. can take expected value estimator equals parameter estimated, unbiased. Mathematically, written\n\\[\nE(\\hat{\\theta})=\\theta\n\\]\nUnbiased required property estimated many practitioners find desirable. words, unbiased means average estimator equal true value. Sample variance using \\(n-1\\) denominator unbiased estimate population variance.Exercise:\nshot 25 free throws make 21. Assuming binomial model fits. Find estimate probability making free throw.two ways approach problem depending define random variable. first case use binomial random variable, \\(X\\) number made free throws 25 attempts. case, ran experiment observed result 21. Recall binomial \\(E(X)=np\\) \\(n\\) number attempts \\(p\\) probability success. sample mean 21 since one data point. Using method moments, set first population mean equal first sample mean \\(np=\\frac{\\sum{x_i}}{m}\\), notice \\(n\\) number trials 25 \\(m\\) number data points 1, \\(25 \\hat{p} = 21\\). Thus \\(\\hat{p} = \\frac{21}{25}\\).second approach let \\(X_i\\) single free throw, Bernoulli random variable. variable takes values 0 miss 1 make free throw. Thus 25 data points. Bernoulli random variable \\(E(X)=p\\). sample \\(\\bar{x} = \\frac{21}{25}\\). Using method moments, set sample mean equal population mean. \\(E(X) = \\hat{p} = \\bar{x} = \\frac{21}{25}\\). natural estimate; estimate probability success number made free throws divided number shots. side note, unbiased estimator since\n\\[\nE(\\hat{p})=E\\left( \\sum{\\frac{X_i}{n}} \\right)\n\\]\\[\n=  \\sum{E\\left( \\frac{X_i}{n} \\right)}= \\sum{ \\frac{E\\left(X_i\\right)}{n}}=\\sum{\\frac{p}{n}}=\\frac{np}{n}=p\n\\]","code":""},{"path":"EST.html","id":"maximum-likelihood","chapter":"17 Estimation Methods","heading":"17.5 Maximum likelihood","text":"Recall using method moments involves finding values parameters cause population moments equal sample moments. Solving parameters yields method moments estimates.Next discuss one estimation method, maximum likelihood estimation. method, finding values parameters make observed data “likely”. order , first need introduce likelihood function.","code":""},{"path":"EST.html","id":"likelihood-function","chapter":"17 Estimation Methods","heading":"17.5.1 Likelihood Function","text":"Suppose \\(x_1,x_2,...,x_n\\) ..d. random sample distribution mass/density function \\(f_{X}(x;\\boldsymbol{\\theta})\\) \\(\\boldsymbol{\\theta}\\) parameters. Let’s take second explain notation. using bold symbol \\(\\boldsymbol{\\theta}\\) indicate vector, can one values. However, pmf/pdf \\(x\\) bold since scalar variable. probability models know \\(\\boldsymbol{\\theta}\\) use model make decision random variable \\(X\\).likelihood function denoted \\(L(\\boldsymbol{\\theta};x_1,x_2,...,x_n) = L(\\boldsymbol{\\theta};\\boldsymbol{x})\\). Now multiple instances random variable, use \\(\\boldsymbol{x}\\). Since random sample ..d., independent identically distributed, can write likelihood function product pmfs/pdfs:\n\\[\nL(\\boldsymbol{\\theta};\\boldsymbol{x})=\\prod_{=1}^n f_X(x_i;\\boldsymbol{\\theta})\n\\]likelihood function really pmf/pdf except instead variables random parameter(s) fixed, values variable known parameter(s) unknown. note notation, using semicolon pdf likelihood function denote known given. pmf/pdf parameters known thus follow semicolon. opposite case likelihood function.Let’s example help understand ideas.Example:\nSuppose presented coin unsure fairness. toss coin 50 times obtain 18 heads 32 tails. Let \\(\\pi\\) probability coin flip results heads, use \\(p\\) getting used two different common ways represent binomial parameter. likelihood function \\(\\pi\\)?binomial process, individual coin flip can thought Bernoulli experiment. , \\(x_1,x_2,...,x_{50}\\) ..d. sample \\(\\textsf{Binom}(1,\\pi)\\) , words, \\(\\textsf{Bernoulli}(\\pi)\\). \\(x_i\\) either 1 0. pmf \\(X\\), Bernoulli random variable, simply:\n\\[\nf_X(x;\\pi)= \\binom{1}{x} \\pi^x(1-\\pi)^{1-x} = \\pi^x(1-\\pi)^{1-x}\n\\]Notice makes sense\\[\nf_X(1)=P(X=1)= \\pi^1(1-\\pi)^{1-1}=\\pi\n\\]\\[\nf_X(0)=P(X=0)= \\pi^0(1-\\pi)^{1-0}=(1-\\pi)\n\\]Generalizing sample size \\(n\\), likelihood function :\n\\[\nL(\\pi;\\boldsymbol{x})=\\prod_{=1}^{n} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{\\sum_{=1}^{n} x_i}(1-\\pi)^{n-\\sum_{=1}^{n} x_i}\n\\]example \\(n=50\\) \\[\nL(\\pi;\\boldsymbol{x})=\\prod_{=1}^{50} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{18}(1-\\pi)^{32}\n\\]makes sense 18 successes, heads, 32 failures, tails. likelihood function function unknown parameter \\(\\pi\\).","code":""},{"path":"EST.html","id":"maximum-likelihood-estimation","chapter":"17 Estimation Methods","heading":"17.5.2 Maximum Likelihood Estimation","text":"likelihood function \\(L(\\boldsymbol{\\theta},\\boldsymbol{x})\\), need figure value \\(\\boldsymbol{\\theta}\\) makes data likely. words, need maximize \\(L\\) respect \\(\\boldsymbol{\\theta}\\).time (always), involve simple optimization calculus (.e., take derivative respect parameter, set 0 solve parameter). maximizing likelihood function calculus, often easier maximize log likelihood function, denoted \\(l\\) often referred “log-likelihood function”:\n\\[\nl(\\boldsymbol{\\theta};\\boldsymbol{x})= \\log L(\\boldsymbol{\\theta};\\boldsymbol{x})\n\\]\nNote since logarithm one--one, onto increasing, maximizing log-likelihood function equivalent maximizing likelihood function, maximum occur values parameters. using log now can take derivative sum instead product, thus making much easier.Example:\nContinuing example. Find maximum likelihood estimator \\(\\pi\\).Recall likelihood function \n\\[\nL(\\pi;\\boldsymbol{x})= \\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\n\\]Figure 17.2 plot likelihood function function unknown parameter \\(\\pi\\).\nFigure 17.2: Likelihood function 18 successes 50 trials\nvisual inspection, value \\(\\pi\\) makes data likely, maximizes likelihood function, something little less 0.4, actual value 0.36 indicated blue line Figure 17.2.maximize mathematical methods, need take derivative likelihood function respect \\(\\pi\\). can likelihood function continuous function. Even though binomial discrete random variable, likelihood continuous function.can find derivative likelihood function applying product rule:\n\\[\n{\\mathop{}\\!\\mathrm{d}L(\\pi;\\boldsymbol{x})\\\\mathop{}\\!\\mathrm{d}\\pi} = \\left(\\sum x_i\\right) \\pi^{\\sum x_i -1}(1-\\pi)^{n-\\sum x_i} + \\pi^{\\sum x_i}\\left(\\sum x_i -n\\right)(1-\\pi)^{n-\\sum x_i -1}\n\\]simplify , set 0, solve \\(\\pi\\). However, may easier use log-likelihood function:\n\\[\nl(\\pi;\\boldsymbol{x})=\\log L(\\pi;\\boldsymbol{x})= \\log \\left(\\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\\right) = \\sum x_i \\log \\pi + (n-\\sum x_i)\\log (1-\\pi)\n\\]Now, taking derivative require product rule:\n\\[\n{\\mathop{}\\!\\mathrm{d}l(\\pi;\\boldsymbol{x})\\\\mathop{}\\!\\mathrm{d}\\pi}= {\\sum x_i \\\\pi} - {n-\\sum x_i\\(1-\\pi)}\n\\]Setting equal 0 yields:\n\\[\n{\\sum x_i \\\\pi} ={n-\\sum x_i\\(1-\\pi)}\n\\]Solving \\(\\pi\\) yields\n\\[\n\\hat{\\pi}_{MLE}={\\sum x_i \\n}\n\\]Note technically, confirm function concave critical value, ensuring \\(\\hat{\\pi}_{MLE}\\) , fact, maximum:\n\\[\n{\\mathop{}\\!\\mathrm{d}^2 l(\\pi;\\boldsymbol{x})\\\\mathop{}\\!\\mathrm{d}\\pi^2}= {-\\sum x_i \\\\pi^2} - {n-\\sum x_i\\(1-\\pi)^2}\n\\]value negative relevant values \\(\\pi\\), \\(l\\) concave \\(\\hat{\\pi}_{MLE}\\) maximum.case example (18 heads 50 trials), \\(\\hat{\\pi}_{MLE}=18/50=0.36\\).seems make sense. best guess probability heads number observed heads divided number trials. great deal algebra calculus appears obvious answer. However, difficult problems, obvious use MLE.","code":"## Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided."},{"path":"EST.html","id":"numerical-methods","chapter":"17 Estimation Methods","heading":"17.5.3 Numerical Methods","text":"obtaining MLEs, times analytical methods (calculus) feasible possible. Pruim book (R. J. Pruim 2011), good example regarding data Old Faithful Yellowstone National Park. need load fastR2 package example.faithful data set preloaded R contains 272 observations 2 variables: eruption time minutes waiting time next eruption. plot eruption durations, notice distribution appears bimodal, see Figure 17.3.\nFigure 17.3: Histogram eruption durations Old Faithful.\nWithin section, distribution appears somewhat bell-curve-ish ’ll model eruption time mixture two normal distributions. mixture, proportion \\(\\alpha\\) eruptions belong one normal distribution remaining \\(1-\\alpha\\) belong normal distribution. density function eruptions given :\n\\[\n\\alpha f(x;\\mu_1,\\sigma_1)+(1-\\alpha)f(x;\\mu_2,\\sigma_2)\n\\]\\(f\\) pdf normal distribution parameters specified.five parameters estimate: \\(\\alpha, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\). Obviously, estimation differentiation feasible thus use numerical methods. code less spirit tidyverse want see example. Try work way code :Next write function log-likelihood function. R vector based programming language send theta function vector argument.Find sample mean standard deviation eruption data use starting points optimization routine.Use function nlmax() maximize non-linear log-likelihood function., according MLEs, 34.84% eruptions belong first normal distribution (one left). Furthermore parameters first distribution mean 2.019 standard deviation 0.236. Likewise, 65.16% eruptions belong second normal mean 4.27 standard deviation 0.437.Plotting density atop histogram shows fairly good fit:\nFigure 17.4: Histogram eruption duration estimated mixture normals plotted top.\nfairly elaborate example cool. can see power method software.","code":"\nlibrary(fastR2)\n# Define function for pdf of eruptions as a mixture of normals\ndmix<-function(x,alpha,mu1,mu2,sigma1,sigma2){\n  if(alpha < 0) dnorm(x,mu2,sigma2)\n  if(alpha > 1) dnorm(x,mu1,sigma1)\n  if(alpha >= 0 && alpha <=1){\n    alpha*dnorm(x,mu1,sigma1)+(1-alpha)*dnorm(x,mu2,sigma2)\n  }\n}\n# Create the log-likelihood function\nloglik<-function(theta,x){\n  alpha=theta[1]\n  mu1=theta[2]\n  mu2=theta[3]\n  sigma1=theta[4]\n  sigma2=theta[5]\n  density<-function(x){\n    if(alpha<0) return (Inf)\n    if(alpha>1) return (Inf)\n    if(sigma1<0) return (Inf)\n    if(sigma2<0) return (Inf)\n    dmix(x,alpha,mu1,mu2,sigma1,sigma2)\n  }\n  sum(log(sapply(x,density)))\n}\nm<-mean(faithful$eruptions)\ns<-sd(faithful$eruptions)\nmle<-nlmax(loglik,p=c(0.5,m-1,m+1,s,s),x=faithful$eruptions)$estimate\nmle## [1] 0.3484040 2.0186065 4.2733410 0.2356208 0.4370633\ndmix2<-function(x) dmix(x,mle[1],mle[2],mle[3],mle[4],mle[5])\n#y_old<-dmix2(seq(1,6,.01))\n#x_old<-seq(1,6,.01)\n#dens_data<-data.frame(x=x_old,y=y_old)\n#faithful%>%\n#gf_histogram(~eruptions,fill=\"cyan\",color = \"black\") %>%\n#  gf_curve(y~x,data=dens_data)%>%\n#  gf_theme(theme_bw()) %>%\n#  gf_labs(x=\"Duration in minutes\",y=\"Count\") \nhist(faithful$eruptions,breaks=40,freq=F,main=\"\",xlab=\"Duration in minutes.\")\ncurve(dmix2,from=1,to=6,add=T)"},{"path":"EST.html","id":"homework-problems-16","chapter":"17 Estimation Methods","heading":"17.6 Homework Problems","text":"Notes, found take sample uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), method moments estimate \\(\\theta\\) \\(\\hat{\\theta}_{MoM}=2\\bar{x}\\). Suppose sample consists following values:\n\\[\n0.2 \\hspace{0.4cm} 0.9 \\hspace{0.4cm} 1.9 \\hspace{0.4cm} 2.2 \\hspace{0.4cm} 4.7 \\hspace{0.4cm} 5.1\n\\]\\(\\hat{\\theta}_{MoM}\\) sample?wrong estimate?Show estimator unbiased.ADVANCED: Use simulation R find often method moment estimator less maximum observed value, (\\(\\hat{\\theta}_{MoM} < \\max x\\)). Report answer various sizes samples. can just pick arbitrary value \\(\\theta\\) sample uniform. However, minimum must 0.Let \\(x_1,x_2,...,x_n\\) simple random sample exponentially distributed population parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MoM}\\).Let \\(x_1,x_2,...,x_n\\) simple random sample exponentially distributed population parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MoM}\\).Let \\(x_1,x_2,...,x_n\\) ..d. random sample exponentially distributed population parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MLE}\\).Let \\(x_1,x_2,...,x_n\\) ..d. random sample exponentially distributed population parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MLE}\\).mathematically difficult determine estimators found questions 2 3 unbiased. Since sample mean denominator; mathematically may work joint pdf. instead, use simulation get sense whether method moments estimator exponential distribution unbiased.mathematically difficult determine estimators found questions 2 3 unbiased. Since sample mean denominator; mathematically may work joint pdf. instead, use simulation get sense whether method moments estimator exponential distribution unbiased.Find maximum likelihood estimator \\(\\theta\\) \\(X\\sim\\textsf{Unif}(0,\\theta)\\). Compare method moments estimator found. Hint: take derivative likelihood function.Find maximum likelihood estimator \\(\\theta\\) \\(X\\sim\\textsf{Unif}(0,\\theta)\\). Compare method moments estimator found. Hint: take derivative likelihood function.","code":""},{"path":"CS3.html","id":"CS3","chapter":"18 Case Study","heading":"18 Case Study","text":"","code":""},{"path":"CS3.html","id":"objectives-17","chapter":"18 Case Study","heading":"18.1 Objectives","text":"Define use properly context new terminology: point estimate, null hypothesis, alternative hypothesis, hypothesis test, randomization, permutation test, test statistic, p-value.Define use properly context new terminology: point estimate, null hypothesis, alternative hypothesis, hypothesis test, randomization, permutation test, test statistic, p-value.Conduct hypothesis test using permutation test include 4 steps.Conduct hypothesis test using permutation test include 4 steps.","code":""},{"path":"CS3.html","id":"introduction","chapter":"18 Case Study","heading":"18.2 Introduction","text":"now foundation move statistical modeling. First begin inference, use ideas estimation variance estimates make decisions population. also briefly introduce ideas prediction. final block material, examine common linear models use prediction inference.","code":""},{"path":"CS3.html","id":"foundation-for-inference","chapter":"18 Case Study","heading":"18.3 Foundation for inference","text":"Suppose professor randomly splits students class two groups: students left students right. \\(\\hat{p}_{_L}\\) \\(\\hat{p}_{_R}\\) represent proportion students Apple product left right, respectively, surprised \\(\\hat{p}_{_L}\\) exactly equal \\(\\hat{p}_{_R}\\)?proportions probably close , probably exactly . probably observe small difference due chance.Exercise:\ndon’t think side room person sits class related whether person owns Apple product, assumption making relationship two variables?67Studying randomness form key focus statistical modeling. block, ’ll explore type randomness context several applications, ’ll learn new tools ideas can applied help make decisions data.","code":""},{"path":"CS3.html","id":"randomization-case-study-gender-discrimination","chapter":"18 Case Study","heading":"18.4 Randomization case study: gender discrimination","text":"consider study investigating gender discrimination 1970s, set context personnel decisions within bank.68 research question hope answer , “females discriminated promotion decisions made male managers?”","code":""},{"path":"CS3.html","id":"variability-within-data","chapter":"18 Case Study","heading":"18.4.1 Variability within data","text":"participants study 48 male bank supervisors attending management institute University North Carolina 1972. asked assume role personnel director bank given personnel file judge whether person promoted branch manager position. files given participants identical, except half indicated candidate male half indicated candidate female. files randomly assigned subjects.Exercise:\nobservational study experiment? type study impact can inferred results?69For supervisor, recorded gender associated assigned file promotion decision. Using results study summarized table , like evaluate whether females unfairly discriminated promotion decisions. study, smaller proportion females promoted males (0.583 versus 0.875), unclear whether difference provides convincing evidence females unfairly discriminated .\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Decision}\\\\\n& & \\mbox{Promoted} & \\mbox{Promoted} & \\mbox{Total}  \\\\\n& \\hline \\mbox{male} & 21 & 3 & 24  \\\\\n\\textbf{Gender}& \\mbox{female} & 14 & 10 & 24  \\\\\n& \\mbox{Total} & 35 & 13 & 48  \\\\\n\\end{array}\n\\]Thought Question:\nStatisticians sometimes called upon evaluate strength evidence. looking rates promotion males females study, might tempted immediately conclude females discriminated ?large difference promotion rates (58.3% females versus 87.5% males) suggest might discrimination women promotion decisions. people come conclusion think sample statistics actual population parameters. yet sure observed difference represents discrimination just random variability. Generally, fluctuation sample data; conducted experiment , likely get different values. also wouldn’t expect sample proportions males females exactly equal, even truth promotion decisions independent gender. make decision, must understand random variability use compare observed difference.question reminder observed outcomes sample may perfectly reflect true relationships variables underlying population. table shows 7 fewer promotions female group male group, difference promotion rates 29.2% \\(\\left( \\frac{21}{24} - \\frac{14}{24} = 0.292 \\right)\\). observed difference call point estimate true effect. point estimate difference large, sample size study small, making unclear observed difference represents discrimination whether simply due chance. label two competing claims, chance discrimination, \\(H_0\\) \\(H_A\\):\\(H_0\\): Null hypothesis. variables gender decision independent. relationship, observed difference proportion males females promoted, 29.2%, due chance.\\(H_A\\): Alternative hypothesis. variables gender decision independent. difference promotion rates 29.2% due chance, equally qualified females less likely promoted males.Hypothesis testing\nhypotheses part called hypothesis test. hypothesis test statistical technique used evaluate competing claims using data. Often times, null hypothesis takes stance difference effect thus skeptical research claim. null hypothesis data notably disagree, reject null hypothesis favor alternative hypothesis.Don’t worry aren’t master hypothesis testing end lesson. ’ll discuss ideas details many times throughout block.mean null hypothesis, says variables gender decision unrelated, true? mean banker decide whether promote candidate without regard gender indicated file. , difference promotion percentages due way files randomly divided bankers, randomization just happened give rise relatively large difference 29.2%.Consider alternative hypothesis: bankers influenced gender listed personnel file. true, especially influence substantial, expect see difference promotion rates male female candidates. gender bias females, expect smaller fraction promotion recommendations female personnel files relative male files.choose two competing claims assessing data conflict much \\(H_0\\) null hypothesis deemed reasonable. case, data support \\(H_A\\), reject notion independence conclude data provide strong evidence discrimination. , determining much difference promotion rates happen random variation compare observed difference. make decision based probability considerations.","code":""},{"path":"CS3.html","id":"simulating-the-study","chapter":"18 Case Study","heading":"18.4.2 Simulating the study","text":"table data shows 35 bank supervisors recommended promotion 13 . Now, suppose bankers’ decisions independent gender, null hypothesis true. , conducted experiment different random assignment files, differences promotion rates based random fluctuation. can actually perform randomization, simulates happened bankers’ decisions independent gender distributed files differently.70 walk steps next.First let’s import data.Let’s inspect data set.Let’s look table data, showing gender versus decision.Let’s categorical data cleaning. get tally() results look like initial table, need change variables characters factors reorder levels. default, factor levels ordered alphabetically, want promoted male appear first levels table.use mutate_if() convert character variables factors fct_relevel() reorder levels.Now data form want, ready conduct permutation test, simulation happened bankers’ decisions independent gender distributed files differently. think simulation, imagine actually personnel files. thoroughly shuffle 48 personnel files, 24 labeled male 24 labeled female, deal files two stacks. deal 35 files first stack, represent 35 supervisors recommended promotion. second stack 13 files, represent 13 supervisors recommended promotion. , keep number files promoted not_promoted categories, imagine simply shuffling male female labels around. Remember files identical except listed gender. simulation assumes gender important , thus, can randomly assign files supervisors. , original data, tabulate results determine fraction male female candidates promoted. Since don’t actually physically files, shuffle via computer code.Since randomization files simulation independent promotion decisions, difference two fractions entirely due chance. following code shows results simulation.shuffle() function randomly rearranges gender column keeping decision column . really sampling without replacement. , randomly sample 35 personnel files promoted 13 personnel files not_promoted.Exercise:\ndifference promotion rates two simulated groups? compare observed difference, 29.2%, actual study?71Calculating hand help simulation, must write function use existing one. use diffprop() mosaic package. code find difference original data :Notice subtracting proportion males promoted proportion females promoted. impact results arbitrary decision. just need consistent analysis. prefer use positive values, can adjust order easily.Notice done ; developed single value metric measure relationship gender decision. single value metric called test statistic. used number different metrics, include just difference number promoted males females. key idea hypothesis testing decide test statistic, need find distribution test statistic, assuming null hypothesis true.","code":"\ndiscrim <- read_csv(\"data/discrimination_study.csv\")\ninspect(discrim)## \n## categorical variables:  \n##       name     class levels  n missing\n## 1   gender character      2 48       0\n## 2 decision character      2 48       0\n##                                    distribution\n## 1 female (50%), male (50%)                     \n## 2 promoted (72.9%), not_promoted (27.1%)\ntally(~gender + decision, discrim, margins = TRUE)##         decision\n## gender   not_promoted promoted Total\n##   female           10       14    24\n##   male              3       21    24\n##   Total            13       35    48\ndiscrim <- discrim %>%\n  mutate_if(is.character, as.factor) %>%\n  mutate(gender = fct_relevel(gender, \"male\"),\n         decision = fct_relevel(decision, \"promoted\"))\nhead(discrim)## # A tibble: 6 x 2\n##   gender decision    \n##   <fct>  <fct>       \n## 1 female not_promoted\n## 2 female not_promoted\n## 3 male   promoted    \n## 4 female promoted    \n## 5 female promoted    \n## 6 female promoted\ntally(~gender + decision, discrim, margins = TRUE)##         decision\n## gender   promoted not_promoted Total\n##   male         21            3    24\n##   female       14           10    24\n##   Total        35           13    48\nset.seed(101)\ntally(~shuffle(gender) + decision, discrim, margins = TRUE)##                decision\n## shuffle(gender) promoted not_promoted Total\n##          male         18            6    24\n##          female       17            7    24\n##          Total        35           13    48\n(obs <- diffprop(decision ~ gender, data = discrim))##   diffprop \n## -0.2916667\ndiffprop(decision ~ fct_relevel(gender, \"female\"), data = discrim)##  diffprop \n## 0.2916667"},{"path":"CS3.html","id":"checking-for-independence","chapter":"18 Case Study","heading":"18.4.3 Checking for independence","text":"computed one possible difference null hypothesis exercise , represents one difference due chance. Repeating simulation, get another difference due chance: -0.042. another: 0.208. repeat simulation enough times good idea represents distribution differences chance alone. , difference really relationship gender promotion decision. using simulation actually finite number permutations gender label. lesson counting, 48 labels 24 male 24 female. Thus total number ways arrange labels differently :\\[\n\\frac{48!}{24!\\cdot24!} \\approx 3.2 \\cdot 10^{13}\n\\]often case, number possible permutations large find hand even via code. Thus, use simulation, subset possible permutations, approximate permutation test. Using simulation way called randomization test.Let’s simulate experiment plot simulated values difference proportions male female files recommended promotion.Figure 18.1, insert vertical line value observed difference.\nFigure 18.1: Distribution test statistic.\nNote distribution simulated differences centered around 0 roughly symmetrical. centered zero simulated differences way made distinction men women. makes sense: expect differences chance alone fall around zero random fluctuation simulation assumption null hypothesis. histogram also looks like normal distribution; coincidence, result Central Limit Theorem, learn later block.Example:\noften observe difference extreme -29.2% (-0.292) according figure? (Often, sometimes, rarely, never?)appears difference extreme -29.2% due chance alone happen rarely. can estimate probability using results object.simulations, 2.6% simulated test statistics less equal observed test statistic, extreme relative null hypothesis. low probability indicates observing large difference proportions chance alone rare. probability known p-value. p-value conditional probability, probability observed value extreme given null hypothesis true.also found exact p-value using hypergeometric distribution. 13 not_promoted positions, anywhere 0 13 females promoted. observed 10 females promoted. Thus, exact p-value hypergeometric distribution probability 10 females promoted (extreme observed) select 13 people pool 24 males 24 females, selection done without replacement., see low probability, 2.4%, observing 10 females promoted, given null hypothesis true.observed difference -29.2% rare (low probability) event truly impact listing gender personnel files. provides us two possible interpretations study results, context hypotheses:\\(H_0\\): Null hypothesis. Gender effect promotion decision, observed difference large happen rarely.\\(H_A\\): Alternative hypothesis. Gender effect promotion decision, observed actually due equally qualified women discriminated promotion decisions, explains large difference -29.2%.conduct formal studies, reject skeptical position (\\(H_0\\)) data strongly conflict position.72In analysis, determined ~ 2% probability obtaining test statistic difference female male promotion proportions 29.2% larger assuming gender impact. conclude data provide sufficient evidence gender discrimination women supervisors. case, reject null hypothesis favor alternative hypothesis.Statistical inference practice making decisions conclusions data context uncertainty. Errors occur, just like rare events, data set hand might lead us wrong conclusion. given data set may always lead us correct conclusion, statistical inference gives us tools control evaluate often errors occur.Let’s summarize case study. research question data test question. performed 4 steps:State null alternative hypotheses.Compute test statistic.Determine p-value.Draw conclusion.decided use randomization test, simulation, answer question. creating randomization distribution, attempted satisfy 3 guiding principles.consistent null hypothesis.\nneed simulate world null hypothesis true. don’t , won’t testing null hypothesis. problem, assumed gender promotion independent.Use data original sample.\noriginal data shed light aspects distribution determined null hypothesis. problem, used difference promotion rates. data give us distribution direction, gives us idea large difference.Reflect way original data collected.\n48 files 48 supervisors. total 35 files recommended promotion. keep simulation.remainder block expands ideas case study.","code":"\nfactorial(48) / (factorial(24)*factorial(24))## [1] 3.22476e+13\nset.seed(2022)\nresults <- do(10000)*diffprop(decision ~ shuffle(gender), data = discrim)\nresults %>%\n  gf_histogram(~diffprop) %>%\n  gf_vline(xintercept = -0.2916667 ) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x = \"Difference in proportions\", y = \"Counts\",\n          title = \"Gender discrimination in hiring permutation test\",\n          subtitle = \"Test statistic is difference in promotion for female and male\")\nresults %>%\n  summarise(p_value = mean(diffprop <= obs))##   p_value\n## 1  0.0257\n1 - phyper(9, 24, 24, 13)## [1] 0.02449571"},{"path":"CS3.html","id":"homework-problems-17","chapter":"18 Case Study","heading":"18.5 Homework Problems","text":"Side effects Avandia\nRosiglitazone active ingredient controversial type~2 diabetes medicine Avandia linked increased risk serious cardiovascular problems stroke, heart failure, death. common alternative treatment pioglitazone, active ingredient diabetes medicine called Actos. nationwide retrospective observational study 227,571 Medicare beneficiaries aged 65 years older, found 2,593 67,593 patients using rosiglitazone 5,386 159,978 using pioglitazone serious cardiovascular problems. data summarized contingency table .Side effects AvandiaRosiglitazone active ingredient controversial type~2 diabetes medicine Avandia linked increased risk serious cardiovascular problems stroke, heart failure, death. common alternative treatment pioglitazone, active ingredient diabetes medicine called Actos. nationwide retrospective observational study 227,571 Medicare beneficiaries aged 65 years older, found 2,593 67,593 patients using rosiglitazone 5,386 159,978 using pioglitazone serious cardiovascular problems. data summarized contingency table .\\[\n\\begin{array}{cc|ccc} & & &\\textit{Cardiovascular problems}\\\\\n& & \\text{Yes}  & \\text{} & \\textbf{Total}  \\\\\n& \\hline \\text{Rosiglitazone}   & 2,593 & 65,000        & 67,593 \\\\\n\\textit{Treatment}& \\text{Pioglitazone}     & 5,386     & 154,592   & 159,978  \\\\\n& \\textbf{Total}            & 7,979 & 219,592       & 227,571 \\\\\n\\end{array}\n\\]Determine following statements true false. false, explain . reasoning may wrong even statement’s conclusion correct. cases, statement considered false.Since patients pioglitazone cardiovascular problems (5,386 vs. 2,593), can conclude rate cardiovascular problems pioglitazone treatment higher.Since patients pioglitazone cardiovascular problems (5,386 vs. 2,593), can conclude rate cardiovascular problems pioglitazone treatment higher.data suggest diabetic patients taking rosiglitazone likely cardiovascular problems since rate incidence (2,593 / 67,593 = 0.038) 3.8% patients treatment, (5,386 / 159,978 = 0.034) 3.4% patients pioglitazone.data suggest diabetic patients taking rosiglitazone likely cardiovascular problems since rate incidence (2,593 / 67,593 = 0.038) 3.8% patients treatment, (5,386 / 159,978 = 0.034) 3.4% patients pioglitazone.fact rate incidence higher rosiglitazone group proves rosiglitazone causes serious cardiovascular problems.fact rate incidence higher rosiglitazone group proves rosiglitazone causes serious cardiovascular problems.Based information provided far, tell difference rates incidences due relationship two variables due chance.Based information provided far, tell difference rates incidences due relationship two variables due chance.Heart transplants\nStanford University Heart Transplant Study conducted determine whether experimental heart transplant program increased lifespan. patient entering program designated official heart transplant candidate, meaning gravely ill likely benefit new heart. patients got transplant . variable indicates group patients ; patients treatment group got transplant control group . Another variable called used indicate whether patient alive end study.\nstudy, 34 patients control group, 4 alive end study. 69 patients treatment group, 24 alive. contingency table summarizes results.Heart transplantsThe Stanford University Heart Transplant Study conducted determine whether experimental heart transplant program increased lifespan. patient entering program designated official heart transplant candidate, meaning gravely ill likely benefit new heart. patients got transplant . variable indicates group patients ; patients treatment group got transplant control group . Another variable called used indicate whether patient alive end study.study, 34 patients control group, 4 alive end study. 69 patients treatment group, 24 alive. contingency table summarizes results.\\[\n\\begin{array}{cc|ccc} & & &\\textit{Group}\\\\\n& & \\text{Control}  & \\text{Treatment}  & \\textbf{Total}  \\\\\n& \\hline \\text{Alive}   & 4     & 24            & 28 \\\\\n\\textit{Outcome}& \\text{Dead}   & 30        & 45            & 75  \\\\\n& \\textbf{Total}            & 34        & 69            & 103\\\\\n\\end{array}\n\\]data file called Stanford_heart_study.csv. Read data answer following questions.proportion patients treatment group proportion patients control group died?claims tested? Use null alternative hypothesis notation used lesson notes.claims tested? Use null alternative hypothesis notation used lesson notes.paragraph describes set approach, without using statistical software. Fill blanks number phrase, whichever appropriate.paragraph describes set approach, without using statistical software. Fill blanks number phrase, whichever appropriate.write alive _______ cards representing patients alive end study, dead _______ cards representing patients . , shuffle cards split two groups: one group size _______ representing treatment, another group size _______ representing control. calculate difference proportion cards control treatment groups (control - treatment), just positive observed value, record value. repeat many times build distribution centered _______. Lastly, calculate fraction simulations simulated differences proportions _______ _______. fraction simulations, empirical p-value, low, conclude unlikely observed outcome chance null hypothesis rejected favor alternative.Next perform simulation use results decide effectiveness transplant program.Find observed value test statistic, decided use difference proportions.Find observed value test statistic, decided use difference proportions.Simulate 1000 values test statistic using shuffle() variable group.Simulate 1000 values test statistic using shuffle() variable group.Plot distribution results. Include vertical line observed value. Clean plot presenting decision maker.Plot distribution results. Include vertical line observed value. Clean plot presenting decision maker.Find p-value. Think carefully extreme mean.Find p-value. Think carefully extreme mean.Decide treatment effective.Decide treatment effective.","code":"One approach for investigating whether or not the treatment is effective is to use a randomization technique.  "},{"path":"HYPOTESTSIM.html","id":"HYPOTESTSIM","chapter":"19 Hypothesis Testing with Simulation","heading":"19 Hypothesis Testing with Simulation","text":"","code":""},{"path":"HYPOTESTSIM.html","id":"objectives-18","chapter":"19 Hypothesis Testing with Simulation","heading":"19.1 Objectives","text":"Know properly use terminology hypothesis test: null hypothesis, alternative hypothesis, test statistic, p-value, randomization test, one-sided test, two-sided test, statistically significant, significance level, type error, type II error, false positive, false negative, null distribution, sampling distribution.Know properly use terminology hypothesis test: null hypothesis, alternative hypothesis, test statistic, p-value, randomization test, one-sided test, two-sided test, statistically significant, significance level, type error, type II error, false positive, false negative, null distribution, sampling distribution.Conduct four steps hypothesis test using randomization.Conduct four steps hypothesis test using randomization.Discuss explain ideas decision errors, one-sided versus two-sided tests, choice significance level.Discuss explain ideas decision errors, one-sided versus two-sided tests, choice significance level.","code":""},{"path":"HYPOTESTSIM.html","id":"decision-making-under-uncertainty","chapter":"19 Hypothesis Testing with Simulation","heading":"19.2 Decision making under uncertainty","text":"point, useful take look book going. case study, want discuss little detail. first looked descriptive models help us understand data. also required us get familiar software. learned graphical summaries, data collection methods, summary metrics.Next learned probability models. models allowed us use assumptions small number parameters make statements data (sample) also simulate data. found close tie probability models statistical models. first efforts statistical modeling, started use data create estimates parameters probability model. work resulted point estimates via method moments maximum likelihood.Now moving depth statistical models. going tie ideas together. going use data sample ideas randomization make conclusions population. require probability models, descriptive models, new ideas terminology. generate point estimates metric designed answer research question find ways determine variability metric. Figure 19.1, demonstrate relationship probability statistical models.\nFigure 19.1: graphical representation probability statistics. probability, describe expect happen sample know underlying process; statistics, don’t know underlying process, must infer population based representative samples.\nComputational/Mathematical hypothesis testing/confidence intervals contextWe going using data sample population make decisions population. many approaches techniques . course, introducing exploring different approaches; establishing foundations. can imagine, ideas varied, subtle, times difficult. just exposing foundational ideas. want make sure understand become accomplished practitioner, must master fundamentals continue learn advanced ideas course.Historically, two approaches statistical decision making, hypothesis testing confidence intervals. mathematical foundation, equivalent, , sometimes practice, offer different perspectives problem. learn approaches.engines drive numeric results decision making model either mathematical computational. reality, computational methods mathematics behind , mathematical methods often require computer computations. real distinction assumptions making population. Mathematical solutions typically stricter assumptions, thus leading tractable mathematical solution sampling distribution test statistic, computational models relax assumptions may require extensive computational power. Like problems, trade consider trying choose approach better. one universal best method. methods perform better certain contexts. important understand computational methods bootstrap need know.","code":""},{"path":"HYPOTESTSIM.html","id":"introduction-1","chapter":"19 Hypothesis Testing with Simulation","heading":"19.3 Introduction","text":"chapter introduce hypothesis testing. really extension last chapter, case study. put emphasis terms core concepts. chapter, use computational solution lead us thinking mathematical solutions.73 role analyst always key regardless perceived power computer. analyst must take research question translate numeric metric evaluation. analyst must decide type data collection evaluate question. analyst must evaluate variability metric determine means relation original research question. analyst must propose answer.","code":""},{"path":"HYPOTESTSIM.html","id":"hypothesis-testing","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4 Hypothesis testing","text":"continue emphasize ideas hypothesis testing data-driven example, also via analogy US court system. let’s begin journey.Example:\nannoyed TV commercials. suspect commercials basic TV channels, typically local area channels, premium channels pay extra . test claim, hypothesis, want collect data draw conclusion. collect data?one approach: watch 20 random half hour shows live TV. Ten hours basic TV ten premium. case, record total length commercials show.Exercise:\nenough data? decide friends help , actually watched 5 hours got rest data friends. problem?determine enough data without type subject matter knowledge. First, need decide metric use determine difference exists (come ). Second, need decide big difference, practical standpoint, interest. loss 1 minute TV show enough say difference? 5 minutes? statistical questions, depend context problem often require subject matter expertise answer. Often, data collected without thought considerations. several methods attempt answer questions. loosely called sample size calculations. book focus sample size calculations leave reader learn sources. second exercise question, answer depends protocol operating procedures used. friends trained measure length commercials, counts ad, skills verified, probably problem use collect data. Consistency measurement key.file ads.csv contains data. Let’s read data R start summarize. Remember load appropriate R packages.Notice data may tidy; row represent single observation? don’t know data obtained, row represents different friend watches one basic one premium channel, possible data tidy. want observation single TV show, let’s clean , tidy, data. Remember ask “want R ?” “need ?” want one column specifies channel type another specify total length commercials.need R put, pivot, data longer form. need function pivot_longer(). information type vignette(\"pivot\") command prompt R.Looks good. Let’s summarize data.summary want, since want break channel type.Exercise:\nVisualize data using boxplot.appears premium channels skewed left. density plot may help us compare distributions see skewness, Figure 19.2.\nFigure 19.2: Commercial length broken channel type.\ndata, looks like difference two type channels, must put research question metric allow us reach decision. hypothesis test. reminder, steps areState null alternative hypotheses.Compute test statistic.Determine p-value.Draw conclusion., let’s visit example hypothesis testing become common knowledge us, US criminal trial system. also use cadet honor system. analogy allows us remember apply steps.","code":"\nads <- read_csv(\"data/ads.csv\")\nads## # A tibble: 10 x 2\n##    basic premium\n##    <dbl>   <dbl>\n##  1  6.95    3.38\n##  2 10.0     7.8 \n##  3 10.6     9.42\n##  4 10.2     4.66\n##  5  8.58    5.36\n##  6  7.62    7.63\n##  7  8.23    4.95\n##  8 10.4     8.01\n##  9 11.0     7.8 \n## 10  8.52    9.58\nglimpse(ads)## Rows: 10\n## Columns: 2\n## $ basic   <dbl> 6.950, 10.013, 10.620, 10.150, 8.583, 7.620, 8.233, 10.350, 11~\n## $ premium <dbl> 3.383, 7.800, 9.416, 4.660, 5.360, 7.630, 4.950, 8.013, 7.800,~\nads <- ads %>%\n  pivot_longer(cols = everything(), names_to = \"channel\", values_to = \"length\")\nads## # A tibble: 20 x 2\n##    channel length\n##    <chr>    <dbl>\n##  1 basic     6.95\n##  2 premium   3.38\n##  3 basic    10.0 \n##  4 premium   7.8 \n##  5 basic    10.6 \n##  6 premium   9.42\n##  7 basic    10.2 \n##  8 premium   4.66\n##  9 basic     8.58\n## 10 premium   5.36\n## 11 basic     7.62\n## 12 premium   7.63\n## 13 basic     8.23\n## 14 premium   4.95\n## 15 basic    10.4 \n## 16 premium   8.01\n## 17 basic    11.0 \n## 18 premium   7.8 \n## 19 basic     8.52\n## 20 premium   9.58\ninspect(ads)## \n## categorical variables:  \n##      name     class levels  n missing\n## 1 channel character      2 20       0\n##                                    distribution\n## 1 basic (50%), premium (50%)                   \n## \n## quantitative variables:  \n##        name   class   min     Q1 median      Q3    max    mean       sd  n\n## ...1 length numeric 3.383 7.4525  8.123 9.68825 11.016 8.03215 2.121412 20\n##      missing\n## ...1       0\nfavstats(length ~ channel, data = ads)##   channel   min      Q1 median       Q3    max   mean       sd  n missing\n## 1   basic 6.950 8.30375  9.298 10.30000 11.016 9.2051 1.396126 10       0\n## 2 premium 3.383 5.05250  7.715  7.95975  9.580 6.8592 2.119976 10       0\nads %>%\n  gf_boxplot(channel ~ length) %>%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Channel Type\" ) %>%\n  gf_theme(theme_bw)\nads %>%\n  gf_dens(~length, color = ~channel)%>%\n  gf_labs(title = \"Commercial Length\", \n          subtitle = \"Random 30 minute shows for 2 channel types\",\n          x = \"Length\", y = \"Density\", color = \"Channel Type\" ) %>%\n  gf_theme(theme_bw)"},{"path":"HYPOTESTSIM.html","id":"hypothesis-testing-in-the-us-court-system","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.1 Hypothesis testing in the US court system","text":"US court considers two possible claims defendant: either innocent guilty. Imagine prosecutor. set claims hypothesis framework, null hypothesis defendant innocent alternative hypothesis defendant guilty. job prosecutor use evidence demonstrate jury alternative hypothesis reasonable conclusion.jury considers whether evidence null hypothesis, innocence, convincing (strong) reasonable doubt regarding person’s guilt. , skeptical perspective (null hypothesis) person innocent evidence presented convinces jury person guilty (alternative hypothesis).Jurors examine evidence assumption innocence see whether evidence unlikely convincingly shows defendant guilty. Notice jury finds defendant guilty, necessarily mean jury confident person’s innocence. simply convinced alternative person guilty.also case hypothesis testing: even fail reject null hypothesis, typically accept null hypothesis truth. Failing find strong evidence alternative hypothesis equivalent providing evidence null hypothesis true.two types mistakes possible scenario, letting guilty person go free sending innocent person jail. criteria making decision, reasonable doubt, establishes likelihood errors.Now back problem.","code":""},{"path":"HYPOTESTSIM.html","id":"step-1--state-the-null-and-alternative-hypotheses","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.2 Step 1- State the null and alternative hypotheses","text":"first step translate research question hypotheses. reminder, research question premium channels less ad time basic channels? collecting data, already decided total length time commercials 30 minute show correct data answering question. believe premium channels less commercial time. However, null hypothesis, straw man, default case makes possible generate sampling distribution.\\(H_0\\): Null hypothesis. distribution length commercials premium basic channels .\\(H_A\\): Alternative hypothesis. distribution length commercials premium basic channels different.hypotheses vague. mean two distributions different measure summarize ? Let’s move second step come back modify hypotheses. Notice null hypothesis states distributions . generate sampling distribution test statistic, null hypothesis.","code":""},{"path":"HYPOTESTSIM.html","id":"step-2---compute-a-test-statistic.","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.3 Step 2 - Compute a test statistic.","text":"Exercise:\ntype metric use test difference distributions commercial lengths two types channels?many ways distributions lengths commercials differ. easiest think summary statistics mean, median, standard deviation, combination . Historically, mathematical reasons, common look differences measures centrality, mean median. second consideration kind difference? example, consider ratio actual difference (subtraction)? historical reasons, difference means used measure. keep things interesting, force high school stats experience think problem differently, going use different metric historically used taught. also requires us write code. Later, ask complete analysis different test statistic, either code using code mosaic package.metric ratio median length commercials basic channels premium. Thus, hypotheses now:\\(H_0\\): Null hypothesis. distribution length commercials premium basic channels .\\(H_A\\): Alternative hypothesis. distribution length commercials premium basic channels different median length basic channels ads bigger premium channel ads.First, let’s calculate median length commercials, channel type, data., ratio median lengths isLet’s put calculation ratio function.Now, let’s save observed value test statistic object.done; needed single number metric use evaluating null alternative hypotheses. null hypothesis commercial lengths two channel types distribution alternative don’t. measure alternative hypothesis, decided use ratio medians. ratio close 1, medians different. may ways distributions different decided ratio medians example.","code":"\nmedian(length ~ channel, data = ads) ##   basic premium \n##   9.298   7.715\nmedian(length ~ channel, data = ads)[1] / median(length ~ channel, data = ads)[2]##    basic \n## 1.205185\nmetric <- function(x){\n  temp <- x[1] / x[2]\n  names(temp) <- \"test_stat\"\n  return(temp)\n}\nmetric(median(length ~ channel, data = ads))## test_stat \n##  1.205185\nobs <- metric(median(length ~ channel, data = ads))\nobs## test_stat \n##  1.205185"},{"path":"HYPOTESTSIM.html","id":"step-3---determine-the-p-value.","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.4 Step 3 - Determine the p-value.","text":"reminder, p-value probability observed test statistic something extreme, given null hypothesis true. Since null hypothesis distributions , can use randomization test. shuffle channel labels since null hypothesis, irrelevant. code one randomization.Let’s generate empirical sampling distribution test statistic developed. perform 1,000 simulations now.Next create plot distribution ratio median commercial lengths basic premium channels, assuming come population, Figure 19.3.\nFigure 19.3: Historgram sampling distribution approxiamte permutation test\nNotice distribution centered 1 appears roughly symmetrical. vertical line observed value test statistic. seems tail, larger expected channels came distribution. Let’s calculate p-value.proceeding, technical question: include observed data calculation p-value?answer people conclude original data one possible permutations thus include . practice also insure p-value randomization test never zero. practice, simply means adding 1 numerator denominator. mosaic package done us prop1() function.test performed called one-sided test checked median length basic channels larger premium channels. case one-sided test, extreme meant ratio much bigger 1. two-sided test also common, fact common, used apriori think one channel type longer commercials . case, find p-value doubling single-sided value. extreme happened either tail sampling distribution.","code":"\nset.seed(371)\nmetric(median(length ~ shuffle(channel), data = ads))## test_stat \n## 0.9957097\nset.seed(371)\nresults <- do(1000)*metric(median(length ~ shuffle(channel), data = ads))\nresults %>%\n  gf_histogram(~test_stat) %>%\n  gf_vline(xintercept = obs) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x = \"Test statistic\")\nresults %>%\n  summarise(p_value = mean(test_stat >= obs))##   p_value\n## 1   0.026\nprop1(~(test_stat >= obs), data = results)##  prop_TRUE \n## 0.02697303"},{"path":"HYPOTESTSIM.html","id":"step-4---draw-a-conclusion","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.5 Step 4 - Draw a conclusion","text":"research question – premium channels less ad time basic channels? – framed context following hypotheses:\\(H_0\\): Null hypothesis. distribution length commercials premium basic channels .\\(H_A\\): Alternative hypothesis. distribution length commercials premium basic channels different median length basic channels ads bigger premium channel ads.simulations, less 2.7% simulated test statistics greater equal (extreme relative null hypothesis) observed test statistic. , observed ratio 1.2 rare event distributions commercial lengths premium basic channels truly . chance alone, expect observed ratio large occur less 3 100 times. results like inconsistent \\(H_0\\), reject \\(H_0\\) favor \\(H_A\\). , reject \\(H_0\\) conclude evidence median length basic channel ads bigger premium channels.less 3--100 chance p-value, probability quantifying strength evidence null hypothesis favor alternative.p-value small, .e. less previously set threshold, say results statistically significant74. means data provide strong evidence \\(H_0\\) reject null hypothesis favor alternative hypothesis. threshold, called significance level often represented Greek letter \\(\\alpha\\), typically set \\(\\alpha = 0.05\\), can vary depending field application. Using significance level \\(\\alpha = 0.05\\) TV channel study, can say data provided statistically significant evidence null hypothesis.say data provide statistically significant evidence null hypothesis p-value less reference value, usually \\(\\alpha=0.05\\).null hypothesis true, unknown us, significance level \\(\\alpha\\) defines probability make Type 1 Error. define decision errors next section.Side note:\n’s special 0.05? often use threshold 0.05 determine whether result statistically significant. 0.05? Maybe use bigger number, maybe smaller number. ’re little puzzled, probably means ’re reading critical eye – good job! many video clips explain use 0.05. Sometimes ’s also good idea deviate standard. really depends risk decision maker wants accept terms two types decision errors.Exercise:\nUse p-value significance level 0.05 make decision.Based data, really difference distribution lengths commercials 30 minute shows basic premium channels probability finding observed ratio medians 0.027. Since less significance level \\(\\alpha = 0.05\\), reject null favor alternative basic channel longer commercials.","code":""},{"path":"HYPOTESTSIM.html","id":"decision-errors","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.6 Decision errors","text":"Hypothesis tests flawless. Just think court system: innocent people sometimes wrongly convicted guilty sometimes walk free. Similarly, data can point wrong conclusion. However, distinguishes statistical hypothesis tests court system framework allows us quantify control often data lead us incorrect conclusion.two competing hypotheses: null alternative. hypothesis test, make statement one might true, might choose incorrectly. four possible scenarios hypothesis test, summarized .\\[\n\\begin{array}{cc|cc} & & \\textbf{Test Conclusion} &\\\\\n& & \\text{reject } H_0 &  \\text{reject } H_0 \\text{ favor }H_A  \\\\\n\\textbf{Truth} & \\hline H_0 \\text{ true} & \\text{Correct Decision} &  \\text{Type 1 Error}  \\\\\n& H_A \\text{true} & \\text{Type 2 Error} & \\text{Correct Decision}  \\\\\n\\end{array}\n\\]Type 1 error, also called false positive, rejecting null hypothesis \\(H_0\\) actually true. Since rejected null hypothesis gender discrimination (Case Study) commercial length studies, possible made Type 1 error one studies. Type 2 error, also called false negative, failing reject null hypothesis alternative actually true. Type 2 error possible gender discrimination commercial length studies rejected null hypothesis.Example:\nUS court, defendant either innocent (\\(H_0\\)) guilty (\\(H_A\\)). Type 1 error represent context? Type 2 error represent?court makes Type 1 error, means defendant truly innocent (\\(H_0\\) true) wrongly convicted. Type 2 error means court failed reject \\(H_0\\) (.e. failed convict person) fact guilty (\\(H_A\\) true).Exercise:\nConsider commercial length study concluded basic channels longer commercials premium channels. Type 1 error represent context?75Exercise:\nreduce Type 1 error rate US courts? influence Type 2 error rate?lower Type 1 error rate, might raise standard conviction “beyond reasonable doubt” “beyond conceivable doubt” fewer people wrongly convicted. However, also make difficult convict people actually guilty, make Type 2 errors.Exercise:\nreduce Type 2 error rate US courts? influence Type 1 error rate?lower Type 2 error rate, want convict guilty people. lower standards conviction “beyond reasonable doubt” “beyond little doubt”. Lowering bar guilt also result wrongful convictions, raising Type 1 error rate.Exercise:\nThink cadet honor system, metric evaluation, impact types decision errors.exercises provide important lesson: reduce often make one type error, generally make type given amount data, information.","code":""},{"path":"HYPOTESTSIM.html","id":"choosing-a-significance-level","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.7 Choosing a significance level","text":"Choosing significance level test important many contexts, traditional level \\(\\alpha = 0.05\\). However, sometimes helpful adjust significance level based application. may select level smaller larger 0.05 depending consequences conclusions reached test.making Type 1 error dangerous especially costly, choose small significance level (e.g. 0.01 0.001). scenario, want cautious rejecting null hypothesis, demand strong evidence favoring alternative \\(H_A\\) reject \\(H_0\\).making Type 2 error relatively dangerous much costly Type 1 error, choose higher significance level (e.g. 0.10). want cautious failing reject \\(H_0\\) null hypothesis actually false.significance level selected test reflect real-world consequences associated making Type 1 Type 2 error.","code":""},{"path":"HYPOTESTSIM.html","id":"introducing-two-sided-hypotheses","chapter":"19 Hypothesis Testing with Simulation","heading":"19.4.8 Introducing two-sided hypotheses","text":"far explored whether women discriminated whether commercials longer depending type channel. two case studies, ’ve actually ignored possibilities:men actually discriminated ?ads premium channels actually longer?possibilities weren’t considered hypotheses analyses. may seemed natural since data pointed directions framed problems. However, two dangers ignore possibilities disagree data conflict worldview:Framing alternative hypothesis simply match direction data point generally inflate Type 1 error rate. work ’ve done (continue ) rigorously control error rates hypothesis tests, careless construction alternative hypotheses can disrupt hard work.use alternative hypotheses agree worldview, ’re going subjecting confirmation bias, means looking data supports ideas. ’s scientific, can better!previous hypothesis tests ’ve seen called one-sided hypothesis tests explored one direction possibilities. hypotheses appropriate exclusively interested single direction, usually want consider possibilities. , let’s discuss two-sided hypothesis tests context new study examines impact using blood thinners patients undergone cardiopulmonary resuscitation, CPR.","code":""},{"path":"HYPOTESTSIM.html","id":"two-sided-hypothesis-test","chapter":"19 Hypothesis Testing with Simulation","heading":"19.5 Two-sided hypothesis test","text":"important distinguish two-sided hypothesis test one-sided hypothesis test. two-sided test, concerned whether population parameter take particular value. parameter \\(\\theta\\), set two-sided hypotheses looks like:\\[\nH_0: \\theta=\\theta_0 \\hspace{1.5cm} H_1: \\theta \\neq \\theta_0\n\\]\n\\(\\theta_0\\) particular value parameter take .one-sided test, concerned whether parameter exceeds exceed specific value. set one-sided hypotheses looks like:\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta > \\theta_0\n\\]\n\\[\nH_0: \\theta = \\theta_0 \\hspace{1.5cm} H_1:\\theta < \\theta_0\n\\]texts, one-sided null hypotheses include inequality (\\(H_0 \\leq \\theta_0\\) \\(H_0 \\geq \\theta_0\\)). already demonstrated one-sided tests , next example, use two-sided test.","code":""},{"path":"HYPOTESTSIM.html","id":"cpr-example","chapter":"19 Hypothesis Testing with Simulation","heading":"19.5.1 CPR example","text":"Cardiopulmonary resuscitation (CPR) procedure used individuals suffering heart attack emergency resources unavailable. procedure helpful providing blood circulation keep person alive, CPR chest compressions can also cause internal injuries. Internal bleeding injuries can result CPR complicate additional treatment efforts. instance, blood thinners may used help release clot causing heart attack patient arrives hospital. However, blood thinners negatively affect internal injuries.consider experiment patients underwent CPR heart attack subsequently admitted hospital.76 patient randomly assigned either receive blood thinner (treatment group) receive blood thinner (control group). outcome variable interest whether patient survived least 24 hours.","code":""},{"path":"HYPOTESTSIM.html","id":"step-1---state-the-null-and-alternative-hypotheses","chapter":"19 Hypothesis Testing with Simulation","heading":"19.5.2 Step 1 - State the null and alternative hypotheses","text":"Exercise:\nForm hypotheses study plain statistical language. Let \\(p_c\\) represent true survival rate people receive blood thinner (corresponding control group) \\(p_t\\) represent survival rate people receiving blood thinner (corresponding treatment group).want understand whether blood thinners helpful harmful. ’ll consider possibilities using two-sided hypothesis test.\\(H_0\\): Blood thinners overall survival effect; survival rate independent experimental treatment group. \\(p_c - p_t = 0\\).\\(H_A\\): Blood thinners impact survival, either positive negative, zero. \\(p_c - p_t \\neq 0\\).Notice accelerated process already defining test statistic, metric, hypothesis. difference survival rates control treatment groups. similar metric used case study. use others allow us use functions mosaic package also help us understand metrics mathematically derived sampling distributions.50 patients experiment receive blood thinner 40 patients . study results file blood_thinner.csv.Let’s put data table.","code":"\nthinner <- read_csv(\"data/blood_thinner.csv\")\nthinner## # A tibble: 90 x 2\n##    group     outcome \n##    <chr>     <chr>   \n##  1 treatment survived\n##  2 control   survived\n##  3 control   died    \n##  4 control   died    \n##  5 control   died    \n##  6 treatment survived\n##  7 control   died    \n##  8 control   died    \n##  9 treatment died    \n## 10 treatment survived\n## # ... with 80 more rows\ntally(~group + outcome, data = thinner, margins = TRUE)##            outcome\n## group       died survived Total\n##   control     39       11    50\n##   treatment   26       14    40\n##   Total       65       25    90"},{"path":"HYPOTESTSIM.html","id":"step-2---compute-a-test-statistic.-1","chapter":"19 Hypothesis Testing with Simulation","heading":"19.5.3 Step 2 - Compute a test statistic.","text":"test statistic selected difference survival rate control group treatment group. following R command finds observed proportions.Notice formula used get correct variable column summary proportions.observed test statistic can now found.77Based point estimate, patients undergone CPR outside hospital, additional 13% patients survive treated blood thinners. wonder difference easily explainable chance.","code":"\ntally(outcome ~ group, data = thinner, margins = TRUE, format = \"proportion\")##           group\n## outcome    control treatment\n##   died        0.78      0.65\n##   survived    0.22      0.35\n##   Total       1.00      1.00\nobs <- diffprop(outcome ~ group, data = thinner)\nobs## diffprop \n##    -0.13"},{"path":"HYPOTESTSIM.html","id":"step-3---determine-the-p-value.-1","chapter":"19 Hypothesis Testing with Simulation","heading":"19.5.4 Step 3 - Determine the p-value.","text":"previous two studies, simulate type differences might see chance alone null hypothesis. randomly assigning treatment control stickers patients’ files, get new grouping. repeat simulation 10,000 times, can build null distribution differences. empirical sampling distribution, distribution differences simulated null hypothesis.Figure 19.4 histogram estimated sampling distribution.\nFigure 19.4: Histogram estimated sampling distribution.\nNotice centered zero, assumption difference. Also notice unimodal symmetric. use develop mathematical sampling distributions. now calculate proportion simulated differences less equal observed difference.left tail area 0.128. (Note: coincidence p-value approximately 0.13 also \\(\\hat{p}_c - \\hat{p}_t= -0.13\\).) However, contrary calculated p-value previous studies, p-value test 0.128!p-value defined probability observe result least favorable alternative hypothesis observed result (.e. observed difference). case, differences greater equal 0.13 provide equally strong evidence favoring alternative hypothesis differences less equal -0.13. difference 0.13 correspond 13% higher survival rate treatment group control group.something different study past studies: study, particularly interested whether blood thinners increase decrease risk death patients undergo CPR arriving hospital.78For two-sided test, take single tail (case, 0.128) double get p-value: 0.256.","code":"\nset.seed(655)\nresults <- do(10000)*diffprop(outcome ~ shuffle(group), data = thinner)\nresults %>%\n  gf_histogram(~diffprop) %>%\n  gf_vline(xintercept = obs) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x = \"Test statistic\")\nprop1(~(diffprop <= obs), data = results)## prop_TRUE \n## 0.1283872"},{"path":"HYPOTESTSIM.html","id":"step-4---draw-a-conclusion-1","chapter":"19 Hypothesis Testing with Simulation","heading":"19.5.5 Step 4 - Draw a conclusion","text":"Since p-value larger 0.05, fail reject null hypothesis. , find statistically significant evidence blood thinner influence survival patients undergo CPR prior arriving hospital. , can discuss causal conclusion since experiment.Default two-sided test\nwant rigorous keep open mind analyze data evidence. general, default using two-sided test conducting hypothesis tests. Use one-sided hypothesis test truly interest one direction.Computing p-value two-sided test\nFirst compute p-value one tail distribution, double value get two-sided p-value. ’s !never okay change two-sided tests one-sided tests observing data.Hypothesis tests set seeing data\nobserving data, can tempting turn two-sided test one-sided test. Avoid temptation. Hypotheses significance level set observing data.","code":""},{"path":"HYPOTESTSIM.html","id":"how-to-use-a-hypothesis-test","chapter":"19 Hypothesis Testing with Simulation","heading":"19.5.6 How to use a hypothesis test","text":"summary general framework using hypothesis testing. steps , just slightly different wording.Frame research question terms hypotheses.\nHypothesis tests appropriate research questions can summarized two competing hypotheses. null hypothesis (\\(H_0\\)) usually represents skeptical perspective perspective difference. alternative hypothesis (\\(H_A\\)) usually represents new view difference.Frame research question terms hypotheses.Hypothesis tests appropriate research questions can summarized two competing hypotheses. null hypothesis (\\(H_0\\)) usually represents skeptical perspective perspective difference. alternative hypothesis (\\(H_A\\)) usually represents new view difference.Collect data observational study experiment.\nresearch question can formed two hypotheses, can collect data run hypothesis test. research question focuses associations variables concern causation, run observational study. research question seeks causal connection two variables, experiment used possible.Collect data observational study experiment.research question can formed two hypotheses, can collect data run hypothesis test. research question focuses associations variables concern causation, run observational study. research question seeks causal connection two variables, experiment used possible.Analyze data.\nChoose analysis technique appropriate data identify p-value. far, ’ve seen one analysis technique: randomization. ’ll encounter several new methods suitable many contexts.Analyze data.Choose analysis technique appropriate data identify p-value. far, ’ve seen one analysis technique: randomization. ’ll encounter several new methods suitable many contexts.Form conclusion.\nUsing p-value analysis, determine whether data provide statistically significant evidence null hypothesis. Also, sure write conclusion plain language casual readers can understand results.Form conclusion.Using p-value analysis, determine whether data provide statistically significant evidence null hypothesis. Also, sure write conclusion plain language casual readers can understand results.","code":""},{"path":"HYPOTESTSIM.html","id":"homework-problems-18","chapter":"19 Hypothesis Testing with Simulation","heading":"19.6 Homework Problems","text":"Repeat analysis commercial lengths basic premium TV channels notes. time, use different test statistic.State null alternative hypotheses.State null alternative hypotheses.Compute test statistic. Remember use something different used text.Compute test statistic. Remember use something different used text.Determine p-value.Determine p-value.Draw conclusion.Draw conclusion.yawning contagious?experiment conducted MythBusters, science entertainment TV program Discovery Channel, tested whether person can subconsciously influenced yawning another person near yawns. 50 people randomly assigned two groups: 34 group person near yawned (treatment) 16 group wasn’t person yawning near (control). following table shows results experiment.\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Group}\\\\\n& & \\text{Treatment } &  \\text{Control} & \\text{Total}  \\\\\n& \\hline \\text{Yawn}    &   10      & 4     & 14  \\\\\n\\textbf{Result} & \\text{Yawn}   & 24        & 12        & 36   \\\\\n    &\\text{Total}       & 34        & 16        & 50 \\\\\n\\end{array}\n\\]data file yawn.csv.hypotheses?hypotheses?Calculate observed difference yawning rates two scenarios. Yes, giving test statistic.Calculate observed difference yawning rates two scenarios. Yes, giving test statistic.Estimate p-value using randomization.Estimate p-value using randomization.Plot empirical sampling distribution.Plot empirical sampling distribution.Determine conclusion hypothesis test.Determine conclusion hypothesis test.traditional belief yawning contagious – one yawn can lead another yawn, might lead another, . exercise, option selecting one-sided two-sided test. recommend (choose)? Justify answer 1-3 sentences.traditional belief yawning contagious – one yawn can lead another yawn, might lead another, . exercise, option selecting one-sided two-sided test. recommend (choose)? Justify answer 1-3 sentences.select level significance? Explain 1-3 sentences.select level significance? Explain 1-3 sentences.","code":""},{"path":"HYPTESTDIST.html","id":"HYPTESTDIST","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20 Hypothesis Testing with Known Distributions","text":"","code":""},{"path":"HYPTESTDIST.html","id":"objectives-19","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.1 Objectives","text":"Know properly use terminology hypothesis test: permutation test, exact test, null hypothesis, alternative hypothesis, test statistic, p-value, power.Know properly use terminology hypothesis test: permutation test, exact test, null hypothesis, alternative hypothesis, test statistic, p-value, power.Conduct four steps hypothesis test using probability models.Conduct four steps hypothesis test using probability models.","code":""},{"path":"HYPTESTDIST.html","id":"hypothesis-testing-using-probability-models","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.2 Hypothesis testing using probability models","text":"lead Central Limit Theorem 21 mathematical sampling distributions, look class hypothesis testing null hypothesis specifies probability model. cases, can get exact answer, others, use simulation get empirical p-value. way, permutation test exact test; mean finding possible permutations calculation p-value. However, since complete enumeration permutations often difficult, approximate randomization, simulation. Thus, p-value randomization test approximation exact (permutation) test.Let’s use three examples illustrate ideas chapter.","code":""},{"path":"HYPTESTDIST.html","id":"tappers-and-listeners","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3 Tappers and listeners","text":"’s game can try friends family. Pick simple, well-known song. Tap tune desk, see person can guess song. simple game, tapper, person listener.Stanford University graduate student named Elizabeth Newton conducted experiment using tapper-listener game.79 study, recruited 120 tappers 120 listeners study. 50% tappers expected listener able guess song. Newton wondered, 50% reasonable expectation?","code":""},{"path":"HYPTESTDIST.html","id":"step-1--state-the-null-and-alternative-hypotheses-1","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.1 Step 1- State the null and alternative hypotheses","text":"Newton’s research question can framed two hypotheses:Exercise:\none-sided two-sided hypothesis test? many variables model?tappers think listeners guess song correctly 50% time, two-sided test since don’t know beforehand listeners better worse 50%.one variable interest, whether listener correct.","code":"$H_0$: The tappers are correct, and, in general, 50\\% of listeners are able to guess the tune. $p = 0.50$    \n\n$H_A$: The tappers are incorrect, and either more than or less than 50\\% of listeners are able to guess the tune. $p \\neq 0.50$  "},{"path":"HYPTESTDIST.html","id":"step-2---compute-a-test-statistic","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.2 Step 2 - Compute a test statistic","text":"Newton’s study, 42 (changed number make problem interesting educational perspective) 120 listeners (\\(\\hat{p} = 0.35\\)) able guess tune! perspective null hypothesis, might wonder, likely get result chance alone? , ’s chance happen see small fraction \\(H_0\\) true true correct-guess rate 0.50?Now use simulation, let’s frame probability model. random variable \\(X\\) number correct guesses 120. observations independent probability success constant (listener probability guessing correctly), use binomial model. can’t assess validity assumptions without knowing experiment, subjects, data collection. educational purposes, assume valid. Thus, test statistic number successes 120 trials. observed value 42.","code":""},{"path":"HYPTESTDIST.html","id":"step-3---determine-the-p-value","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.3 Step 3 - Determine the p-value","text":"now want find p-value \\(2 \\cdot \\mbox{P}(X \\leq 42)\\) \\(X\\) binomial random variable \\(p = 0.5\\) \\(n = 120\\). , p-value probability observed data something extreme, given null hypothesis true. , null hypothesis true implies probability success 0.50. use R get one-sided p-value double get two-sided p-value problem. selected \\(\\mbox{P}(X \\leq 42)\\) “extreme” means observed values values get null hypothesis true, 60 problem.small p-value.","code":"\n2*pbinom(42, 120, prob = 0.5)## [1] 0.001299333"},{"path":"HYPTESTDIST.html","id":"step-4---draw-a-conclusion-2","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.4 Step 4 - Draw a conclusion","text":"Based data, listeners guessing correct 50% time, less \\(0.0013\\) probability 42 less (78 ) listeners get correctly. probability observed something extreme, given null hypothesis true. probability much less 0.05, reject null hypothesis listeners guessing correctly half time conclude correct-guess rate rate different 50%.decision region looks like pmf Figure 20.1. observed values inside red boundary lines consistent null hypothesis. , observed values inside red boundary lines result p-value larger 0.05. values red line extreme rejection region, resulting p-value smaller 0.05. also plotted observed value black.\nFigure 20.1: Binomial pmf\n","code":"\ngf_dist(\"binom\", size = 120, prob = 0.5, xlim = c(50, 115)) %>%\n  gf_vline(xintercept = c(48, 72), color = \"red\") %>%\n  gf_vline(xintercept = c(42), color = \"black\") %>%\n  gf_theme(theme_bw) %>%\n  gf_labs(title = \"Binomial pmf\", subtitle = \"Probability of success is 0.5\", \n          y = \"Probability\")"},{"path":"HYPTESTDIST.html","id":"repeat-using-simulation","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.5 Repeat using simulation","text":"repeat analysis using empirical (observed simulated data) p-value. Step 1, stating null alternative hypothesis, .","code":""},{"path":"HYPTESTDIST.html","id":"step-2---compute-a-test-statistic-1","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.6 Step 2 - Compute a test statistic","text":"use proportion listeners get song correct instead number. minor change since simply dividing 120.","code":"\nobs <- 42 / 120\nobs## [1] 0.35"},{"path":"HYPTESTDIST.html","id":"step-3---determine-the-p-value-1","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.7 Step 3 - Determine the p-value","text":"simulate 120 games null hypothesis \\(p = 0.50\\), flip coin 120 times. time coin comes heads, represent listener guessing correctly, tails represent listener guessing incorrectly. example, can simulate 5 tapper-listener pairs flipping coin 5 times:\\[\n\\begin{array}{ccccc}\nH & H & T & H & T \\\\\nCorrect & Correct & Wrong & Correct & Wrong \\\\\n\\end{array}\n\\]flipping coin 120 times, got 56 heads proportion \\(\\hat{p}_{sim} = 0.467\\). randomization technique, seeing happen one simulation isn’t enough. order evaluate whether originally observed proportion 0.35 unusual , generate simulations. , ’ve repeated simulation 10,000 times:Note, simulate number ways. way using () look like ’ve done randomization tests.\nFigure 20.2: estimated sampling distribution.\nNotice Figure 20.2 sampling distribution centered 0.5 looks symmetrical.p-value found using prop1 function. problem, really need observed value included prevent p-value zero.","code":"\nset.seed(604)\nresults <- rbinom(10000, 120, 0.5) / 120\nset.seed(604)\nresults <- do(10000)*mean(sample(c(0, 1), size = 120, replace = TRUE))\nhead(results)##        mean\n## 1 0.4250000\n## 2 0.5250000\n## 3 0.5916667\n## 4 0.5000000\n## 5 0.5250000\n## 6 0.5083333\nresults %>%\n  gf_histogram(~mean, fill = \"cyan\", color = \"black\") %>%\n  gf_vline(xintercept = c(obs, 1 - obs), color = \"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x = \"Test statistic\")\n2*prop1(~(mean <= obs), data = results)##  prop_TRUE \n## 0.00119988"},{"path":"HYPTESTDIST.html","id":"step-4---draw-a-conclusion-3","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.3.8 Step 4 - Draw a conclusion","text":"10,000 simulations, see results close 0.35. Based data, listeners guessing correct 50% time, less \\(0.0012\\) probability 35% less 65% listeners get right. p-value much less 0.05, reject listeners guessing correctly half time conclude correct-guess rate different 50%.Exercise:\ncontext experiment, p-value hypothesis test?80Exercise:\ndata provide statistically significant evidence null hypothesis? State appropriate conclusion context research question.81","code":""},{"path":"HYPTESTDIST.html","id":"cardiopulmonary-resuscitation-cpr","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.4 Cardiopulmonary resuscitation (CPR)","text":"Let’s return CPR example last chapter. reminder, repeat background material.Cardiopulmonary resuscitation (CPR) procedure sometimes used individuals suffering heart attack. helpful providing blood circulation keep person alive, CPR chest compressions can also cause internal injuries, complicate additional treatment efforts. instance, blood thinners may used help release clot causing heart attack, blood thinners negatively affect internal injuries.Patients underwent CPR heart attack subsequently admitted hospital82 randomly assigned either receive blood thinner (treatment group) receive blood thinner (control group). outcome variable interest whether patient survived least 24 hours.","code":""},{"path":"HYPTESTDIST.html","id":"step-1--state-the-null-and-alternative-hypotheses-2","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.4.1 Step 1- State the null and alternative hypotheses","text":"want understand whether blood thinners helpful harmful. ’ll consider possibilities using two-sided hypothesis test.\\(H_0\\): Blood thinners overall survival effect; survival rate independent experimental treatment group. \\(p_c - p_t = 0\\).\\(H_A\\): Blood thinners impact survival, either positive negative, zero. \\(p_c - p_t \\neq 0\\).Let’s put data table.","code":"\nthinner <- read_csv(\"data/blood_thinner.csv\")\nhead(thinner)## # A tibble: 6 x 2\n##   group     outcome \n##   <chr>     <chr>   \n## 1 treatment survived\n## 2 control   survived\n## 3 control   died    \n## 4 control   died    \n## 5 control   died    \n## 6 treatment survived\ntally(~group + outcome, data = thinner, margins = TRUE)##            outcome\n## group       died survived Total\n##   control     39       11    50\n##   treatment   26       14    40\n##   Total       65       25    90"},{"path":"HYPTESTDIST.html","id":"step-2---compute-a-test-statistic.-2","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.4.2 Step 2 - Compute a test statistic.","text":"example, can think data coming hypergeometric distribution. really binomial finite population. can calculate p-value using probability distribution. random variable number control patients survived total population 90 patients, 50 control patients 40 treatment patients, total 25 survived.","code":""},{"path":"HYPTESTDIST.html","id":"step-3---determine-the-p-value.-2","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.4.3 Step 3 - Determine the p-value.","text":"case, want find \\(\\mbox{P}(X \\leq 11)\\) (observed number control patients survived) double since two-sided test.Note: picked lower right cell reference cell. now want \\(\\mbox{P}(X \\geq 14)\\) (observed number treatment patients survived) appropriate change parameter values. Notice get answer.thing two cells. find \\(\\mbox{P}(X \\leq 26)\\) (observed number treatment patients survive).find \\(\\mbox{P}(X \\geq 39)\\) (observed number control patients survive).R also built function, fisher.test(), use. function calculates Fisher’s exact test, p-values obtained using hypergeometric distribution.p-value slightly different since hypergeometric distribution symmetric. reason, doubling p-value single side result quite right. algorithm fisher.test() finds adds probabilities less equal value \\(\\mbox{P}(X = 11)\\), see Figure 20.3. Using fisher.test() gives correct p-value.\nFigure 20.3: Hypergeometric pmf showing cutoff p-value calculation.\nfisher.test() calculating p-value:randomization test last chapter yielded p-value 0.257 tests consistent.","code":"\n2*phyper(11, 50, 40, 25)## [1] 0.2581356\n2*(1 - phyper(13, 40, 50, 25))## [1] 0.2581356\n2*phyper(26, 40, 50, 65)## [1] 0.2581356\n2*(1 - phyper(38, 50, 40, 65))## [1] 0.2581356\nfisher.test(tally(~group + outcome, data = thinner))## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tally(~group + outcome, data = thinner)\n## p-value = 0.2366\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  0.6794355 5.4174460\n## sample estimates:\n## odds ratio \n##   1.895136\ngf_dist(\"hyper\", m = 50, n = 40, k = 25) %>%\n  gf_hline(yintercept = dhyper(11, 50, 40, 25), color = \"red\") %>%\n  gf_labs(title = \"Hypergeometric pmf\", subtitle = \"Red line is P(X = 11)\", \n          y = \"Probability\") %>%\n  gf_theme(theme_bw())\ntemp <- dhyper(0:25, 50, 40, 25)\nsum(temp[temp <= dhyper(11, 50, 40, 25)])## [1] 0.2365928"},{"path":"HYPTESTDIST.html","id":"step-4---draw-a-conclusion-4","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.4.4 Step 4 - Draw a conclusion","text":"Since p-value larger 0.05, fail reject null hypothesis. , find statistically significant evidence blood thinner influence survival patients undergo CPR prior arriving hospital. , can discuss causal conclusion since experiment.Notice first two examples, test single proportion test two proportions. single proportion test equivalent randomization test since second variable shuffle. able get answers since found probability model use instead.","code":""},{"path":"HYPTESTDIST.html","id":"golf-balls","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.5 Golf Balls","text":"last example interesting distribution multiple parameters test metric obvious point.owners residence located along golf course collected first 500 golf balls landed property. golf balls labeled make golf ball number, example “Nike 1” “Titleist 3”. numbers typically 1 4, owners residence wondered numbers equally likely (least among golf balls used golfers poor enough quality lose yards residences along fairway.)use significance level \\(\\alpha = 0.05\\) since reason favor one decision error .","code":""},{"path":"HYPTESTDIST.html","id":"step-1--state-the-null-and-alternative-hypotheses-3","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.5.1 Step 1- State the null and alternative hypotheses","text":"think numbers equally likely. question one-sided versus two-sided relevant test. see write hypotheses.\\(H_0\\): numbers equally likely.\\(\\pi_1 = \\pi_2 = \\pi_3 = \\pi_4\\) \\(\\pi_1 = \\frac{1}{4}, \\pi_2 =\\frac{1}{4}, \\pi_3 =\\frac{1}{4}, \\pi_4 =\\frac{1}{4}\\)\\(H_A\\): distribution numbers population. least one population proportion \\(\\frac{1}{4}\\).Notice switched using \\(\\pi\\) instead \\(p\\) population parameter. reason make aware used.problem extension binomial. Instead two outcomes, four outcomes. called multinomial distribution. can read like, methods make necessary learn probability mass function.500 golf balls collected, 486 number 1 4. deal 486 golf balls. Let’s get data `golf_balls.csv”.","code":"\ngolf_balls <- read_csv(\"data/golf_balls.csv\")\ninspect(golf_balls)## \n## quantitative variables:  \n##        name   class min Q1 median Q3 max     mean       sd   n missing\n## ...1 number numeric   1  1      2  3   4 2.366255 1.107432 486       0\ntally(~number, data = golf_balls)## number\n##   1   2   3   4 \n## 137 138 107 104"},{"path":"HYPTESTDIST.html","id":"step-2---compute-a-test-statistic.-3","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.5.2 Step 2 - Compute a test statistic.","text":"numbers equally likely, expect see 121.5 golf balls number. point estimate thus, actual value realized. course, sample variation thus, departure state. need test statistic help us determine observed values reasonable null hypothesis. Remember test statistic single number metric used evaluate hypothesis.Exercise:\npropose test statistic?four proportions, need way combine . seems tricky, let’s just use simple approach. Let’s take maximum number balls across cells table subtract minimum. called range denote parameter \\(R\\). null hypothesis, zero. re-write hypotheses :\\(H_0\\): \\(R=0\\)\\(H_A\\): \\(R>0\\)Notice \\(R\\) always non-negative, thus test one-sided.observed range 34, \\(138 - 104\\).","code":"\nobs <- diff(range(tally(~number, data = golf_balls)))\nobs## [1] 34"},{"path":"HYPTESTDIST.html","id":"step-3---determine-the-p-value.-3","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.5.3 Step 3 - Determine the p-value.","text":"don’t know distribution test statistic, use simulation. simulate data multinomial distribution null hypothesis calculate new value test statistic. repeat 10,000 times give us estimate sampling distribution.use sample() function simulate distribution numbers null hypothesis. help us understand process build code, initially using sample size 12 keep printout reasonable easy read.Notice using tidyverse coding ideas. don’t think need tibbles data frames went straight nested R code. can break code starting code center.now ready ramp full problem. Let’s simulate data null hypothesis. sampling 486 golf balls (instead 12) numbers 1 4 . number equally likely. find range, test statistic. Finally repeat 10,000 get estimate sampling distribution test statistic.Figure 20.4 plot sampling distribution range.\nFigure 20.4: Sampling distribution range.\nNotice distribution skewed right. p-value 0.14.","code":"\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))## [1] 4\nset.seed(3311)\nsample(1:4, size = 12, replace = TRUE)##  [1] 3 1 2 3 2 3 1 3 3 4 1 1\nset.seed(3311)\ntable(sample(1:4, size = 12, replace = TRUE))## \n## 1 2 3 4 \n## 4 2 5 1\nset.seed(3311)\nrange(table(sample(1:4, size = 12, replace = TRUE)))## [1] 1 5\nset.seed(3311)\ndiff(range(table(sample(1:4, size = 12, replace = TRUE))))## [1] 4\nset.seed(3311)\nresults <- do(10000)*diff(range(table(sample(1:4, size = 486, replace = TRUE))))\nresults %>%\n  gf_histogram(~diff, fill = \"cyan\", color = \"black\") %>%\n  gf_vline(xintercept = obs, color = \"black\") %>%\n  gf_labs(title = \"Sampling Distribution of Range\", \n          subtitle = \"Multinomial with equal probability\",\n          x = \"Range\") %>%\n  gf_theme(theme_bw)\nprop1(~(diff >= obs), data = results)## prop_TRUE \n## 0.1393861"},{"path":"HYPTESTDIST.html","id":"step-4---draw-a-conclusion-5","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.5.4 Step 4 - Draw a conclusion","text":"Since p-value larger 0.05, fail reject null hypothesis. , based data, find statistically significant evidence claim number golf balls equally likely. can’t say proportion golf balls number differs 0.25.","code":""},{"path":"HYPTESTDIST.html","id":"repeat-with-a-different-test-statistic","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.6 Repeat with a different test statistic","text":"test statistic developed helpful, seems weak use information four cells. let’s devise metric . hypotheses , jump step 2.","code":""},{"path":"HYPTESTDIST.html","id":"step-2---compute-a-test-statistic.-4","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.6.1 Step 2 - Compute a test statistic.","text":"number equally likely, 121.5 balls bin. can find test statistic looking deviation cell 121.5.Now need collapse single number. Just adding always result value 0, ? let’s take absolute value add cells together.test statistic.","code":"\ntally(~number, data = golf_balls) - 121.5## number\n##     1     2     3     4 \n##  15.5  16.5 -14.5 -17.5\nobs <- sum(abs(tally(~number, data = golf_balls) - 121.5))\nobs## [1] 64"},{"path":"HYPTESTDIST.html","id":"step-3---determine-the-p-value.-4","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.6.2 Step 3 - Determine the p-value.","text":"use similar code new metric. Now sample 486 golf balls numbers 1 4 , find test statistic, sum absolute deviations cell table expected count, 121.5. repeat process 10,000 times get estimate sampling distribution test statistic.Figure 20.5 plot sampling distribution absolute value deviations.\nFigure 20.5: Sampling distribution absolute deviations.\nNotice distribution skewed right test statistic seems extreme.p-value 0.014. value much smaller previous result. test statistic matters decision process nothing problem changed except test statistic.","code":"\nset.seed(9697)\nresults <- do(10000)*sum(abs(table(sample(1:4, size = 486, replace = TRUE)) - 121.5))\nresults %>%\n  gf_histogram(~sum, fill = \"cyan\", color = \"black\") %>%\n  gf_vline(xintercept = obs, color = \"black\") %>%\n  gf_labs(title = \"Sampling Distribution of Absolute Deviations\",\n          subtitle = \"Multinomial with equal probability\",\n          x = \"Absolute deviations\") %>%\n  gf_theme(theme_bw)\nprop1(~(sum >= obs), data = results)##  prop_TRUE \n## 0.01359864"},{"path":"HYPTESTDIST.html","id":"step-4---draw-a-conclusion-6","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.6.3 Step 4 - Draw a conclusion","text":"Since p-value smaller 0.05, reject null hypothesis. , based data, find statistically significant evidence claim numbers golf balls equally likely. conclude numbers golf balls equally likely, least one different.","code":""},{"path":"HYPTESTDIST.html","id":"summary","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.7 Summary","text":"chapter, used probability models help us make decisions data. chapter different randomization section randomization two variables (one shuffle) null hypothesis difference. case single proportion, able use binomial distribution get exact p-value null hypothesis. case \\(2 \\times 2\\) table, able show use hypergeometric distribution get exact p-value assumptions model.also found choice test statistic impact decision. Even though get valid p-values desired Type 1 error rate, information data used fullest, lose power. Note: power probability correctly rejecting null hypothesis alternative hypothesis true.next chapter, learn mathematical solutions finding sampling distribution. key difference methods selection test statistic assumptions made derive sampling distribution.","code":""},{"path":"HYPTESTDIST.html","id":"homework-problems-19","chapter":"20 Hypothesis Testing with Known Distributions","heading":"20.8 Homework Problems","text":"Repeat analysis yawning data last chapter, time use hypergeometric distribution.\nyawning contagious?\nexperiment conducted MythBusters, science entertainment TV program Discovery Channel, tested person can subconsciously influenced yawning another person near yawns. 50 people randomly assigned two groups: 34 group person near yawned (treatment) 16 group wasn’t person yawning near (control). following table shows results experiment.Repeat analysis yawning data last chapter, time use hypergeometric distribution.yawning contagious?experiment conducted MythBusters, science entertainment TV program Discovery Channel, tested person can subconsciously influenced yawning another person near yawns. 50 people randomly assigned two groups: 34 group person near yawned (treatment) 16 group wasn’t person yawning near (control). following table shows results experiment.\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Group}\\\\\n& & \\text{Treatment } &  \\text{Control} & \\text{Total}  \\\\\n& \\hline \\text{Yawn}    &   10      & 4     & 14  \\\\\n\\textbf{Result} & \\text{Yawn}   & 24        & 12        & 36   \\\\\n    &\\text{Total}       & 34        & 16        & 50 \\\\\n\\end{array}\n\\]data file yawn.csv.hypotheses?hypotheses?Choose cell, calculate observed statistic.Choose cell, calculate observed statistic.Find p-value using hypergeometric distribution.Find p-value using hypergeometric distribution.Plot sampling distribution.Plot sampling distribution.Determine conclusion hypothesis test.Determine conclusion hypothesis test.Compare results randomization test.Compare results randomization test.Repeat analysis golf ball data using different test statistic.\nUse level significance 0.05.Repeat analysis golf ball data using different test statistic.Use level significance 0.05.State null alternative hypotheses.State null alternative hypotheses.Compute test statistic.Compute test statistic.Determine p-value.Determine p-value.Draw conclusion.Draw conclusion.Body TemperatureShoemaker83 cites paper American Medical Association84 questions conventional wisdom average body temperature human 98.6 degrees Fahrenheit. One main points original article traditional mean 98.6 , essence, 100 years date. authors cite problems original study’s methodology, diurnal fluctuations (0.9 degrees F per day), unreliable thermometers. authors believe average human body temperature less 98.6. Conduct hypothesis test.State null alternative hypotheses.State null alternative hypotheses.State significance level used.State significance level used.Load data file “temperature.csv” generate summary statistics boxplot temperature data. using gender heart rate problem.Load data file “temperature.csv” generate summary statistics boxplot temperature data. using gender heart rate problem.Compute test statistic. going help part. randomization test since don’t second variable. nice use mean test statistic don’t yet know sampling distribution sample mean.\nLet’s get clever. distribution sample symmetric (assumption look boxplot summary statistics determine comfortable ), null hypothesis, observed values equally likely either greater less 98.6. Thus, test statistic number cases positive difference 98.6 observed value. binomial distribution probability success (positive difference) 0.5. must also account possibility observations 98.6 data.Compute test statistic. going help part. randomization test since don’t second variable. nice use mean test statistic don’t yet know sampling distribution sample mean.Let’s get clever. distribution sample symmetric (assumption look boxplot summary statistics determine comfortable ), null hypothesis, observed values equally likely either greater less 98.6. Thus, test statistic number cases positive difference 98.6 observed value. binomial distribution probability success (positive difference) 0.5. must also account possibility observations 98.6 data.Determine p-value.Determine p-value.Draw conclusion.Draw conclusion.","code":""},{"path":"HYPTESTCLT.html","id":"HYPTESTCLT","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21 Hypothesis Testing with the Central Limit Theorem","text":"","code":""},{"path":"HYPTESTCLT.html","id":"objectives-20","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.1 Objectives","text":"Explain central limit theorem can used inference.Explain central limit theorem can used inference.Conduct hypothesis tests single mean proportion using CLT R.Conduct hypothesis tests single mean proportion using CLT R.Explain \\(t\\) distribution relates normal distribution, used, changing parameters impacts shape distribution.Explain \\(t\\) distribution relates normal distribution, used, changing parameters impacts shape distribution.","code":""},{"path":"HYPTESTCLT.html","id":"central-limit-theorem","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.2 Central limit theorem","text":"’ve encountered several research questions associated hypothesis tests far block material. differ settings, outcomes, also technique use analyze data, many something common: certain class test statistics, general shape sampling distribution null hypothesis looks like normal distribution.","code":""},{"path":"HYPTESTCLT.html","id":"null-distribution","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.2.1 Null distribution","text":"reminder, tapping listening problem, used proportion correct guesses test statistic. null hypothesis, assumed probability success 0.5. estimate sampling distribution test statistic shown Figure 21.1.\nFigure 21.1: Sampling distribution proportion.\nExercise:\nDescribe shape distribution note anything find interesting.85In Figure 21.2, overlayed normal distribution histogram estimated sampling distribution. allows us visually compare normal probability density curve empirical (based data, simulation case) sampling distribution.\nFigure 21.2: Sampling distribution sample proportion.\nsimilarity empirical theoretical distributions coincidence, rather guaranteed mathematical theory. chapter little notation- algebra-intensive previous chapters. However, goal develop tool help us find sampling distributions many types test statistics , thus, find p-values. chapter involves classical statistics often taught AP high school classes, well many introductory undergraduate statistics courses. Remember advances modern computing, mathematical solutions available.","code":""},{"path":"HYPTESTCLT.html","id":"theorem---central-limit-theorem","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.2.2 Theorem - central limit theorem","text":"Theorem: Let \\(X_1, X_2, ..., X_n\\) sequence ..d., independent identically distributed, random variables distribution mean \\(\\mu\\) standard deviation \\(\\sigma < \\infty\\). ,\\[\n\\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\mu,{\\sigma\\\\sqrt{n}}\\right)\n\\]lot going theorem. First, notice drawing independent samples parent population. central limit theorem (CLT) specify form parent distribution, finite variance (\\(\\sigma < \\infty\\)). Second, CLT tells us form new random variable involves sum individual random variables (case, sample mean \\(\\bar{X}\\)), distribution new random variable approximately normal. case sample mean, expected value (first parameter normal distribution) mean parent population. standard deviation (second parameter normal distribution) standard deviation parent population divided sample size \\(n\\). Let’s summarize ideas.process creating new random variable sum independent, identically distributed random variables approximately normal.process creating new random variable sum independent, identically distributed random variables approximately normal.approximation normal distribution improves sample size \\(n\\).approximation normal distribution improves sample size \\(n\\).mean variance sampling distribution function mean variance parent population, sample size \\(n\\), form new random variable.mean variance sampling distribution function mean variance parent population, sample size \\(n\\), form new random variable.go back review examples, exercises, homework problems previous lessons hypothesis testing, see found symmetric, normal-“looking” sampling distributions created test statistics involved process summing. One example skewed sampling distribution golf ball example, test statistic difference maximum minimum value (involve summation). hard overstate historical importance theorem field inferential statistics science general.get understanding intuition central limit theorem, let’s simulate data evaluate.","code":""},{"path":"HYPTESTCLT.html","id":"simulating-data-for-the-clt","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.2.3 Simulating data for the CLT","text":"section, going use artificial example know population distribution parameters. repeat sampling population distribution many times plot distribution summary statistic interest, sample mean, demonstrate CLT. purely educational thought experiment give confidence validity CLT.Suppose upcoming election Colorado Proposition ballot. Now suppose 65% Colorado voters support Proposition . poll random sample \\(n\\) Colorado voters. Prior conducting sample, can think sample sequence ..d. random variables (voters) binomial distribution one trial (vote) run probability success (support measure) 0.65. words, random variable take value 1 (support) 0 (oppose). Figure 21.3 plot pmf parent distribution (\\(\\textsf{Binom}(1,\\, 0.65)\\)):\nFigure 21.3: Binomial pmf one trial probability success 0.65.\nclearly normally distributed. , fact, discrete. mean \\(X\\) 0.65 standard deviation \\(\\sqrt{0.65(1 - 0.65)} = 0.477\\).first simulation, let sample size ten, \\(n = 10\\). typically small CLT apply, still use starting point. code , obtain sample size 10 binomial distribution record observed mean \\(\\bar{x}\\), method moments estimate probability success. repeat process 10,000 times get empirical distribution \\(\\bar{X}\\). (Note \\(\\bar{X}\\) mean 1s 0s, can thought proportion voters sample support measure. Often, population proportion denoted \\(\\pi\\) sample proportion denoted \\(\\hat{\\pi}\\).)Since summing ..d. variables, sampling distribution mean look like normal distribution. mean close 0.65 (mean parent distribution), standard deviation close \\(\\sqrt{\\frac{p(1 - p)}{n}} = \\sqrt{\\frac{0.65(1 - 0.65)}{10}} = 0.151\\) (standard deviation parent distribution divided \\(\\sqrt{n}\\)).Remember lessons probability, results mean standard deviation depend CLT. results properties expectation independent samples. distribution sample mean (.e., shape sampling distribution) approximately normal result CLT, Figure 21.4.\nFigure 21.4: Sampling distribution sample proportion sample size 10.\nNote sampling distribution sample mean bell-curve shape, skew left particular small sample size. state approximation improves sample size.way determine impact sample size inference population, let’s record often sample 10 failed indicate support measure. (often sample proportion less equal 0.5?) Remember, artificial example, know population favor measure, 65% approval. However, point estimate 0.5, led believe population support measure.Even though know 65% Colorado voters support measure, sample size 10 failed indicate support 25.05% time.Let’s take larger sample. code , repeat sample size 25. Figure 21.5 plots sampling distribution.\nFigure 21.5: Sampling distribution sample proportion sample size 25.\nincreasing sample size 25, standard deviation sample proportion decreased. According central limit theorem, decreased \\(\\sigma/\\sqrt{25} = \\sqrt{\\frac{p(1 - p)}{25}} = 0.095\\). Also, skew became less severe (shape became “normal”). , sample size 25 failed indicate support 6.23% time. reasonably follows even larger sample continue trends. Figure 21.6 demonstrates trends.\nFigure 21.6: Sampling distribution proportion different sample sizes.\n","code":"\ngf_dist(\"binom\", size = 1, prob = 0.65, plot_size = 1) %>%\n  gf_theme(theme_classic()) %>%\n  gf_theme(scale_x_continuous(breaks = c(0, 1))) %>%\n  gf_labs(y = \"Probability\", x = \"X\")\nset.seed(5501)\nresults <- do(10000)*mean(rbinom(10, 1, 0.65))\nfavstats(~mean, data = results)##  min  Q1 median  Q3 max    mean        sd     n missing\n##  0.1 0.5    0.7 0.8   1 0.64932 0.1505716 10000       0\nresults %>%\n summarise(low_result = mean(~mean <= 0.5))##   low_result\n## 1     0.2505\nset.seed(5501)\nresults <- do(10000)*mean(rbinom(25, 1, 0.65))\nresults %>%\n summarise(low_result = mean(~mean <= 0.5))##   low_result\n## 1     0.0623"},{"path":"HYPTESTCLT.html","id":"summary-of-example","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.2.4 Summary of example","text":"example, knew true proportion voters supported proposition. Based knowledge, simulated behavior sample proportion. taking sample size \\(n\\), recording sample proportion (sample mean 1s 0s), repeating process thousands times. reality, know true underlying level support; , take sample repeatedly, thousands times, parent population. Sampling can expensive time-consuming. Thus, take one random sample size \\(n\\), acknowledge resulting sample proportion one observation underlying normal distribution. determine values \\(\\pi\\) (true unknown population proportion) reasonably resulted observed sample proportion.","code":""},{"path":"HYPTESTCLT.html","id":"other-distributions-for-estimators","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.3 Other distributions for estimators","text":"Prior using CLT hypothesis testing, want discuss sampling distributions based CLT normality assumptions. large part theoretical statistics mathematically deriving distribution sample statistics. methods, obtain sample statistic, determine distribution statistic certain conditions, use information make statement population parameter. now discuss commonly used sampling distribution: \\(t\\) distribution.","code":""},{"path":"HYPTESTCLT.html","id":"students-t","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.3.1 Student’s t","text":"Let \\(X_1, X_2, ..., X_n\\) ..d. sequence random variables, mean \\(\\mu\\) standard deviation \\(\\sigma\\). Recall central limit theorem tells us \\[\n\\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\mu, {\\sigma\\\\sqrt{n}}\\right)\n\\]Rearranging, find test statistic left-side expression distributed approximately standard normal (normal distribution mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\)):\\[\n{\\bar{X} - \\mu\\\\sigma/\\sqrt{n}} \\overset{approx}{\\sim} \\textsf{Norm}(0, 1)\n\\], \\(\\sigma\\) unknown. Thus, estimate . can estimate \\(S\\), sample standard deviation, now need know distribution \\({\\bar{X} - \\mu\\S/\\sqrt{n}}\\). follow normal distribution.Lemma: Let \\(X_1, X_2, ..., X_n\\) ..d. sequence random variables normal population mean \\(\\mu\\) standard deviation \\(\\sigma\\). ,\n\\[\n{\\overline{X} - \\mu\\S/\\sqrt{n}} \\sim \\textsf{t}(n - 1)\n\\]\\(\\textsf{t}(n - 1)\\) distribution read “\\(t\\)” distribution. \\(t\\) distribution one parameter: degrees freedom. left-hand side expression \\(\\left({\\bar{X}-\\mu\\S/\\sqrt{n}}\\right)\\) referred \\(t\\) statistic, tells us many standard deviations sample mean population mean.proof lemma outside scope book, terribly complicated. follows simple algebra fact ratio standard normal random variable square root chi-squared random variable, \\(S\\), divided ’s degrees freedom follows \\(t\\) distribution.\\(t\\) distribution similar standard normal distribution, longer tails. seems make sense context estimating \\(\\mu\\) substituting sample standard deviation \\(S\\) population standard deviation \\(\\sigma\\) adds variability random variable.Figure 21.7 plot \\(t\\) distribution, shown blue line, bell shape looks similar normal distribution, show red line. However, tails \\(t\\) distribution thicker, means observations likely fall beyond two standard deviations mean normal distribution. sample small, value \\(s\\) used compute standard error \\((s/\\sqrt{n})\\) isn’t reliable. extra thick tails \\(t\\) distribution exactly correction need resolve problem. degrees freedom 30 , \\(t\\) distribution nearly indistinguishable normal distribution.\nFigure 21.7: distibtion t.\n","code":"\ngf_dist(\"norm\", color = \"red\") %>%\n  gf_dist(\"t\", df = 3, color = \"blue\") %>%\n  gf_theme(theme_bw())"},{"path":"HYPTESTCLT.html","id":"important-note","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.3.2 Important Note","text":"may noticed important condition lemma . assumed \\(X_i\\) sequence random variables normally distributed. central limit theorem normality assumption, distribution \\(t\\) statistic subject distribution underlying population. large enough sample size, assumption necessary. magic number, resources state long \\(n\\) least 30-40, underlying distribution doesn’t matter. coincides said previously: degrees freedom 30 , \\(t\\) distribution nearly indistinguishable normal distribution. smaller sample sizes, underlying distribution relatively symmetric unimodal.One advantage simulation-based inference methods methods rely distributional assumptions. However, simulation-based methods may smaller power sample size.","code":""},{"path":"HYPTESTCLT.html","id":"hypothesis-tests-using-the-clt","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.4 Hypothesis tests using the CLT","text":"now ready reexamine previous examples using mathematically derived sampling distribution via CLT.","code":""},{"path":"HYPTESTCLT.html","id":"body-temperature","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.4.1 Body temperature","text":"repeat body temperature analysis Chapter 20 homework, now using CLT. use \\(\\alpha = 0.05\\). Recall paper American Medical Association86 questioned long-held belief average body temperature human 98.6 degrees Fahrenheit. authors paper believe average human body temperature less 98.6.","code":""},{"path":"HYPTESTCLT.html","id":"step-1--state-the-null-and-alternative-hypotheses-4","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.4.1.1 Step 1- State the null and alternative hypotheses","text":"\\(H_0\\): average body temperature 98.6; \\(\\mu = 98.6\\)\\(H_A\\): average body temperature less 98.6; \\(\\mu < 98.6\\)","code":""},{"path":"HYPTESTCLT.html","id":"step-2---compute-a-test-statistic.-5","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.4.1.2 Step 2 - Compute a test statistic.","text":"population variance unknown, use \\(t\\) distribution. Remember \\[\n{\\bar{X} - \\mu\\S/\\sqrt{n}} \\sim \\textsf{t}(n - 1)\n\\]\nThus, test statistic \\[\n\\frac{\\bar{x} - 98.6}{s / \\sqrt{n}}\n\\]data available file “temperature.csv”.Remember, \\(t\\) statistic tells us many standard deviations sample mean population mean (null hypothesis value). sample mean data 5 standard deviations null hypothesis mean. assumptions discuss end problem.","code":"\nfavstats(~temperature, data = temperature)##   min   Q1 median   Q3   max     mean        sd   n missing\n##  96.3 97.8   98.3 98.7 100.8 98.24923 0.7331832 130       0\ntemperature %>%\n  summarise(mean = mean(temperature), sd = sd(temperature), \n            test_stat = (mean - 98.6) / (sd / sqrt(130)))## # A tibble: 1 x 3\n##    mean    sd test_stat\n##   <dbl> <dbl>     <dbl>\n## 1  98.2 0.733     -5.45"},{"path":"HYPTESTCLT.html","id":"step-3---determine-the-p-value.-5","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.4.1.3 Step 3 - Determine the p-value.","text":"now want find p-value \\(\\mbox{P}(t \\leq -5.45)\\) 129 \\((n - 1)\\) degrees freedom, given null hypothesis true. , given true mean human body temperature 98.6. use R get one-sided p-value.also use R function t_test(), specify variable interest, data set, hypothesized mean value, alternative hypothesis. Remember use help(t_test) ?t_test access R documentation t_test function.notice p-value much smaller p-value method used homework problem 3 last chapter. test statistic involves assumptions uses data continuous discrete (positive negative difference 98.6 observed value).","code":"\npt(-5.45, 129)## [1] 1.232178e-07\nt_test(~temperature, data = temperature, mu = 98.6, alternative = \"less\")## \n##  One Sample t-test\n## \n## data:  temperature\n## t = -5.4548, df = 129, p-value = 1.205e-07\n## alternative hypothesis: true mean is less than 98.6\n## 95 percent confidence interval:\n##      -Inf 98.35577\n## sample estimates:\n## mean of x \n##  98.24923"},{"path":"HYPTESTCLT.html","id":"step-4---draw-a-conclusion-7","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.4.1.4 Step 4 - Draw a conclusion","text":"Based data, true mean human body temperature 98.6, probability observing mean 98.25 less 0.00000012. extremely unlikely, reject null hypothesis average body temperature 98.6 conclude sufficient evidence say true average body temperature less 98.6.","code":""},{"path":"HYPTESTCLT.html","id":"summary-and-rules-of-thumb","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5 Summary and rules of thumb","text":"covered great deal lesson. core, central limit theorem statement distribution sum independent, identically distributed random variables. sum approximately normal.","code":""},{"path":"HYPTESTCLT.html","id":"numerical-data-1","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5.1 Numerical data","text":"First, summarize rules thumb use CLT \\(t\\) distribution.central limit theorem works regardless underlying distribution. However, parent population highly skewed, data needed. CLT works well sample sizes exceed 30 40. data fairly symmetric, less data needed.central limit theorem works regardless underlying distribution. However, parent population highly skewed, data needed. CLT works well sample sizes exceed 30 40. data fairly symmetric, less data needed.estimating mean standard error sample numerical data, \\(t\\) distribution little accurate normal distribution. assumption parent population normally distributed. \\(t\\) distribution works well even small samples, long data close symmetrical unimodal.estimating mean standard error sample numerical data, \\(t\\) distribution little accurate normal distribution. assumption parent population normally distributed. \\(t\\) distribution works well even small samples, long data close symmetrical unimodal.medium-sized samples, least 15 data points, \\(t\\) distribution still works long data roughly symmetric unimodal.medium-sized samples, least 15 data points, \\(t\\) distribution still works long data roughly symmetric unimodal.large data sets 30-40 , \\(t\\) normal distribution can used, suggest always using \\(t\\) distribution.large data sets 30-40 , \\(t\\) normal distribution can used, suggest always using \\(t\\) distribution.Now, let’s discuss assumptions \\(t\\) distribution check .Independence observations. difficult assumption verify. collect simple random sample less 10% population, data experiment random process, feel better assumption. data comes experiment, can plot data versus time collected see patterns indicate relationship. design experiment course discusses ideas detail.Independence observations. difficult assumption verify. collect simple random sample less 10% population, data experiment random process, feel better assumption. data comes experiment, can plot data versus time collected see patterns indicate relationship. design experiment course discusses ideas detail.Observations come nearly normal distribution. second condition difficult verify small data sets. often () take look plot data obvious departures normal distribution, usually form prominent outliers, (ii) consider whether previous experiences alert us data may nearly normal. However, sample size somewhat large, can relax condition. example, moderate skew acceptable sample size 30 , strong skew acceptable sample size 60 .Observations come nearly normal distribution. second condition difficult verify small data sets. often () take look plot data obvious departures normal distribution, usually form prominent outliers, (ii) consider whether previous experiences alert us data may nearly normal. However, sample size somewhat large, can relax condition. example, moderate skew acceptable sample size 30 , strong skew acceptable sample size 60 .typical plot used evaluate normality assumption called quantile-quantile plot. form scatterplot empirical quantiles data versus exact quantile values theoretical distribution. points fall along line, data match distribution. exact match realistic, look major departures line.Figure 21.8 quantile-quantile plot body temperature data. largest value may outlier. may want verify data point entered correctly. fact points line larger values line smaller values indicates data may longer tails normal distribution. really 3 values larger quantiles, fact, data may slightly skewed left. also indicated comparison mean median. However, since 130 data points results overly concern us, impact findings.\nFigure 21.8: Q-Q plot body temperature data.\nExtreme data points, outliers, can cause concern. later chapters, look ways detect outliers also seen boxplots. First, outliers problematic normal distributions rarely outliers, presence one may indicate departure normality. Second, outliers big impact estimation methods mean standard deviation, whether method moments maximum likelihood estimate.can also check impacts assumptions using methods (like previous chapters) hypothesis test. methods give conclusion, can confident results. Another way check robustness assumptions simulate data different distributions evaluate performance test simulated data.","code":"\ngf_qq(~temperature, data = temperature) %>%\n  gf_qqline(~temperature, data = temperature) %>%\n  gf_theme(theme_bw())"},{"path":"HYPTESTCLT.html","id":"binary-data","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5.2 Binary data","text":"distribution binomial random variable simple scalar transformations , proportions success found dividing sample size, approximately normal CLT. Since binomial random variables bounded zero number trials, make sure probability success close zero one. , number successes close 0 \\(n\\). general rule thumb number success failures least 10.","code":""},{"path":"HYPTESTCLT.html","id":"tappers-and-listeners-1","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5.3 Tappers and listeners","text":"Recall Stanford University graduate student conducted experiment using tapper-listener game. tapper picks well-known song, taps ’s tune, sees listener can guess song. 50% tappers expected listener correctly guess song. researcher wanted determine whether reasonable expectation.","code":""},{"path":"HYPTESTCLT.html","id":"step-1--state-the-null-and-alternative-hypotheses.","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5.3.1 Step 1- State the null and alternative hypotheses.","text":"two hypotheses:\\(H_0\\): tappers correct, generally 50% time listeners able guess tune. \\(p = 0.50\\)\\(H_A\\): tappers incorrect, either less 50% listeners able guess tune. \\(p \\neq 0.50\\)","code":""},{"path":"HYPTESTCLT.html","id":"step-2---compute-a-test-statistic.-6","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5.3.2 Step 2 - Compute a test statistic.","text":"test statistic want use sample mean \\(\\bar{X}\\). mean 1s 0s guess, 1 indicates correct guess 0 indicates incorrect guess. method moments estimate probability success. independent samples binomial distribution, CLT,\\[\n\\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\pi,\\sqrt\\frac{\\pi(1-\\pi)}{n}\\right)\n\\]learned, approximation improves sample size. rule thumb, analysts comfortable using CLT problem number successes failures 10 greater.study, 42 120 listeners (\\(\\bar{x} = \\hat{p} = 0.35\\)) able guess tune. observed value test statistic.","code":""},{"path":"HYPTESTCLT.html","id":"step-3---determine-the-p-value.-6","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5.3.3 Step 3 - Determine the p-value.","text":"now want find p-value one-sided probability \\(\\mbox{P}(\\bar{X} \\leq 0.35)\\), given null hypothesis true. , given true probability success 0.50. use R get one-sided value double since test two-sided sampling distribution symmetrical.small p-value consistent got using exact binomial test simulated empirical p-values Chapter 20.Important note:\ncalculation standard deviation sampling distribution, used null hypothesized value probability success.","code":"\n2*pnorm(0.35, mean = 0.5, sd = sqrt(0.5*0.5 / 120))## [1] 0.001015001"},{"path":"HYPTESTCLT.html","id":"step-4---draw-a-conclusion-8","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.5.3.4 Step 4 - Draw a conclusion","text":"Based data, listeners guessing correct 50% time, 1--1000 chance 42 less, 78 , listeners get right. much less 0.05, reject null hypothesis listeners guessing correctly half time. sufficient evidence conclude true correct-guess rate different 50%.Note R built functions perform test. explore functions, use help(prop.test) ?prop.test learn . find options improve performance test. welcome read methods. , computers, researchers spent time optimizing performance asymptotic methods CLT.test single proportion tapper-listener example using R.p-value small, reported \\(0.0014\\). study confidence interval soon, don’t worry part output yet. alternative hypothesis also listed, options one-sided two-sided tests.Exercise:\nconduct one-sided test? null value 0.45?87The exact test uses function binom.test().code used Chapter 20:","code":"\nprop.test(x = 42, n = 120, p = 0.5)## \n##  1-sample proportions test with continuity correction\n## \n## data:  42 out of 120\n## X-squared = 10.208, df = 1, p-value = 0.001398\n## alternative hypothesis: true p is not equal to 0.5\n## 95 percent confidence interval:\n##  0.2667083 0.4430441\n## sample estimates:\n##    p \n## 0.35\npval(prop.test(42, 120, alternative = \"less\", p = 0.45))##   p.value \n## 0.0174214\nbinom.test(42,120)## \n## \n## \n## data:  42 out of 120\n## number of successes = 42, number of trials = 120, p-value = 0.001299\n## alternative hypothesis: true probability of success is not equal to 0.5\n## 95 percent confidence interval:\n##  0.2652023 0.4423947\n## sample estimates:\n## probability of success \n##                   0.35\n2*pbinom(42, 120, prob = 0.5)## [1] 0.001299333"},{"path":"HYPTESTCLT.html","id":"homework-problems-20","chapter":"21 Hypothesis Testing with the Central Limit Theorem","heading":"21.6 Homework Problems","text":"Suppose roll fair six-sided die let \\(X\\) resulting number. distribution \\(X\\) discrete uniform. (six discrete outcomes equally likely.)Suppose roll fair die five times record value \\(\\bar{X}\\), mean resulting rolls. central limit theorem, distribution \\(\\bar{X}\\)?Suppose roll fair die five times record value \\(\\bar{X}\\), mean resulting rolls. central limit theorem, distribution \\(\\bar{X}\\)?Simulate process R. Plot resulting empirical distribution \\(\\bar{X}\\) report mean standard deviation \\(\\bar{X}\\). expected?\n(HINT: can simulate die roll using sample function. careful make sure use properly. See function documentation help.)Simulate process R. Plot resulting empirical distribution \\(\\bar{X}\\) report mean standard deviation \\(\\bar{X}\\). expected?(HINT: can simulate die roll using sample function. careful make sure use properly. See function documentation help.)Repeat parts ) b) \\(n = 20\\) \\(n = 50\\). Describe notice. Make sure three plots plotted \\(x\\)-axis scale. can use facets combine data one tibble.Repeat parts ) b) \\(n = 20\\) \\(n = 50\\). Describe notice. Make sure three plots plotted \\(x\\)-axis scale. can use facets combine data one tibble.nutrition label bag potato chips says one ounce (28 gram) serving potato chips 130 calories contains ten grams fat, three grams saturated fat. random sample 35 bags yielded sample mean 134 calories standard deviation 17 calories. evidence nutrition label provide accurate measure calories bags potato chips? conditions necessary applying normal distribution parent population checked satisfied.\nquestion framed terms two possibilities: nutrition label accurately lists correct average calories per bag chips , may investigated hypothesis test.nutrition label bag potato chips says one ounce (28 gram) serving potato chips 130 calories contains ten grams fat, three grams saturated fat. random sample 35 bags yielded sample mean 134 calories standard deviation 17 calories. evidence nutrition label provide accurate measure calories bags potato chips? conditions necessary applying normal distribution parent population checked satisfied.question framed terms two possibilities: nutrition label accurately lists correct average calories per bag chips , may investigated hypothesis test.Write null alternative hypotheses.Write null alternative hypotheses.level significance going use?level significance going use?distribution test statistic \\({\\bar{X} - \\mu\\S/\\sqrt{n}}\\)? Calculate observed value.distribution test statistic \\({\\bar{X} - \\mu\\S/\\sqrt{n}}\\)? Calculate observed value.Calculate p-value.Calculate p-value.Draw conclusion.Draw conclusion.Paired dataAre textbooks actually cheaper online? compare price textbooks University California, Los Angeles (UCLA) bookstore Amazon.com. Seventy-three UCLA courses randomly sampled Spring 2010, representing less 10% UCLA courses. class multiple books, expensive text considered.data file textbooks.csv data folder.textbook two corresponding prices data set: one UCLA bookstore one Amazon. Therefore, textbook price UCLA bookstore natural correspondence textbook price Amazon. two sets observations special correspondence, said paired.analyze paired data, often useful look difference outcomes pair observations. textbooks, look difference prices, represented diff variable. important always subtract using consistent order; Amazon prices always subtracted UCLA prices.data tidy? Explain.data tidy? Explain.Make scatterplot UCLA price versus Amazon price. Add 45 degree line plot.Make scatterplot UCLA price versus Amazon price. Add 45 degree line plot.Make histogram differences price.\nhypotheses :\\(H_0\\): \\(\\mu_{diff}=0\\). difference average textbook price.\\(H_A\\): \\(\\mu_{diff} \\neq 0\\). difference average prices.Make histogram differences price.hypotheses :\\(H_0\\): \\(\\mu_{diff}=0\\). difference average textbook price.\\(H_A\\): \\(\\mu_{diff} \\neq 0\\). difference average prices.use \\(t\\) distribution, variable diff independent normally distributed. Since 73 books represent less 10% population, assumption random sample independent reasonable. Check normality using qqnorsim() openintro package. generates 8 qq plots simulated normal data can use judge diff variable.use \\(t\\) distribution, variable diff independent normally distributed. Since 73 books represent less 10% population, assumption random sample independent reasonable. Check normality using qqnorsim() openintro package. generates 8 qq plots simulated normal data can use judge diff variable.Run \\(t\\) test diff variable. Report p-value conclusion.Run \\(t\\) test diff variable. Report p-value conclusion.Create bootstrap distribution generate 95% confidence interval mean differences, diff column.Create bootstrap distribution generate 95% confidence interval mean differences, diff column.really difference book sources, variable binomial , null, probability success \\(\\pi = 0.5\\). Run hypothesis test using variable .really difference book sources, variable binomial , null, probability success \\(\\pi = 0.5\\). Run hypothesis test using variable .use permutation test example? Explain.use permutation test example? Explain.lesson, used expression degrees freedom lot. expression mean? sample size \\(n\\), \\(n-1\\) degrees freedom \\(t\\) distribution? Give short concise answer (one paragraph). likely little research .lesson, used expression degrees freedom lot. expression mean? sample size \\(n\\), \\(n-1\\) degrees freedom \\(t\\) distribution? Give short concise answer (one paragraph). likely little research .Deborah Toohey running Congress, campaign manager claims 50% support district’s electorate. Ms. Toohey’s opponent claimed Ms. Toohey less 50%. Set hypothesis test evaluate right.Deborah Toohey running Congress, campaign manager claims 50% support district’s electorate. Ms. Toohey’s opponent claimed Ms. Toohey less 50%. Set hypothesis test evaluate right.run one-sided two-sided hypothesis test?run one-sided two-sided hypothesis test?Write null alternative hypothesis.Write null alternative hypothesis.level significance going use?level significance going use?assumptions test?assumptions test?Calculate test statistic.\nNote: newspaper collects simple random sample 500 likely voters district estimates Toohey’s support 52%.Calculate test statistic.Note: newspaper collects simple random sample 500 likely voters district estimates Toohey’s support 52%.Calculate p-value.Calculate p-value.Draw conclusion.Draw conclusion.","code":""},{"path":"ADDTESTS.html","id":"ADDTESTS","chapter":"22 Additional Hypothesis Tests","heading":"22 Additional Hypothesis Tests","text":"","code":""},{"path":"ADDTESTS.html","id":"objectives-21","chapter":"22 Additional Hypothesis Tests","heading":"22.1 Objectives","text":"Conduct interpret goodness fit test using Pearson’s chi-squared randomization evaluate independence two categorical variables.Conduct interpret goodness fit test using Pearson’s chi-squared randomization evaluate independence two categorical variables.Explain chi-squared distribution relates normal distribution, used, changing parameters impacts shape distribution.Explain chi-squared distribution relates normal distribution, used, changing parameters impacts shape distribution.Conduct interpret hypothesis test equality two means equality two variances using permutation CLT.Conduct interpret hypothesis test equality two means equality two variances using permutation CLT.Know check assumptions tests reading.Know check assumptions tests reading.","code":""},{"path":"ADDTESTS.html","id":"introduction-2","chapter":"22 Additional Hypothesis Tests","heading":"22.2 Introduction","text":"purpose chapter put learned block perspective also add couple new tests demonstrate statistical tests.Remember using data answer research questions. far can hypothesis tests confidence intervals. close link two methods. key ideas generate single number metric use answering research question obtain sampling distribution metric.obtaining sampling distribution used randomization approximation permutation exact tests, probability models, mathematical models, bootstrap. different assumptions different areas applied. cases, several methods can applied problem get sense robustness different assumptions. example, run randomization test test using CLT give similar results, can feel better decision.Finding single number metric answer research question can difficult. example, homework last chapter, wanted determine prices books campus bookstore different Amazon’s prices. metric decided use mean differences prices. best way answer question? metric used historically need use \\(t\\) distribution. However, ways prices books can differ. Jack Welch CEO GE years made claim customers don’t care average care variability. average temperature setting GE refrigerator adapt. However temperature great variability, upset. maybe metrics incorporate variability might good. bootstrap notes, looked ages males females HELP study. using randomization permutation test, assumed difference distribution ages males females. However, alternative measured difference distributions using means. means two populations equal distributions differ ways, example variability. conduct separate test variances careful multiple comparisons case Type 1 error inflated.also learned use information data impacts power test. golf ball example, used range metric, power looking differences expected values null hypothesis. mathematical theory leads better estimators, called likelihood ratio tests, beyond scope book. can create simulation simulate data alternative hypothesis measure power. give sense quality metric. briefly looked measuring power earlier chapter go idea chapter.finish block examining problems two variables. first case categorical least one categorical variables two levels. second case, examine two variables one numeric categorical. categorical variable two levels.","code":""},{"path":"ADDTESTS.html","id":"other-distribution-for-estimators","chapter":"22 Additional Hypothesis Tests","heading":"22.3 Other distribution for estimators","text":"Chapter 21, discussed \\(t\\) distribution, different sampling distribution based CLT normality assumption. theoretical statistics, often mathematically derive sampling distribution obtaining sampling statistic, determining distribution statistic certain conditions, using information make statement population parameter. now discuss another commonly used sampling distribution: chi-squared distribution.","code":""},{"path":"ADDTESTS.html","id":"chi-squared","chapter":"22 Additional Hypothesis Tests","heading":"22.3.1 Chi-squared","text":"Recall central limit theorem tells us reasonably large sample sizes, \\(\\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(\\mu,\\sigma/\\sqrt{n})\\). However, expression involves two unknowns: \\(\\mu\\) \\(\\sigma\\). case binary data, population variance function population proportion (\\(\\mbox{Var}(X)=\\pi(1-\\pi)\\)), really just one unknown. case continuous data, standard deviation need estimated.Let \\(S^2\\) defined :\\[\nS^2={\\sum (X_i-\\bar{X})^2\\n-1}\n\\]Recall unbiased estimate \\(\\sigma^2\\). sampling distribution \\(S^2\\) can found using following lemma.Lemma: Let \\(X_1,X_2,...,X_n\\) iid sequence random variables normal population mean \\(\\mu\\) standard deviation \\(\\sigma\\). ,\\[\n{(n-1)S^2\\\\sigma^2}\\sim \\textsf{Chisq}(n-1)\n\\]\\(\\textsf{Chisq}(n-1)\\) distribution read “chi-squared” distribution (“chi” pronounced “kye”). can also written \\(\\chi^2 (n-1)\\). chi-squared distribution one parameter: degrees freedom. chi-squared distribution used contexts goodness fit problems like golf ball example last lesson, discuss particular application later chapter.proof lemma outside scope book, terribly complicated. follows fact sum \\(n\\) squared random variables, standard normal distribution, follows chi-squared distribution \\(n\\) degrees freedom.lemma can used draw inferences \\(\\sigma^2\\). particular value \\(\\sigma^2\\), know \\(S^2\\) behave. , particular value \\(S^2\\), can figure reasonable values \\(\\sigma^2\\).practice, one rarely estimates \\(\\sigma\\) purpose inferring \\(\\sigma\\). Typically, interested estimating \\(\\mu\\) need account added uncertainty estimating \\(\\sigma\\) well. discuss next section.","code":""},{"path":"ADDTESTS.html","id":"important-note-1","chapter":"22 Additional Hypothesis Tests","heading":"22.3.2 Important Note","text":"Just like \\(t\\) distribution, lemma assumed \\(X_i\\) sequence random variables normally distributed. central limit theorem normality assumption, distribution chi-square statistic subject distribution underlying population. large enough expected counts, assumption necessary. , magic number, resources state expected count less one 20% expected counts less five.One advantage simulation-based inference methods methods rely distributional assumptions. However, simulation-based methods may smaller power sample size.","code":""},{"path":"ADDTESTS.html","id":"categorical-data-1","chapter":"22 Additional Hypothesis Tests","heading":"22.4 Categorical data","text":"worth spending time common approaches categorical data may come across. already dealt categorical data extent course. performed hypothesis tests built confidence intervals \\(\\pi\\), population proportion “success” binary cases (example, support local measure vote). problem single variable. Also, golf ball example involved counts four types golf ball. considered categorical data observation characterized qualitative value (number ball). data summarized counting many balls sample belong type. single variable.another scenario, suppose presented two qualitative variables like know independent. example, discussed methods determining whether coin fair. wanted know whether flipping coin day night changes fairness coin? case, two categorical variables two levels : result coin flip (heads vs tails) time day (day vs night). solved type problem looking difference probabilities success using randomization mathematically derived solutions, CLT. also used hypergeometric distribution obtain exact p-value.next explore scenario involves categorical data two variables least one variable two levels. However, note merely scratching surface studies. take entire course statistical methods categorical data. book giving solid foundation learn advanced methods.","code":""},{"path":"ADDTESTS.html","id":"health-evaluation-and-linkage-to-primary-care","chapter":"22 Additional Hypothesis Tests","heading":"22.4.1 Health evaluation and linkage to primary care","text":"Health Evaluation Linkage Primary Care (HELP) study clinical trial adult inpatients recruited detoxification unit. Patients primary care physician randomized receive multidisciplinary assessment brief motivational intervention usual care, goal linking primary medical care.HELPrct data set available mosaicData package. interested whether differences males females, particularly variable substance, primary substance abuse.","code":""},{"path":"ADDTESTS.html","id":"remove-or-edit-this---interested-in-diff-of-means-in-ch-25-bootstrap","chapter":"22 Additional Hypothesis Tests","heading":"22.4.1.1 REMOVE OR EDIT THIS - INTERESTED IN DIFF OF MEANS IN CH 25 BOOTSTRAP","text":"\nFigure 22.1: distribution age HELP study gender.\n\nFigure 22.2: distribution age HELP study gender.\nFigures 22.1 22.2 indicate might slight difference means, statistically significant?","code":"\ndata(\"HELPrct\")\nHELP_sub <- HELPrct %>%\n  select(age,sex)\nfavstats(age~sex,data=HELP_sub)##      sex min Q1 median   Q3 max     mean       sd   n missing\n## 1 female  21 31     35 40.5  58 36.25234 7.584858 107       0\n## 2   male  19 30     35 40.0  60 35.46821 7.750110 346       0\nHELP_sub %>%\n  gf_boxplot(age~sex) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Gender\",y=\"Age (years)\")\nHELP_sub %>%\n  gf_dhistogram(~age|sex,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Age\",y=\"\")"},{"path":"ADDTESTS.html","id":"age-and-sex-example-ends-here","chapter":"22 Additional Hypothesis Tests","heading":"22.4.1.2 Age and Sex example ends here","text":"three substances: alcohol, cocaine, heroin. ’d like know evidence proportions use differ males females. data set, observe modest differences.need test statistic determine difference substance abuse males females.","code":"\ntally(substance ~ sex, data = HELPrct, format = \"prop\", margins = TRUE)##          sex\n## substance    female      male\n##   alcohol 0.3364486 0.4075145\n##   cocaine 0.3831776 0.3208092\n##   heroin  0.2803738 0.2716763\n##   Total   1.0000000 1.0000000"},{"path":"ADDTESTS.html","id":"test-statistic","chapter":"22 Additional Hypothesis Tests","heading":"22.4.2 Test statistic","text":"help us develop understand test statistic, let’s simplify use simple theoretical example.Suppose 2 x 2 contingency table like one .\\[\n\\begin{array}{lcc}\n& \\mbox{Response 1} & \\mbox{Response 2} \\\\\n\\mbox{Group 1} & n_{11} & n_{12} \\\\\n\\mbox{Group 2} & n_{21} & n_{22}\n\\end{array}\n\\]null hypothesis two variables independent, classical test statistic used Pearson chi-squared test statistic (\\(X^2\\)). similar one used golf ball example. Let \\(e_{ij}\\) expected count \\(\\)th row \\(j\\)th column null hypothesis, test statistic :\\[\nX^2=\\sum_{=1}^2 \\sum_{j=1}^2 {(n_{ij}-e_{ij})^2\\e_{ij}}\n\\]find \\(e_{ij}\\)? expect count \\(H_0\\)? find , recognize \\(H_0\\) (independence), joint probability equal product marginal probabilities. Let \\(\\pi_{ij}\\) probability outcome appearing row \\(\\) column \\(j\\). absence information, best guess \\(\\pi_{ij}\\) \\(\\hat{\\pi}_{ij}={n_{ij}\\n}\\), \\(n\\) total sample size. null hypothesis assumption independence, thus \\(\\pi_{ij}=\\pi_{+}\\pi_{+j}\\) \\(\\pi_{+}\\) represents total probability ending row \\(\\) \\(\\pi_{+j}\\) represents total probability ending column \\(j\\). Note \\(\\pi_{+}\\) estimated \\(\\hat{\\pi}_{+}\\) \\[\n\\hat{\\pi}_{+}={n_{+}\\n}\n\\]\nThus simple 2 x 2 example, :\\[\n\\hat{\\pi}_{+}={n_{+}\\n}={n_{i1}+n_{i2}\\n}\n\\]Group 1 :\\[\n\\hat{\\pi}_{1+}={n_{1+}\\n}={n_{11}+n_{12}\\n}\n\\], \\(H_0\\), best guess \\(\\pi_{ij}\\) :\\[\n\\hat{\\pi}_{ij}=\\hat{\\pi}_{+}\\hat{\\pi}_{+j}={n_{+}\\n}{n_{+j}\\n} = {n_{i1}+n_{i2}\\n}{n_{1j}+n_{2j}\\n}\n\\]Continuing, \\(H_0\\) expected cell count :\\[\ne_{ij}=n\\hat{\\pi}_{ij}=n{n_{+}\\n}{n_{+j}\\n}={n_{+}n_{+j}\\n}\n\\]","code":""},{"path":"ADDTESTS.html","id":"extension-to-larger-tables","chapter":"22 Additional Hypothesis Tests","heading":"22.4.3 Extension to larger tables","text":"advantage using Pearson chi-squared can extended larger contingency tables, name given tables multiple categorical variables. Suppose comparing two categorical variables, one \\(r\\) levels \\(c\\) levels. ,\n\\[\nX^2=\\sum_{=1}^r \\sum_{j=1}^c {(n_{ij}-e_{ij})^2\\e_{ij}}\n\\]null hypothesis independence, \\(X^2\\) test statistic follows chi-squared distribution \\((r-1)(c-1)\\) degrees freedom.","code":""},{"path":"ADDTESTS.html","id":"assumptions-reiterated","chapter":"22 Additional Hypothesis Tests","heading":"22.4.3.1 Assumptions reiterated","text":"Note use test statistic, expected cell counts must reasonably large. fact, \\(e_{ij}\\) less 1 20% \\(e_{ij}\\)’s less 5. occurs, combine cells look different test.may look abstract, let’s break example.","code":""},{"path":"ADDTESTS.html","id":"test-statistic-for-the-help-example","chapter":"22 Additional Hypothesis Tests","heading":"22.4.4 Test statistic for the HELP example","text":"","code":""},{"path":"ADDTESTS.html","id":"rewrite-the-following-with-the-help-data-and-omit-the-coin-flip-example","chapter":"22 Additional Hypothesis Tests","heading":"22.4.4.1 REWRITE THE FOLLOWING WITH THE HELP DATA AND OMIT THE COIN FLIP EXAMPLE","text":"Suppose flip coin 40 times day 40 times night obtain results .\\[\n\\begin{array}{lcc}\n& \\mbox{Heads} & \\mbox{Tails} \\\\\n\\mbox{Day} & 22 & 18 \\\\\n\\mbox{Night} & 17 & 23\n\\end{array}\n\\]find Pearson chi-squared (\\(X^2\\)), need figure expected value \\(H_0\\). Recall \\(H_0\\) two variables independent. ’s helpful add row column totals prior finding expected counts:\\[\n\\begin{array}{lccc}\n& \\mbox{Heads} & \\mbox{Tails} & \\mbox{Row Total}\\\\\n\\mbox{Day} & 22 & 18  & 40\\\\\n\\mbox{Night} & 17 & 23 & 40 \\\\\n\\mbox{Column Total} & 39 & 41 & 80\n\\end{array}\n\\]Thus independence, expected count equal row sum multiplied column sum divided overall sum. ,\\[\ne_{11} = {40*39\\80}= 19.5\n\\]Continuing fashion yields following table expected counts:\\[\n\\begin{array}{lcc}\n& \\mbox{Heads} & \\mbox{Tails} \\\\\n\\mbox{Day} & 19.5 & 20.5 \\\\\n\\mbox{Night} & 19.5 & 20.5\n\\end{array}\n\\]Now can find \\(X^2\\):\\[\nX^2= {(22-19.5)^2\\19.5}+{(17-19.5)^2\\19.5}+{(18-20.5)^2\\20.5}+{(23-20.5)^2\\20.5}\n\\]can probably tell, \\(X^2\\) essentially comparing observed counts expected counts \\(H_0\\). larger difference observed expected, larger value \\(X^2\\). normalized dividing expected counts since data cell leads larger contribution sum. \\(H_0\\), statistic follows chi-squared distribution \\((R-1)(C-1)\\), case 1, degrees freedom (\\(R\\) number rows \\(C\\) number columns).","code":""},{"path":"ADDTESTS.html","id":"calculate-the-p-value","chapter":"22 Additional Hypothesis Tests","heading":"22.4.5 Calculate the p-value","text":"find Pearson chi-squared statistic (\\(X^2\\)) corresponding p-value chi-squared distribution R use following code:Note chi-squared test statistic sum squared differences. Thus distribution, chi-squared, skewed right bounded left zero. departure null hypothesis means value right tail distribution. use one minus CDF calculation p-value., \\(p\\)-value suggests enough evidence say two variables dependent.course built function R make calculations easier. chisq.test().just want test statistic, permutation tests, use:","code":"\ne<-c(19.5,19.5,20.5,20.5)\no<-c(22,17,18,23)\nx2<-sum(((o-e)^2)/e)\n\nx2## [1] 1.250782\n1-pchisq(x2,1)## [1] 0.2634032\ncoin <- tibble(time = c(rep(\"Day\",40),rep(\"Night\",40)),\n               result = c(rep(c(\"Heads\",\"Tails\"),c(22,18)),rep(c(\"Heads\",\"Tails\"),c(17,23))))\ntally(~time+result,data=coin)##        result\n## time    Heads Tails\n##   Day      22    18\n##   Night    17    23\nchisq.test(tally(~time+result,data=coin),correct = FALSE)## \n##  Pearson's Chi-squared test\n## \n## data:  tally(~time + result, data = coin)\n## X-squared = 1.2508, df = 1, p-value = 0.2634\nchisq(~time+result,data=coin)## X.squared \n##  1.250782"},{"path":"ADDTESTS.html","id":"permutation-test","chapter":"22 Additional Hypothesis Tests","heading":"22.4.6 Permutation test","text":"complete analysis HELP data first using randomization, approximate permutation, test.First let’s write hypotheses:\\(H_0\\): variables sex substance independent.\\(H_a\\): variables sex substance dependent.use chi-squared test statistic test statistic. use different test statistic using absolute value function instead square function need write custom function.First, let’s get observed value test statistic:Next use permutation randomization process find sampling distribution test statistics.Figure 22.3 visual summary results helps us gain intuition p-value. also plot theoretical chi-squared distribution dark blue overlay.\nFigure 22.3: Sampling distribution chi-squared statistic randomization test.\nfind p-value using prop1().don’t double value chi-squared one sided test due fact squared differences.Based p-value, fail reject hypothesis variables independent.","code":"\nobs <- chisq(substance~sex,data=HELPrct)\nobs## X.squared \n##  2.026361\nset.seed(2720)\nresults <- do(1000)*chisq(substance~shuffle(sex),data=HELPrct)\nresults %>%\n  gf_dhistogram(~X.squared,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept = obs,color=\"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_dist(\"chisq\",df=2,color=\"darkblue\") %>%\n  gf_labs(title=\"Sampling distribution of chi-squared test statistic\",\n          subtitle=\"For the variables sex and substance in the HELPrct data set\",\n          x=\"Test statistic\")\nprop1((~X.squared>=obs),data=results)## prop_TRUE \n## 0.3536464"},{"path":"ADDTESTS.html","id":"chi-squared-test-in-r","chapter":"22 Additional Hypothesis Tests","heading":"22.4.7 Chi-squared test in R","text":"jump straight using function chisq.test().get p-value close one randomization permutation test. Remember randomization test shuffled variable sex many replications calculated value test statistic replication. shuffling null hypothesis assumed independence two variables. process led empirical estimate sampling distribution, gray histogram previous graph. section, null hypothesis appropriate assumptions, sampling distribution chi-squared, blue line previous graph. used calculate p-value directly.Notice null hypothesis true, test statistic minimum value zero. can’t use bootstrap confidence interval problem zero never interval. can edge interval.","code":"\nchisq.test(tally(substance~sex,data=HELPrct))## \n##  Pearson's Chi-squared test\n## \n## data:  tally(substance ~ sex, data = HELPrct)\n## X-squared = 2.0264, df = 2, p-value = 0.3631"},{"path":"ADDTESTS.html","id":"numerical-data-2","chapter":"22 Additional Hypothesis Tests","heading":"22.5 Numerical data","text":"","code":""},{"path":"ADDTESTS.html","id":"convert-to-a-two-sample-t-test","chapter":"22 Additional Hypothesis Tests","heading":"22.5.0.1 CONVERT TO A TWO SAMPLE T-TEST","text":"Sometimes want compare means across many groups. case two variables one continuous categorical. might initially think pairwise comparisons, two sample t-tests, solution; example, three groups, might tempted compare first mean second, third, finally compare second third means total three comparisons. However, strategy can treacherous. many groups many comparisons, likely eventually find difference just chance, even difference populations.section, learn new method called analysis variance (ANOVA) new test statistic called \\(F\\). ANOVA uses single hypothesis test check whether means across many groups equal. hypotheses :\\(H_0\\): mean outcome across groups. statistical notation, \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\) \\(\\mu_i\\) represents mean outcome observations category \\(\\).\\(H_A\\): least one mean different.Generally must check three conditions data performing ANOVA \\(F\\) distribution:observations independent within across groups,data within group nearly normal, andthe variability across groups equal.three conditions met, may perform ANOVA determine whether data provide strong evidence null hypothesis \\(\\mu_i\\) equal.","code":""},{"path":"ADDTESTS.html","id":"mlb-batting-performance","chapter":"22 Additional Hypothesis Tests","heading":"22.5.1 MLB batting performance","text":"like discern whether real differences batting performance baseball players according position: outfielder (), infielder (), designated hitter (DH), catcher (C). use data set mlbbat10 openintro package saved file mlb_obp.csv modified original data set include 200 bats. batting performance measured -base percentage. -base percentage roughly represents fraction time player successfully gets base hits home run.Read data R.Let’s review data:Next change variable position factor give us greater control.means group pretty close .Exercise:\nnull hypothesis consideration following: \\(\\mu_{} = \\mu_{} = \\mu_{DH} = \\mu_{C}\\).\nWrite null corresponding alternative hypotheses plain language.88If data 2010 season, need hypothesis test? population interest?making decisions claims 2010 season, need hypothesis testing. can just use summary statistics. However, want generalize years leagues, need hypothesis test.Exercise:\nConstruct side--side boxplots.Figure 22.4 side--side boxplots.\nFigure 22.4: Boxplots base percentage position played.\nlargest difference sample means designated hitter catcher positions. Consider original hypotheses:\\(H_0\\): \\(\\mu_{} = \\mu_{} = \\mu_{DH} = \\mu_{C}\\)\\(H_A\\): average -base percentage (\\(\\mu_i\\)) varies across () groups.","code":"\nmlb_obp <- read_csv(\"data/mlb_obp.csv\")\ninspect(mlb_obp)## \n## categorical variables:  \n##       name     class levels   n missing\n## 1 position character      4 327       0\n##                                    distribution\n## 1 IF (47.1%), OF (36.7%), C (11.9%) ...        \n## \n## quantitative variables:  \n##      name   class   min    Q1 median     Q3   max     mean         sd   n\n## ...1  obp numeric 0.174 0.309  0.331 0.3545 0.437 0.332159 0.03570249 327\n##      missing\n## ...1       0\nmlb_obp <- mlb_obp %>%\n  mutate(position=as.factor(position))\nfavstats(obp~position,data=mlb_obp)##   position   min      Q1 median      Q3   max      mean         sd   n missing\n## 1        C 0.219 0.30000 0.3180 0.35700 0.405 0.3226154 0.04513175  39       0\n## 2       DH 0.287 0.31625 0.3525 0.36950 0.412 0.3477857 0.03603669  14       0\n## 3       IF 0.174 0.30800 0.3270 0.35275 0.437 0.3315260 0.03709504 154       0\n## 4       OF 0.265 0.31475 0.3345 0.35300 0.411 0.3342500 0.02944394 120       0\nmlb_obp %>%\n  gf_boxplot(obp~position) %>%\n  gf_labs(x=\"Position Played\",y=\"On Base Percentage\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title=\"Comparison of OBP for different positions\")"},{"path":"ADDTESTS.html","id":"add-permutation-test","chapter":"22 Additional Hypothesis Tests","heading":"22.5.1.1 ADD PERMUTATION TEST","text":"might inappropriate run test simply estimating whether difference \\(\\mu_{DH}\\) \\(\\mu_{C}\\) statistically significant 0.05 significance level? primary issue inspecting data picking groups compared. inappropriate examine data eye (informal testing) afterwards decide parts formally test. called data snooping data fishing. Naturally pick groups large differences formal test, leading inflation Type 1 Error rate. understand better, let’s consider slightly different problem.Suppose measure aptitude students 20 classes large elementary school beginning year. school, students randomly assigned classrooms, differences observe classes start year completely due chance. However, many groups, probably observe groups look rather different . select classes look different, probably make wrong conclusion assignment wasn’t random. might formally test differences pairs classes, informally evaluated classes eye choosing extreme cases comparison.next chapter, learn use \\(F\\) statistic ANOVA test whether observed differences means happened just chance even difference respective population means.","code":""},{"path":"ADDTESTS.html","id":"homework-problems-21","chapter":"22 Additional Hypothesis Tests","heading":"22.6 Homework Problems","text":"Golf ballsRepeat analysis golf ball problem earlier book.Load data tally data table. data golf_balls.csv.Load data tally data table. data golf_balls.csv.Using function chisq.test(), conduct hypothesis test equally likely distribution balls. may read help menu chisq.test().Using function chisq.test(), conduct hypothesis test equally likely distribution balls. may read help menu chisq.test().Repeat part b, assume balls numbers 1 2 occur 30% time balls 3 4 occur 20% time.Repeat part b, assume balls numbers 1 2 occur 30% time balls 3 4 occur 20% time.Test variance\nperformed test variance create .Test varianceWe performed test variance create .Using MLB reading, subset .Using MLB reading, subset .Create side--side boxplot.\nhypotheses :Create side--side boxplot.hypotheses :\\(H_0\\): \\(\\sigma^2_{}=\\sigma^2_{}\\). difference variance base percentage infielders outfielders.\\(H_A\\): \\(\\sigma^2_{}\\neq \\sigma^2_{}\\). difference variances.Use differences sample standard deviations test statistic. Using permutation test, find p-value discuss decision.","code":""},{"path":"ADDTESTS.html","id":"remove-this","chapter":"22 Additional Hypothesis Tests","heading":"22.6.0.1 REMOVE THIS???","text":"Create bootstrap distribution differences sample standard deviations, report 95% confidence interval. Compare part d.Exploration chi-squared \\(t\\) distributions.R, plot pdf random variable chi-squared distribution 1 degree freedom. plot, include pdfs degrees freedom 5, 10 50. Describe behavior pdf changes increasing degrees freedom.R, plot pdf random variable chi-squared distribution 1 degree freedom. plot, include pdfs degrees freedom 5, 10 50. Describe behavior pdf changes increasing degrees freedom.Repeat part () \\(t\\) distribution. Add pdf standard normal random variable well. notice?Repeat part () \\(t\\) distribution. Add pdf standard normal random variable well. notice?","code":""},{"path":"ANOVA.html","id":"ANOVA","chapter":"23 Analysis of Variance","heading":"23 Analysis of Variance","text":"","code":""},{"path":"ANOVA.html","id":"objectives-22","chapter":"23 Analysis of Variance","heading":"23.1 Objectives","text":"Conduct interpret hypothesis test equality two means using permutation \\(F\\) distribution.Conduct interpret hypothesis test equality two means using permutation \\(F\\) distribution.Know check assumptions tests reading.Know check assumptions tests reading.","code":""},{"path":"ANOVA.html","id":"introduction---keep-any-of-this","chapter":"23 Analysis of Variance","heading":"23.2 Introduction - KEEP ANY OF THIS???","text":"purpose chapter put learned block perspective also add couple new tests demonstrate statistical tests.Remember using data answer research questions. far can hypothesis tests confidence intervals. close link two methods. key ideas generate single number metric use answering research question obtain sampling distribution metric.obtaining sampling distribution used randomization approximation permutation exact tests, probability models, mathematical models, bootstrap. different assumptions different areas applied. cases, several methods can applied problem get sense robustness different assumptions. example, run randomization test test using CLT give similar results, can feel better decision.Finding single number metric answer research question can difficult. example, homework last chapter, wanted determine prices books campus bookstore different Amazon’s prices. metric decided use mean differences prices. best way answer question? metric used historically need use \\(t\\) distribution. However, ways prices books can differ. Jack Welch CEO GE years made claim customers don’t care average care variability. average temperature setting GE refrigerator adapt. However temperature great variability, upset. maybe metrics incorporate variability might good. bootstrap notes, looked ages males females HELP study. using randomization permutation test, assumed difference distribution ages males females. However, alternative measured difference distributions using means. means two populations equal distributions differ ways, example variability. conduct separate test variances careful multiple comparisons case Type 1 error inflated.also learned use information data impacts power test. golf ball example, used range metric, power looking differences expected values null hypothesis. mathematical theory leads better estimators, called likelihood ratio tests, beyond scope book. can create simulation simulate data alternative hypothesis measure power. give sense quality metric. briefly looked measuring power earlier chapter go idea chapter.finish block examining problems two variables. first case categorical least one categorical variables two levels. second case, examine two variables one numeric categorical. categorical variable two levels.","code":""},{"path":"ANOVA.html","id":"numerical-data---keep-any-of-this","chapter":"23 Analysis of Variance","heading":"23.3 Numerical data - KEEP ANY OF THIS???","text":"Sometimes want compare means across many groups. case two variables one continuous categorical. might initially think pairwise comparisons, two sample t-tests, solution; example, three groups, might tempted compare first mean second, third, finally compare second third means total three comparisons. However, strategy can treacherous. many groups many comparisons, likely eventually find difference just chance, even difference populations.section, learn new method called analysis variance (ANOVA) new test statistic called \\(F\\). ANOVA uses single hypothesis test check whether means across many groups equal. hypotheses :\\(H_0\\): mean outcome across groups. statistical notation, \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\) \\(\\mu_i\\) represents mean outcome observations category \\(\\).\\(H_A\\): least one mean different.Generally must check three conditions data performing ANOVA \\(F\\) distribution:observations independent within across groups,data within group nearly normal, andthe variability across groups equal.three conditions met, may perform ANOVA determine whether data provide strong evidence null hypothesis \\(\\mu_i\\) equal.","code":""},{"path":"ANOVA.html","id":"mlb-batting-performance-1","chapter":"23 Analysis of Variance","heading":"23.3.1 MLB batting performance","text":"like discern whether real differences batting performance baseball players according position: outfielder (), infielder (), designated hitter (DH), catcher (C). use data set mlbbat10 openintro package saved file mlb_obp.csv modified original data set include 200 bats. batting performance measured -base percentage. -base percentage roughly represents fraction time player successfully gets base hits home run.Read data R.Let’s review data:Next change variable position factor give us greater control.means group pretty close .Exercise:\nnull hypothesis consideration following: \\(\\mu_{} = \\mu_{} = \\mu_{DH} = \\mu_{C}\\).\nWrite null corresponding alternative hypotheses plain language.89If data 2010 season, need hypothesis test? population interest?making decisions claims 2010 season, need hypothesis testing. can just use summary statistics. However, want generalize years leagues, need hypothesis test.Exercise:\nConstruct side--side boxplots.Figure 22.4 side--side boxplots.\nFigure 22.4: Boxplots base percentage position played.\nlargest difference sample means designated hitter catcher positions. Consider original hypotheses:\\(H_0\\): \\(\\mu_{} = \\mu_{} = \\mu_{DH} = \\mu_{C}\\)\\(H_A\\): average -base percentage (\\(\\mu_i\\)) varies across () groups.","code":"\nmlb_obp <- read_csv(\"data/mlb_obp.csv\")\ninspect(mlb_obp)## \n## categorical variables:  \n##       name     class levels   n missing\n## 1 position character      4 327       0\n##                                    distribution\n## 1 IF (47.1%), OF (36.7%), C (11.9%) ...        \n## \n## quantitative variables:  \n##      name   class   min    Q1 median     Q3   max     mean         sd   n\n## ...1  obp numeric 0.174 0.309  0.331 0.3545 0.437 0.332159 0.03570249 327\n##      missing\n## ...1       0\nmlb_obp <- mlb_obp %>%\n  mutate(position = as.factor(position))\nfavstats(obp ~ position, data = mlb_obp)##   position   min      Q1 median      Q3   max      mean         sd   n missing\n## 1        C 0.219 0.30000 0.3180 0.35700 0.405 0.3226154 0.04513175  39       0\n## 2       DH 0.287 0.31625 0.3525 0.36950 0.412 0.3477857 0.03603669  14       0\n## 3       IF 0.174 0.30800 0.3270 0.35275 0.437 0.3315260 0.03709504 154       0\n## 4       OF 0.265 0.31475 0.3345 0.35300 0.411 0.3342500 0.02944394 120       0\nmlb_obp %>%\n  gf_boxplot(obp ~ position) %>%\n  gf_labs(x = \"Position Played\", y = \"On Base Percentage\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title = \"Comparison of OBP for different positions\")"},{"path":"ANOVA.html","id":"delete-these-two-sections---duplicated","chapter":"23 Analysis of Variance","heading":"23.3.1.1 DELETE THESE TWO SECTIONS - DUPLICATED","text":"might inappropriate run test simply estimating whether difference \\(\\mu_{DH}\\) \\(\\mu_{C}\\) statistically significant 0.05 significance level? primary issue inspecting data picking groups compared. inappropriate examine data eye (informal testing) afterwards decide parts formally test. called data snooping data fishing. Naturally pick groups large differences formal test, leading inflation Type 1 Error rate. understand better, let’s consider slightly different problem.Suppose measure aptitude students 20 classes large elementary school beginning year. school, students randomly assigned classrooms, differences observe classes start year completely due chance. However, many groups, probably observe groups look rather different . select classes look different, probably make wrong conclusion assignment wasn’t random. might formally test differences pairs classes, informally evaluated classes eye choosing extreme cases comparison.next section learn use \\(F\\) statistic ANOVA test whether observed differences means happened just chance even difference respective population means.","code":""},{"path":"ANOVA.html","id":"analysis-of-variance-anova-and-the-f-test","chapter":"23 Analysis of Variance","heading":"23.3.2 Analysis of variance (ANOVA) and the F test","text":"method analysis variance context focuses answering one question: variability sample means large seems unlikely chance alone? question different earlier testing procedures since simultaneously consider many groups, evaluate whether sample means differ expect natural variation. call variability mean square groups (\\(MSG\\)), associated degrees freedom, \\(df_{G}=k-1\\) \\(k\\) groups. \\(MSG\\) can thought scaled variance formula means. null hypothesis true, variation sample means due chance shouldn’t large. typically use software find \\(MSG\\), however, derivation follows. Let \\(\\bar{x}\\) represent mean outcomes across groups. mean square groups computed \\[\nMSG = \\frac{1}{df_{G}}SSG = \\frac{1}{k-1}\\sum_{=1}^{k} n_{}\\left(\\bar{x}_{} - \\bar{x}\\right)^2\n\\]\\(SSG\\) called sum squares groups \\(n_{}\\) sample size group \\(\\).mean square groups , , quite useless hypothesis test. need benchmark value much variability expected among sample means null hypothesis true. end, compute pooled variance estimate, often abbreviated mean square error (\\(MSE\\)), associated degrees freedom value \\(df_E=n-k\\). helpful think \\(MSE\\) measure variability within groups. find \\(MSE\\), let \\(\\bar{x}\\) represent mean outcomes across groups. sum squares total (\\(SST\\))} computed \n\\(SST = \\sum_{=1}^{n} \\left(x_{} - \\bar{x}\\right)^2\\),\nsum observations data set. compute sum squared errors (\\(SSE\\)) one two equivalent ways:\\[\nSSE = SST - SSG = (n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2\n\\]\\(s_i^2\\) sample variance (square standard deviation) residuals group \\(\\). \\(MSE\\) standardized form \\(SSE\\): \\(MSE = \\frac{1}{df_{E}}SSE\\).null hypothesis true, differences among sample means due chance, \\(MSG\\) \\(MSE\\) equal. test statistic ANOVA, examine fraction \\(MSG\\) \\(MSE\\):\\[F = \\frac{MSG}{MSE}\\]\\(MSG\\) represents measure -group variability, \\(MSE\\) measures variability within groups. Using permutation test, look difference mean squared errors test statistic instead ratio.can use \\(F\\) statistic evaluate hypotheses called F test. p-value can computed \\(F\\) statistic using \\(F\\) distribution, two associated parameters: \\(df_{1}\\) \\(df_{2}\\). \\(F\\) statistic ANOVA, \\(df_{1} = df_{G}\\) \\(df_{2}= df_{E}\\). \\(F\\) really ratio chi-squared distributions.larger observed variability sample means (\\(MSG\\)) relative within-group observations (\\(MSE\\)), larger \\(F\\) stronger evidence null hypothesis. larger values \\(F\\) represent stronger evidence null hypothesis, use upper tail distribution compute p-value.\\(F\\) statistic \\(F\\) test\nAnalysis variance (ANOVA) used test whether mean outcome differs across 2 groups. ANOVA uses test statistic \\(F\\), represents standardized ratio variability sample means relative variability within groups. \\(H_0\\) true model assumptions satisfied, statistic \\(F\\) follows \\(F\\) distribution parameters \\(df_{1}=k-1\\) \\(df_{2}=n-k\\). upper tail \\(F\\) distribution used represent p-value.","code":""},{"path":"ANOVA.html","id":"anova","chapter":"23 Analysis of Variance","heading":"23.3.2.1 ANOVA","text":"use R perform calculations ANOVA. let’s check assumptions first.three conditions must check ANOVA analysis: observations must independent, data group must nearly normal, variance within group must approximately equal.Independence\ndata simple random sample less 10% population, condition reasonable. processes experiments, carefully consider whether data may independent (e.g. pairing). MLB data, data sampled. However, obvious reasons independence hold observations. bit hand waving remember independence difficult assess.Approximately normal\none- two-sample testing means, normality assumption especially important sample size quite small. normal probability plots group MLB data shown ; deviation normality infielders, isn’t substantial concern since 150 observations group outliers extreme. Sometimes ANOVA many groups observations per group checking normality group isn’t reasonable. One solution combine groups one set data. First calculate residuals baseball data, calculated taking observed values subtracting corresponding group means. example, outfielder OBP 0.435 residual \\(0.435 - \\bar{x}_{} = 0.082\\). check normality condition, create normal probability plot using residuals simultaneously.Figure 23.1 quantile-quantile plot assess normality assumption.\nFigure 23.1: Quantile-quantile plot two-sample test means.\nConstant variance\nlast assumption variance groups equal one group next. assumption can checked examining side--side box plot outcomes across groups previously. case, variability similar four groups identical. also see output favstats standard deviation varies bit one group next. Whether differences natural variation unclear, report uncertainty meeting assumption final results reported. permutation test assumption can used check results ANOVA.summary, independence always important ANOVA analysis. normality condition important sample sizes group relatively small. constant variance condition especially important sample sizes differ groups.Let’s write hypotheses .\\(H_0\\): average -base percentage equal across four positions.\\(H_A\\): average -base percentage varies across () groups.test statistic ratio means variance pooled within group variance.table contains information need. degrees freedom, mean squared errors, test statistic, p-value. test statistic 1.994, \\(\\frac{0.002519}{0.001263}=1.994\\). p-value larger 0.05, indicating evidence strong enough reject null hypothesis significance level 0.05. , data provide strong evidence average -base percentage varies player’s primary field position.calculation p-value isFigure 23.2 plot \\(F\\) distribution.\nFigure 23.2: F distribution\n","code":"\nmlb_obp %>%\n  gf_qq(~obp | position) %>%\n  gf_qqline() %>%\n  gf_theme(theme_bw())\nsummary(aov(obp ~ position, data = mlb_obp))##              Df Sum Sq  Mean Sq F value Pr(>F)\n## position      3 0.0076 0.002519   1.994  0.115\n## Residuals   323 0.4080 0.001263\npf(1.994, 3, 323, lower.tail = FALSE)## [1] 0.1147443\ngf_dist(\"f\", df1 = 3, df2 = 323) %>%\n  gf_vline(xintercept = 1.994, color = \"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title = \"F distribution\", x = \"F value\")"},{"path":"ANOVA.html","id":"permutation-test-1","chapter":"23 Analysis of Variance","heading":"23.3.2.2 Permutation test","text":"can repeat analysis using permutation test. first run using ratio variances interest difference variances.need way extract mean squared errors output. package called broom within function called tidy() cleans output functions makes data frames.Let’s summarize values meansq column develop test statistic, just pull statistic want able generate difference test statistic well.Now ready. First get test statistic using pull().Let’s put test statistic function include shuffling position variable.Next run randomization test using () function. easier way work purrr package continue work started.slow executing using tidyverse functions slow.Figure 23.3 plot sampling distribution randomization test.\nFigure 23.3: sampling distribution randomization test statistic.\np-value isThis similar p-value ANOVA output.Now let’s repeat analysis use difference variance test statistic.Figure 23.4 plot sampling distribution difference variance.\nFigure 23.4: sampling distribution difference variance randomization test statistic.\nneed observed value find p-value.p-value isAgain similar p-value.reject ANOVA test, know difference least one mean don’t know ones. approach answering question, means different?","code":"\nlibrary(broom)\naov(obp ~ position, data = mlb_obp) %>%\n  tidy()## # A tibble: 2 x 6\n##   term         df   sumsq  meansq statistic p.value\n##   <chr>     <dbl>   <dbl>   <dbl>     <dbl>   <dbl>\n## 1 position      3 0.00756 0.00252      1.99   0.115\n## 2 Residuals   323 0.408   0.00126     NA     NA\naov(obp ~ position, data = mlb_obp) %>%\n  tidy() %>%\n  summarize(stat = meansq[1] / meansq[2]) ## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1  1.99\nobs <- aov(obp ~ position, data = mlb_obp) %>%\n  tidy() %>%\n  summarize(stat = meansq[1] / meansq[2]) %>%\n  pull()\nobs## [1] 1.994349\nf_stat <- function(x){\n  aov(obp ~ shuffle(position), data = x) %>%\n  tidy() %>%\n  summarize(stat = meansq[1] / meansq[2]) %>%\n  pull()\n}\nset.seed(5321)\nf_stat(mlb_obp)## [1] 0.1185079\nset.seed(5321)\nresults <- do(1000)*(f_stat(mlb_obp))\nresults %>%\n  gf_dhistogram(~result, fill = \"cyan\", color = \"black\") %>%\n  gf_dist(\"f\", df1 = 3, df2 = 323, color = \"darkblue\") %>%\n  gf_vline(xintercept = 1.994, color = \"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title = \"Randomization test sampling distribution\",\n          subtitle = \"F distribution is overlayed in blue\",\n          x = \"Test statistic\")\nprop1(~(result >= obs), results)## prop_TRUE \n## 0.0959041\nf_stat2 <- function(x){\n  aov(obp ~ shuffle(position), data = x) %>%\n  tidy() %>%\n  summarize(stat = meansq[1] - meansq[2]) %>%\n  pull(stat)\n}\nset.seed(5321)\nresults <- do(1000)*(f_stat2(mlb_obp))\nresults %>%\n  gf_dhistogram(~result, fill = \"cyan\", color = \"black\") %>%\n  gf_vline(xintercept = 0.001255972, color = \"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title = \"Randomization test sampling distribution\",\n          subtitle = \"Test statistic is the difference in variances\",\n          x = \"Test statistic\")\nobs <- aov(obp ~ position, data = mlb_obp) %>%\n  tidy() %>%\n  summarize(stat = meansq[1] - meansq[2]) %>%\n  pull(stat)\nobs## [1] 0.001255972\nprop1(~(result >= obs), results)## prop_TRUE \n## 0.0959041"},{"path":"ANOVA.html","id":"homework-problems-22","chapter":"23 Analysis of Variance","heading":"23.4 Homework Problems","text":"","code":""},{"path":"ANOVA.html","id":"can-we-keep-this","chapter":"23 Analysis of Variance","heading":"23.4.0.1 CAN WE KEEP THIS???","text":"","code":""},{"path":"ANOVA.html","id":"move-test-of-two-variances-here","chapter":"23 Analysis of Variance","heading":"23.4.0.2 MOVE TEST OF TWO VARIANCES HERE???","text":"Bootstrap hypothesis testingRepeat analysis MLB data reading time generate bootstrap distribution \\(F\\) statistic.","code":""},{"path":"CI.html","id":"CI","chapter":"24 Confidence Intervals","heading":"24 Confidence Intervals","text":"","code":""},{"path":"CI.html","id":"objectives-23","chapter":"24 Confidence Intervals","heading":"24.1 Objectives","text":"Using asymptotic methods based normal distribution, construct interpret confidence interval unknown parameter.Using asymptotic methods based normal distribution, construct interpret confidence interval unknown parameter.Describe relationships confidence intervals, confidence level, sample size.Describe relationships confidence intervals, confidence level, sample size.proportions, able calculate three different approaches confidence intervals using R.proportions, able calculate three different approaches confidence intervals using R.","code":""},{"path":"CI.html","id":"confidence-interval","chapter":"24 Confidence Intervals","heading":"24.2 Confidence interval","text":"point estimate provides single plausible value parameter. However, point estimate rarely perfect; usually error estimate. addition supplying point estimate parameter, next logical step provide plausible range values parameter.","code":""},{"path":"CI.html","id":"capturing-the-population-parameter","chapter":"24 Confidence Intervals","heading":"24.2.1 Capturing the population parameter","text":"plausible range values population parameter called confidence interval. Using point estimate like fishing murky lake spear, using confidence interval like fishing net. can throw spear saw fish, probably miss. hand, toss net area, good chance catching fish.report point estimate, probably hit exact population parameter. hand, report range plausible values – confidence interval – good shot capturing parameter.Exercise:\nwant certain capture population parameter, use wider interval smaller interval?90","code":""},{"path":"CI.html","id":"constructing-a-confidence-interval","chapter":"24 Confidence Intervals","heading":"24.2.2 Constructing a confidence interval","text":"point estimate best guess value parameter, makes sense build confidence interval around value. standard error, measure uncertainty associated point estimate, provides guide large make confidence interval.Generally, know building confidence intervals laid following steps:Identify parameter like estimate (example, \\(\\mu\\)).Identify parameter like estimate (example, \\(\\mu\\)).Identify good estimate parameter (sample mean, \\(\\bar{X}\\)).Identify good estimate parameter (sample mean, \\(\\bar{X}\\)).Determine distribution estimate function estimate.Determine distribution estimate function estimate.Use distribution obtain range feasible values (confidence interval) parameter. (example \\(\\mu\\) parameter interest using CLT, \\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim \\textsf{Norm}(0,1)\\). can solve equation \\(\\mu\\) find reasonable range feasible values.)Use distribution obtain range feasible values (confidence interval) parameter. (example \\(\\mu\\) parameter interest using CLT, \\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim \\textsf{Norm}(0,1)\\). can solve equation \\(\\mu\\) find reasonable range feasible values.)Let’s example solidify ideas.Constructing 95% confidence interval mean\nsampling distribution point estimate can reasonably modeled normal, point estimate observe within 1.96 standard errors true value interest 95% time. Thus, 95% confidence interval point estimate can constructed:\\[ \\hat{\\theta} \\pm\\ 1.96 \\times SE_{\\hat{\\theta}}\\]\n\\(\\hat{\\theta}\\) estimate parameter \\(SE_{\\hat{\\theta}}\\) standard error estimate.can 95% confident interval captures true value. 1.96 can found using qnorm() function. want .95 middle, leaves 0.025 tail. Thus use .975 qnorm() function.Exercise:\nCompute area -1.96 1.96 normal distribution mean 0 standard deviation 1.mathematical terms, derivation confidence follows:Let \\(X_1,X_2,...,X_n\\) ..d. sequence random variables, mean \\(\\mu\\) standard deviation \\(\\sigma\\). central limit theorem tells us \n\\[\n\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\overset{approx}{\\sim}\\textsf{Norm}(0,1)\n\\]significance level \\(0\\leq \\alpha \\leq 1\\), confidence level \\(1-\\alpha\\). Yes \\(\\alpha\\) significance level hypothesis testing. Thus\n\\[\n\\mbox{P}\\left(-z_{\\alpha/2}\\leq {\\bar{X}-\\mu\\\\sigma/\\sqrt{n}} \\leq z_{\\alpha/2}\\right)=1-\\alpha\n\\]\\(z_{\\alpha/2}\\) \\(\\mbox{P}(Z\\geq z_{\\alpha/2})=\\alpha/2\\), \\(Z\\sim \\textsf{Norm}(0,1)\\), see Figure 24.1.\nFigure 24.1: pdf standard normal distribution showing idea develop confidence interval.\n, know \\((1-\\alpha)*100\\%\\) time, \\({\\bar{X}-\\mu\\\\sigma/\\sqrt{n}}\\) \\(-z_{\\alpha/2}\\) \\(z_{\\alpha/2}\\).rearranging expression solving \\(\\mu\\), get:\n\\[\n\\mbox{P}\\left(\\bar{X}-z_{\\alpha/2}{\\sigma\\\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}{\\sigma\\\\sqrt{n}}\\right)=1-\\alpha\n\\]careful interpretation expression. reminder \\(\\bar{X}\\) random variable . population mean, \\(\\mu\\), variable. unknown parameter. Thus, expression probabilistic statement \\(\\mu\\), rather random variable \\(\\bar{X}\\).Nonetheless, expression gives us nice interval “reasonable” values \\(\\mu\\) given particular sample.\\((1-\\alpha)*100\\%\\) confidence interval mean given :\n\\[\n\\mu\\\\left(\\bar{x}\\pm z_{\\alpha/2}{\\sigma\\\\sqrt{n}}\\right)\n\\]Notice equation using lower case \\(\\bar{x}\\), sample mean, thus nothing random interval. Thus use probabilistic statements confidence intervals calculate numerical values data upper /lower limits.applications, common value \\(\\alpha\\) 0.05. case, construct 95% confidence interval, need find \\(z_{0.025}\\) can found quickly qnorm():","code":"\nqnorm(.975)## [1] 1.959964\npnorm(1.96)-pnorm(-1.96)## [1] 0.9500042\nqnorm(1-0.05/2)## [1] 1.959964\nqnorm(.975)## [1] 1.959964"},{"path":"CI.html","id":"unknown-variance","chapter":"24 Confidence Intervals","heading":"24.2.2.1 Unknown Variance","text":"inferring population mean, usually estimate underlying standard deviation well. introduces extra level uncertainty. found \\({\\bar{X}-\\mu\\\\sigma/\\sqrt{n}}\\) approximate normal distribution, \\({\\bar{X}-\\mu\\S/\\sqrt{n}}\\) follows \\(t\\)-distribution \\(n-1\\) degrees freedom. adds additional assumption parent population, distribution \\(X\\), must normal.Thus, \\(\\sigma\\) unknown, \\((1-\\alpha)*100\\%\\) confidence interval mean given :\n\\[\n\\mu\\\\left(\\bar{x}\\pm t_{\\alpha/2,n-1}{s\\\\sqrt{n}}\\right)\n\\]Similar case , \\(t_{\\alpha/2,n-1}\\) can found using qt() function R.practice, \\(X\\) close symmetrical unimodal, can relax assumption normality. Always look sample data. Outliers skewness can causes concern. can always run methods don’t require assumption normality compare results.large sample sizes, choice using normal distribution \\(t\\) distribution irrelevant since close . \\(t\\) distribution requires use degrees freedom careful.","code":""},{"path":"CI.html","id":"body-temperature-example","chapter":"24 Confidence Intervals","heading":"24.2.3 Body Temperature Example","text":"Example:\nFind 95% confidence interval body temperature data last lesson.need mean, standard deviation, sample size data. following R code calculates confidence interval, make sure can follow code.95% confidence interval \\(\\mu\\) \\((98.12,98.38)\\). 95% confident \\(\\mu\\), average human body temperature, interval. Alternatively equally relevant, say 95% similarly constructed intervals contain true mean, \\(\\mu\\). important understand use word confident word probability.link hypothesis testing confidence intervals. Remember used data hypothesis test, null hypothesis \\(H_0\\): average body temperature 98.6 \\(\\mu = 98.6\\). null hypothesized value interval, reject null hypothesis confidence interval.","code":"\ntemperature %>%\n  favstats(~temperature,data=.) %>%\n  select(mean,sd,n) %>%\n  summarise(lower_bound=mean-qt(0.975,129)*sd/sqrt(n),\n            upper_bound=mean+qt(0.975,129)*sd/sqrt(n))##   lower_bound upper_bound\n## 1      98.122    98.37646"},{"path":"CI.html","id":"connect-this-to-the-t-test","chapter":"24 Confidence Intervals","heading":"24.2.3.1 CONNECT THIS TO THE T-TEST","text":"also use R find confidence interval conduct hypothesis test. Read function t_test() help menu determine used mu option.just want interval:reviewing hypothesis test single mean, can see confidence interval formed inverting test statistic. reminder, following equation inverts test statistic.\\[\n\\mbox{P}\\left(\\bar{X}-z_{\\alpha/2}{\\sigma\\\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}{\\sigma\\\\sqrt{n}}\\right)=1-\\alpha\n\\]","code":"\nt_test(~temperature,data=temperature,mu=98.6)## \n##  One Sample t-test\n## \n## data:  temperature\n## t = -5.4548, df = 129, p-value = 2.411e-07\n## alternative hypothesis: true mean is not equal to 98.6\n## 95 percent confidence interval:\n##  98.12200 98.37646\n## sample estimates:\n## mean of x \n##  98.24923\nconfint(t_test(~temperature,data=temperature,mu=98.6))##   mean of x  lower    upper level\n## 1  98.24923 98.122 98.37646  0.95"},{"path":"CI.html","id":"one-sided-intervals","chapter":"24 Confidence Intervals","heading":"24.2.4 One-sided Intervals","text":"remember hypothesis test temperature central limit theorem lesson, may crying foul. one-sided hypothesis test just conducted two-sided test. far, discussed “two-sided” intervals. intervals upper lower bound. Typically, \\(\\alpha\\) apportioned equally two tails. (Thus, look \\(z_{\\alpha/2}\\).)“one-sided” intervals, bound interval one side. construct one-sided intervals concerned whether parameter exceeds stays threshold. Building one-sided interval similar building two-sided intervals, except rather dividing \\(\\alpha\\) two, simply apportion \\(\\alpha\\) relevant side. difficult part determine need upper bound lower bound.body temperature study, alternative hypothesis mean less 98.6. confidence interval, want find largest value mean thus want upper bound. trying reject hypothesis showing alternative smaller null hypothesized value. Finding lower limit help us since confidence interval indicates interval starts lower value unbounded . Let’s just make numbers; suppose lower confidence bound 97.5. know true average temperature value greater. helpful. However, find upper confidence bound value 98.1, know true average temperature likely larger value. much helpful.Repeating analysis mind.Notice upper bound one-sided interval smaller upper bound two-sided interval since 0.05 going upper tail.","code":"\ntemperature %>%\n  favstats(~temperature,data=.) %>%\n  select(mean,sd,n) %>%\n  summarise(upper_bound=mean+qt(0.95,129)*sd/sqrt(n))##   upper_bound\n## 1    98.35577\nconfint(t_test(~temperature,data=temperature,alternative=\"less\"))##   mean of x lower    upper level\n## 1  98.24923  -Inf 98.35577  0.95"},{"path":"CI.html","id":"confidence-intervals-for-two-proportions","chapter":"24 Confidence Intervals","heading":"24.3 Confidence intervals for two proportions","text":"","code":""},{"path":"CI.html","id":"convert-to-one-proportion","chapter":"24 Confidence Intervals","heading":"24.3.0.1 CONVERT TO ONE PROPORTION","text":"hypothesis testing several examples two proportions. tested problems permutation test using hypergeometric. chapters homework, presented hypothesis test two proportions using asymptotic normal distribution, central limit theorem. chapter present three methods answering research question, permutation test, hypothesis test using normal distribution, confidence interval.Earlier book, fact first chapter, encountered experiment examined whether implanting stent brain patient risk stroke helps reduce risk stroke. results first 30 days study, included 451 patients, summarized R code . results surprising! point estimate suggests patients received stents may higher risk stroke: \\(p_{trmt} - p_{control} = 0.090\\).Notice R uses variables names alphabetic order \\(p_{control} - p_{trmt} = - 0.090\\). problem. fix changing variables factors.","code":"\nstent <- read_csv(\"data/stent_study.csv\")\ntally(~group+outcome30,data=stent,margins = TRUE)##          outcome30\n## group     no_event stroke Total\n##   control      214     13   227\n##   trmt         191     33   224\n##   Total        405     46   451\ntally(outcome30~group,data=stent,margins = TRUE,format=\"proportion\")##           group\n## outcome30     control       trmt\n##   no_event 0.94273128 0.85267857\n##   stroke   0.05726872 0.14732143\n##   Total    1.00000000 1.00000000\nobs<-diffprop(outcome30~group,data=stent)\nobs##    diffprop \n## -0.09005271"},{"path":"CI.html","id":"stop-here---remove-permutation-test-for-two-proportions-and-skip-to-confidence-interval-for-two-proportions-using-normal-model","chapter":"24 Confidence Intervals","heading":"24.3.0.2 STOP HERE - REMOVE PERMUTATION TEST FOR TWO PROPORTIONS AND SKIP TO CONFIDENCE INTERVAL FOR TWO PROPORTIONS USING NORMAL MODEL","text":"","code":""},{"path":"CI.html","id":"confidence-interval-for-two-proportions-using-normal-model","chapter":"24 Confidence Intervals","heading":"24.3.1 Confidence interval for two proportions using normal model","text":"","code":""},{"path":"CI.html","id":"convert-to-one-proportion-1","chapter":"24 Confidence Intervals","heading":"24.3.1.1 CONVERT TO ONE PROPORTION","text":"conditions applying normal model already verified, can proceed construction confidence interval. Remember form confidence interval \\[\\text{point estimate} \\ \\pm\\ z^{\\star}SE\\]point estimate -0.09. standard error different since can’t assume proportion strokes equal. estimate standard error \\[SE    = \\sqrt{\\frac{p_{control}(1-p_{control})}{n_{control}} + \\frac{p_{trmt}(1-p_{trmt})}{n_{trmt}}}\\]\\[SE \\approx \\sqrt{\\frac{0.057(1-0.057)}{227} + \\frac{0.15(1-0.15)}{224}} = 0.0284\\]close pooled value nearly equal sample sizes.critical value found normal quantile.95% confidence interval \\[ - 0.09 \\ \\pm\\ 1.96 \\times  0.0284 \\quad \\\\quad (-0.146,- 0.034)\\]\n95% confident difference proportions strokes control treatment groups -0.146 -0.034. Since include zero, confident different. supports hypothesis tests. confidence interval accurate method smaller samples sizes. actual coverage rate, percentage intervals contain true population parameter, nominal coverage rate. means true 95% similarly constructed 95% confidence intervals contain true parameter. pooled estimate standard error accurate small sample sizes. example , sample sizes large performance method adequate.course, R built function calculate hypothesis test confidence interval two proportions.p-value little different one calculated closer randomization test, approximation exact permutation test, correction factor applied. Read online correction learn . run code correction factor get p-value calculated . confidence interval little different function used stroke success event, since zero interval, get conclusion.Essentially, confidence intervals hypothesis tests serve similar purposes, answer slightly different questions. confidence interval gives range feasible values parameter given particular sample. hypothesis test tells whether specific value feasible given sample. Sometimes can informally conduct hypothesis test simply building interval observing whether hypothesized value contained interval. disadvantage approach yield specific \\(p\\)-value. disadvantage hypothesis test give range values test statistic.hypothesis tests, confidence intervals imperfect. 1--20 properly constructed 95% confidence intervals fail capture parameter interest. similar idea Type 1 error.","code":"\nqnorm(.975)## [1] 1.959964\nprop_test(outcome30~group,data=stent)## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  tally(outcome30 ~ group)\n## X-squared = 9.0233, df = 1, p-value = 0.002666\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  0.03022922 0.14987619\n## sample estimates:\n##    prop 1    prop 2 \n## 0.9427313 0.8526786\nprop_test(outcome30~group,data=stent,correct=FALSE)## \n##  2-sample test for equality of proportions without continuity\n##  correction\n## \n## data:  tally(outcome30 ~ group)\n## X-squared = 9.9823, df = 1, p-value = 0.001581\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  0.03466401 0.14544140\n## sample estimates:\n##    prop 1    prop 2 \n## 0.9427313 0.8526786"},{"path":"CI.html","id":"changing-the-confidence-level","chapter":"24 Confidence Intervals","heading":"24.4 Changing the confidence level","text":"Suppose want consider confidence intervals confidence level somewhat higher 95%; perhaps like confidence level 99%. Think back analogy trying catch fish: want sure catch fish, use wider net. create 99% confidence level, must also widen 95% interval. hand, want interval lower confidence, 90%, make original 95% interval slightly slimmer.95% confidence interval structure provides guidance make intervals new confidence levels. general 95% confidence interval point estimate comes nearly normal distribution:\\[\\text{point estimate}\\ \\pm\\ 1.96\\times SE \\]three components interval: point estimate, “1.96”, standard error. choice \\(1.96\\times SE\\), also called margin error, based capturing 95% data since estimate within 1.96 standard errors true value 95% time. choice 1.96 corresponds 95% confidence level.Exercise:\n\\(X\\) normally distributed random variable, often \\(X\\) within 2.58 standard deviations mean?91To create 99% confidence interval, change 1.96 95% confidence interval formula \\(2.58\\).normal approximation crucial precision confidence intervals. learn method called bootstrap allow us find confidence intervals without assumption normality.","code":""},{"path":"CI.html","id":"interpreting-confidence-intervals","chapter":"24 Confidence Intervals","heading":"24.5 Interpreting confidence intervals","text":"careful eye might observed somewhat awkward language used describe confidence intervals.Correct interpretation:\nXX% confident population parameter …Incorrect language might try describe confidence interval capturing population parameter certain probability. one common errors: might useful think probability, confidence level quantifies plausible parameter interval.Another especially important consideration confidence intervals try capture population parameter. intervals say nothing confidence capturing individual observations, proportion observations, capturing point estimates. Confidence intervals attempt capture population parameters.","code":""},{"path":"CI.html","id":"homework-problems-23","chapter":"24 Confidence Intervals","heading":"24.6 Homework Problems","text":"Chronic illnessIn 2013, Pew Research Foundation reported “45% U.S. adults report live one chronic conditions”.92 However, value based sample, may perfect estimate population parameter interest . study reported standard error 1.2%, normal model may reasonably used setting.Create 95% confidence interval proportion U.S. adults live one chronic conditions. Also interpret confidence interval context study.Create 99% confidence interval proportion U.S. adults live one chronic conditions. Also interpret confidence interval context study.Identify following statements true false. Provide explanation justify answers.can say certainty confidence interval part contains true percentage U.S. adults suffer chronic illness.repeated study 1,000 times constructed 95% confidence interval study, approximately 950 confidence intervals contain true fraction U.S. adults suffer chronic illnesses.poll provides statistically significant evidence (\\(\\alpha = 0.05\\) level) percentage U.S. adults suffer chronic illnesses 50%.Since standard error 1.2%, 1.2% people study communicated uncertainty answer.Suppose researchers formed one-sided hypothesis, believed true proportion less 50%. find equivalent one-sided 95% confidence interval taking upper bound two-sided 95% confidence interval.Vegetarian college studentsSuppose 8% college students vegetarians. Determine following statements true false, explain reasoning.distribution sample proportions vegetarians random samples size 60 approximately normal since \\(n \\ge 30\\).distribution sample proportions vegetarian college students random samples size 50 right skewed.random sample 125 college students 12% vegetarians considered unusual.random sample 250 college students 12% vegetarians considered unusual.standard error reduced one-half increased sample size 125 ~250.99% confidence wider 95% higher confidence level requires wider interval.Orange tabbiesSuppose 90% orange tabby cats male. Determine following statements true false, explain reasoning.\n. distribution sample proportions random samples size 30 left skewed.\nb. Using sample size 4 times large reduce standard error sample proportion one-half.\nc. distribution sample proportions random samples size 140 approximately normal.Working backwardsA 90% confidence interval population mean (65,77). population distribution approximately normal population standard deviation unknown. confidence interval based simple random sample 25 observations. Calculate sample mean, margin error, sample standard deviation.Find p-valueAn independent random sample selected approximately normal population unknown standard deviation. Find p-value given set hypotheses \\(T\\) test statistic. Also determine null hypothesis rejected \\(\\alpha = 0.05\\).\\(H_{}: \\mu > \\mu_{0}\\), \\(n = 11\\), \\(T = 1.91\\)\\(H_{}: \\mu < \\mu_{0}\\), \\(n = 17\\), \\(T = - 3.45\\)\\(H_{}: \\mu \\ne \\mu_{0}\\), \\(n = 7\\), \\(T = 0.83\\)\\(H_{}: \\mu > \\mu_{0}\\), \\(n = 28\\), \\(T = 2.13\\)Sleep habits New YorkersNew York known “city never sleeps”. random sample 25 New Yorkers asked much sleep get per night. Statistical summaries data shown . data provide strong evidence New Yorkers sleep less 8 hours night average?\\[\n\\begin{array}{ccccc} & & &\\\\\n\\hline\nn   & \\bar{x}   & s     & min   & max \\\\\n\\hline\n25  & 7.73      & 0.77  & 6.17  & 9.78 \\\\\n  \\hline\n\\end{array}\n\\]Write hypotheses symbols words.Check conditions, calculate test statistic, \\(T\\), associated degrees freedom.Find interpret p-value context.conclusion hypothesis test?Construct 95% confidence interval corresponded hypothesis test, expect 8 hours interval?Vegetarian college students IIFrom problem 2 part c, suppose reported 8% college students vegetarians. think USAFA typical fitness health awareness, think vegetarians. collect random sample 125 cadets find 12% claimed vegetarians. enough evidence claim USAFA cadets different?Use binom.test() conduct hypothesis test find confidence interval.Use prop.test() correct=FALSE conduct hypothesis test find confidence interval.Use prop.test() correct=TRUE conduct hypothesis test find confidence interval.test use?","code":""},{"path":"CI.html","id":"add-two-proportions","chapter":"24 Confidence Intervals","heading":"24.6.0.1 ADD TWO PROPORTIONS?","text":"","code":""},{"path":"BOOT.html","id":"BOOT","chapter":"25 Bootstrap","heading":"25 Bootstrap","text":"","code":""},{"path":"BOOT.html","id":"objectives-24","chapter":"25 Bootstrap","heading":"25.1 Objectives","text":"Use bootstrap estimate standard error, standard deviation, sample statistic.Use bootstrap estimate standard error, standard deviation, sample statistic.Using bootstrap methods, obtain interpret confidence interval unknown parameter, based random sample.Using bootstrap methods, obtain interpret confidence interval unknown parameter, based random sample.Describe advantages, disadvantages, assumptions behind using bootstrapping confidence intervals.Describe advantages, disadvantages, assumptions behind using bootstrapping confidence intervals.","code":""},{"path":"BOOT.html","id":"confidence-intervals","chapter":"25 Bootstrap","heading":"25.2 Confidence intervals","text":"last chapter, introduced concept confidence intervals. reminder, confidence intervals used describe uncertainty around estimate parameter. confidence interval can interpreted range feasible values unknown parameter, given representative sample population.Recall four general steps building confidence interval:Identify parameter like estimate.Identify parameter like estimate.Identify good estimate parameter.Identify good estimate parameter.Determine distribution estimate function estimate.Determine distribution estimate function estimate.Use distribution obtain range feasible values (confidence interval) parameter.Use distribution obtain range feasible values (confidence interval) parameter.previously used central limit theorem determine distribution estimate. lesson, build bootstrap distributions sample estimates.","code":""},{"path":"BOOT.html","id":"bootstrapping","chapter":"25 Bootstrap","heading":"25.3 Bootstrapping","text":"","code":""},{"path":"BOOT.html","id":"stress-that-we-are-estimating-the-standard-error-not-getting-a-p-value","chapter":"25 Bootstrap","heading":"25.3.0.1 STRESS THAT WE ARE ESTIMATING THE STANDARD ERROR, NOT GETTING A P-VALUE","text":"many contexts, sampling distribution sample statistic either unknown subject assumptions. example, suppose wanted obtain 95% confidence interval median population. central limit theorem apply median; don’t know distribution.theory required quantify uncertainty sample median complex. ideal world, sample data population recompute median new sample. . . get enough median estimates good sense precision original estimate. ideal world sampling data free extremely cheap. rarely case, poses challenge “resample population” approach.However, can sample sample. Bootstrapping allows us simulate sampling distribution resampling sample. Suppose \\(x_1,x_2,...,x_n\\) ..d. random sample population. First define empirical distribution function \\(X\\) assigning equal probability \\(x_i\\). , sample empirical probability mass function. practice, simply means sampling original sample replacement. Thus treating sample discrete uniform random variable.general procedure bootstrapping sample replacement original sample, calculate record sample statistic bootstrapped sample, repeat process many times. collection sample statistics comprises bootstrap distribution sample statistic. Generally, procedure works quite well, provided sample representative population. Otherwise, bias misrepresentation simply amplified throughout bootstrap process. , small sample sizes, bootstrap distributions become “choppy” hard interpret. Thus small sample cases, must use permutation mathematical methods determine sampling distribution.completed procedure, bootstrap distribution can used build confidence interval population parameter estimate standard error. using bootstrap find p-values.","code":""},{"path":"BOOT.html","id":"bootstrap-example","chapter":"25 Bootstrap","heading":"25.4 Bootstrap example","text":"help us understand bootstrap, let’s use example single mean. like estimate mean height students local college. collect sample size 50 (stored vector heights ).Exercise\nUsing traditional method, via CLT t-distribution, bootstrap method, find 95% confidence intervals \\(\\mu\\). Compare two intervals.Let’s look data; Figures 25.1 25.2.\nFigure 25.1: Boxplot heights local college students.\n\nFigure 25.2: Density plot heights local college students.\nlooks bimodal since probably men women sample thus two different population distributions heights.","code":"\nheights<-c(62.0,73.8,59.8,66.9,75.6,63.3,64.0,63.1,65.0,67.2,73.0,\n     62.3,60.8,65.7,60.8,65.8,63.3,54.9,67.8,65.1,74.8,75.0,\n     77.8,73.7,74.3,68.4,77.5,77.9,66.5,65.5,71.7,75.9,81.7,\n     76.5,77.8,75.0,64.6,59.4,60.7,69.2,78.2,65.7,69.6,80.0,\n     67.6,73.0,65.3,67.6,66.2,69.6)\ngf_boxplot(~heights) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(y=\"Heights (in)\")\ngf_density(~heights,fill=\"lightgrey\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Heights (in)\",y=\"\")\nfavstats(~heights)##   min   Q1 median     Q3  max   mean       sd  n missing\n##  54.9 64.7   67.6 74.675 81.7 68.938 6.345588 50       0"},{"path":"BOOT.html","id":"using-traditional-mathematical-methods","chapter":"25 Bootstrap","heading":"25.4.1 Using traditional mathematical methods","text":"data comes less 10% population feel good assumption independence. However, data bimodal clearly come normal distribution. sample size larger, may help us. use t-distribution compare answer CLT compare bootstrap.can also calculate hand.want use tidyverse, must convert dataframe.Using CLT haveThis much different results using \\(t\\) distribution.","code":"\nconfint(t_test(~heights))##   mean of x   lower   upper level\n## 1    68.938 67.1346 70.7414  0.95\n## Using t\nxbar<-mean(heights)\nsd<-sd(heights)\nn<-length(heights)\ntval<-qt(0.975,n-1)\nxbar+c(-1,1)*tval*sd/sqrt(n)## [1] 67.1346 70.7414\nheights <- tibble(height=heights)\nhead(heights)## # A tibble: 6 x 1\n##   height\n##    <dbl>\n## 1   62  \n## 2   73.8\n## 3   59.8\n## 4   66.9\n## 5   75.6\n## 6   63.3\nheights %>%\n  summarize(mean=mean(height),stand_dev=sd(height),n=n(),\n            ci=mean+c(-1,1)*qt(0.975,n-1)*stand_dev/sqrt(n))## # A tibble: 2 x 4\n##    mean stand_dev     n    ci\n##   <dbl>     <dbl> <int> <dbl>\n## 1  68.9      6.35    50  67.1\n## 2  68.9      6.35    50  70.7\nheights %>%\n  summarize(mean=mean(height),stand_dev=sd(height),n=n(),\n            ci=mean+c(-1,1)*qnorm(0.975)*stand_dev/sqrt(n))## # A tibble: 2 x 4\n##    mean stand_dev     n    ci\n##   <dbl>     <dbl> <int> <dbl>\n## 1  68.9      6.35    50  67.2\n## 2  68.9      6.35    50  70.7"},{"path":"BOOT.html","id":"bootstrap","chapter":"25 Bootstrap","heading":"25.4.2 Bootstrap","text":"idea behind bootstrap get estimate distribution statistic interest sampling original data replacement. must sample regime original data collected. R, use resample() function mosaic package. entire packages dedicated resampling boot great deal information types packages online.applied dataframe, resample() function samples rows replacement produce new data\nframe number rows original, rows duplicated others missing.illustrate, let’s use resample() first 10 positive integers.Notice 8, 4 2 appeared least twice. number 3 appear. single bootstrap replicate data.calculate point estimate bootstrap replicate. repeat process large number times, 1000 maybe even 10000. collection point estimates called bootstrap distribution. sample mean, ideally, bootstrap distribution unimodal, roughly symmetric, centered original estimate.go problem.first rows results :() function default gives column name last function used, case mean. unfortunate name can cause us confusion.Figure 25.3 plot bootstrap sampling distribution.\nFigure 25.3: sampling distribution approximated using bootstrap distribution.\nsummary bootstrap distribution:Now two ways go calculate confidence interval. first called percentile method go bootstrap distribution find appropriate quantiles. second call t interval bootstrap error. second method construct confidence interval like using CLT except use bootstrap estimate standard error.","code":"\nset.seed(305)\nresample(1:10)##  [1] 8 7 8 1 4 4 2 2 6 9\nset.seed(2115)\nboot_results<-do(1000)*mean(~height,data=resample(heights))\nhead(boot_results)##     mean\n## 1 68.390\n## 2 68.048\n## 3 67.732\n## 4 68.534\n## 5 70.980\n## 6 68.424\nboot_results %>%\n  gf_histogram(~mean,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept = 68.938) %>%\n  gf_theme(theme_classic) %>%\n  gf_labs(x=\"Sample mean\")\nfavstats(~mean,data=boot_results)##     min      Q1 median    Q3  max     mean        sd    n missing\n##  65.684 68.3915 68.976 69.55 72.3 68.96724 0.9040555 1000       0"},{"path":"BOOT.html","id":"bootstrap-percentile","chapter":"25 Bootstrap","heading":"25.4.2.1 Bootstrap percentile","text":"function cdata() makes easy us.can use qdata().","code":"\ncdata(~mean,data=boot_results,p=0.95)##        lower   upper central.p\n## 2.5% 67.2197 70.7964      0.95\nqdata(~mean,data=boot_results,p=c(0.025,0.975))##    2.5%   97.5% \n## 67.2197 70.7964"},{"path":"BOOT.html","id":"add-to-ntis-not-book---could-stop-after-percentile-based-method-but-this-is-more-accurate-see-hesterberg-etc.-for-more-details","chapter":"25 Bootstrap","heading":"25.4.2.2 ADD TO NTIs (NOT BOOK?) - COULD STOP AFTER PERCENTILE BASED METHOD BUT THIS IS MORE ACCURATE, SEE HESTERBERG, ETC. FOR MORE DETAILS","text":"","code":""},{"path":"BOOT.html","id":"t-interval-with-bootstrap-standard-error","chapter":"25 Bootstrap","heading":"25.4.2.3 t interval with bootstrap standard error","text":"Since bootstrap distribution looks like \\(t\\) distribution, can use \\(t\\) interval bootstrap standard error. standard deviation bootstrap distribution standard error sample mean. divide \\(\\sqrt{n}\\) since dealing distribution mean directly.course use tidyverse must change column name.course function make easier us.three intervals similar.","code":"\nxbar<-mean(boot_results$mean)\nSE<-sd(boot_results$mean)\nxbar+c(-1,1)*qt(.975,49)*SE## [1] 67.15047 70.78401\nboot_results %>%\n  mutate(stat=mean) %>%\n  summarise(mean=mean(stat),stand_dev=sd(stat),ci=mean+c(-1,1)*qt(0.975,49)*stand_dev)##       mean stand_dev       ci\n## 1 68.96724 0.9040555 67.15047\n## 2 68.96724 0.9040555 70.78401\nconfint(boot_results, method = c(\"percentile\", \"stderr\"))##   name    lower    upper level     method estimate margin.of.error df\n## 1 mean 67.21970 70.79640  0.95 percentile   68.938              NA NA\n## 2 mean 67.15047 70.78401  0.95     stderr   68.938        1.816768 49"},{"path":"BOOT.html","id":"non-standard-sample-statistics","chapter":"25 Bootstrap","heading":"25.5 Non-standard sample statistics","text":"One huge advantages simulation-based methods ability build confidence intervals parameters whose estimates don’t known sampling distributions distributions difficult derive.","code":""},{"path":"BOOT.html","id":"example-median","chapter":"25 Bootstrap","heading":"25.5.1 Example median","text":"Consider height data , like know median student height use confidence interval estimate. However, idea sampling distribution median. can use bootstrapping obtain empirical distribution median.Exercise:\nFind 90% confidence interval median height students local college.\nFigure 25.4: sampling distribution approximated using bootstrap distribution.\nFigure 25.4, bootstrap sampling distribution symmetrical may want use t interval approach. still calculate confidence interval based approaches compare results.little difference two methods large may expected.","code":"\nset.seed(427)\nboot_results<-do(10000)*median(~height,data=resample(heights))\nboot_results %>%\n  gf_histogram(~median,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept = 67.6) %>%\n  gf_theme(theme_classic) %>%\n  gf_labs(x=\"Sample median\")\ncdata(~median,data=boot_results,p=0.90)##    lower upper central.p\n## 5%  65.8 70.65       0.9\nconfint(boot_results, method = c(\"percentile\", \"stderr\"),level=0.9)##     name   lower   upper level     method estimate margin.of.error\n## 1 median 65.8000 70.6500   0.9 percentile     67.6              NA\n## 2 median 65.4648 70.1297   0.9     stderr     67.6        2.332455"},{"path":"BOOT.html","id":"summary-bootstrap","chapter":"25 Bootstrap","heading":"25.5.2 Summary bootstrap","text":"key idea behind bootstrap estimate population sample, called plug principle, something unknown substitute estimate . can generate new samples population estimate. bootstrap improve accuracy original estimate, fact bootstrap distribution centered original sample estimate. Instead get information variability sample estimate. people suspicious using data . remember just getting estimates variability. traditional statistics, calculate sample standard deviation, using sample mean. Thus using data twice. Always think bootstrap providing way find variability estimate.","code":""},{"path":"BOOT.html","id":"rename---putting-it-all-together-or-culminating-example","chapter":"25 Bootstrap","heading":"25.5.2.1 RENAME - PUTTING IT ALL TOGETHER or CULMINATING EXAMPLE","text":"","code":""},{"path":"BOOT.html","id":"confidence-interval-for-difference-in-means","chapter":"25 Bootstrap","heading":"25.6 Confidence interval for difference in means","text":"bring ideas learned far block work example testing difference two means. opinion, easiest method understand permutation test difficult one based mathematical derivation, assumptions necessary get mathematical solution sampling distribution. also introduce use bootstrap get confidence interval.","code":""},{"path":"BOOT.html","id":"help-example","chapter":"25 Bootstrap","heading":"25.6.1 HELP example","text":"Let’s return Health Evaluation Linkage Primary Care data set, HELPrct mosaicData package. Previously, looked whether difference substance abuse males females.now interested whether difference male female ages.\nFigure 22.1: distribution age HELP study gender.\n\nFigure 22.2: distribution age HELP study gender.\nFigures 22.1 22.2 indicate might slight difference means, statistically significant?","code":"\ndata(\"HELPrct\")\nHELP_sub <- HELPrct %>%\n  select(age,sex)\nfavstats(age~sex,data=HELP_sub)##      sex min Q1 median   Q3 max     mean       sd   n missing\n## 1 female  21 31     35 40.5  58 36.25234 7.584858 107       0\n## 2   male  19 30     35 40.0  60 35.46821 7.750110 346       0\nHELP_sub %>%\n  gf_boxplot(age~sex) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Gender\",y=\"Age (years)\")\nHELP_sub %>%\n  gf_dhistogram(~age|sex,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Age\",y=\"\")"},{"path":"BOOT.html","id":"permutation-test-2","chapter":"25 Bootstrap","heading":"25.6.2 Permutation test","text":"permutation test ideally suited hypothesis test. conduct first see can generate confidence interval.hypotheses :\\(H_0\\): difference average age men women detoxification unit. statistical notation: \\(\\mu_{male} - \\mu_{female} = 0\\), \\(\\mu_{female}\\) represents female inpatients \\(\\mu_{male}\\) represents male inpatients.\\(H_A\\): difference average age men women detoxification unit (\\(\\mu_{male} - \\mu_{female} \\neq 0\\)).Let’s perform randomization, permutation, test.sampling distribution centered null value 0, less, standard deviation 0.849. estimate variability difference mean ages.\nFigure 25.5: approximate sampling distribution difference means bootstrap process.\ntest statistic appear extreme, Figure 25.5.Based p-value, fail reject null hypothesis.Now construct confidence interval careful think . object results distribution difference means assuming difference. get confidence interval, want center difference observed difference means zero.95% confident true difference mean ages female male participants study -2.45 0.88. Since 0 confidence interval, fail reject null hypothesis.assuming test statistic can transformed. turns percentile method transformation invariant can transform shifting null distribution observed value.","code":"\nfavstats(age~sex,data=HELP_sub)##      sex min Q1 median   Q3 max     mean       sd   n missing\n## 1 female  21 31     35 40.5  58 36.25234 7.584858 107       0\n## 2   male  19 30     35 40.0  60 35.46821 7.750110 346       0\nobs_stat<-diffmean(age~sex,data=HELP_sub)\nobs_stat##   diffmean \n## -0.7841284\nset.seed(345)\nresults <- do(10000)*diffmean(age~shuffle(sex),data=HELP_sub)\nfavstats(~diffmean,data=results)##        min         Q1     median     Q3      max        mean        sd     n\n##  -3.378154 -0.5638809 0.01120955 0.5863 3.486224 0.009350908 0.8492454 10000\n##  missing\n##        0\nresults %>%\n  gf_histogram(~diffmean,color=\"black\",fill=\"cyan\") %>%\n  gf_vline(xintercept=obs_stat,color=\"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Difference of means\",title=\"Sampling distribution of difference of two means\",\n  subtitle=\"Null assumes equal means\")\n2*prop1(~(diffmean <= obs_stat),data=results)## prop_TRUE \n## 0.3523648\ncdata(~(diffmean+obs_stat),data=results)##          lower     upper central.p\n## 2.5% -2.449246 0.8789368      0.95"},{"path":"BOOT.html","id":"traditional-mathematical-methods","chapter":"25 Bootstrap","heading":"25.6.3 Traditional mathematical methods","text":"Using CLT \\(t\\) distribution becomes difficult find way calculate standard error. many proposed methods, welcome research , present couple ideas section. Let’s summarize process hypothesis testing confidence intervals case difference two means using \\(t\\) distribution.","code":""},{"path":"BOOT.html","id":"hypothesis-tests","chapter":"25 Bootstrap","heading":"25.6.4 Hypothesis tests","text":"applying \\(t\\) distribution hypothesis test, proceed follows:Write appropriate hypotheses.Write appropriate hypotheses.Verify conditions using \\(t\\) distribution.\ndifference means data paired: sample mean must separately satisfy one-sample conditions \\(t\\) distribution, data group must also independent. Just like one-sample case, slight skewness problem larger sample sizes. can moderate skewness fine sample 30 . can extreme skewness sample 60 .Verify conditions using \\(t\\) distribution.difference means data paired: sample mean must separately satisfy one-sample conditions \\(t\\) distribution, data group must also independent. Just like one-sample case, slight skewness problem larger sample sizes. can moderate skewness fine sample 30 . can extreme skewness sample 60 .Compute point estimate interest, standard error, degrees freedom.Compute point estimate interest, standard error, degrees freedom.Compute T score p-value.Compute T score p-value.Make conclusion based p-value, write conclusion context plain language anyone can understand result.Make conclusion based p-value, write conclusion context plain language anyone can understand result.added extra step checking assumptions.","code":""},{"path":"BOOT.html","id":"confidence-intervals-1","chapter":"25 Bootstrap","heading":"25.6.5 Confidence intervals","text":"Similarly, following generally computed confidence interval using \\(t\\) distribution:Verify conditions using \\(t\\) distribution. (See .)Verify conditions using \\(t\\) distribution. (See .)Compute point estimate interest, standard error, degrees freedom, \\(t^{\\star}_{df}\\).Compute point estimate interest, standard error, degrees freedom, \\(t^{\\star}_{df}\\).Calculate confidence interval using general formula, point estimate \\(\\pm\\ t_{df}^{\\star} SE\\).Calculate confidence interval using general formula, point estimate \\(\\pm\\ t_{df}^{\\star} SE\\).Put conclusions context plain language even non-statisticians can understand results.Put conclusions context plain language even non-statisticians can understand results.assumptions met, sample mean can modeled using \\(t\\) distribution samples independent, sample difference two means, \\(\\bar{x}_1 - \\bar{x}_2\\), can modeled using \\(t\\) distribution standard error\n\\[SE_{\\bar{x}_{1} - \\bar{x}_{2}} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\]calculate degrees freedom, use statistical software conservatively use smaller \\(n_1 - 1\\) \\(n_2 - 1\\).","code":""},{"path":"BOOT.html","id":"results","chapter":"25 Bootstrap","heading":"25.6.5.1 Results","text":"Back study, men women independent . Additionally, distributions population don’t show clear deviations normality, slight skewness sample size reduces concern, Figure 25.6. Finally, within group also need independence. represent less 10% population, good go . condition might difficult verify.\nFigure 25.6: quantile-quantile plots check normality assumption.\ndistribution males tends longer tails normal female distribution skewed right. sample sizes large enough worry us.Let’s find confidence interval first.result close got permutation test.Now let’s find p-value hypothesis test.test statistic :\n\\[T = \\frac{\\text{point estimate} - \\text{null value}}{SE}\\]\\[ = \\frac{(35.47 - 36.25) - 0}{\\sqrt{\\left( \\frac{7.58^2}{107}+ \\frac{7.75^2}{346}\\right)}} = - 0.92976 \\]\nuse smaller \\(n_1-1\\) \\(n_2-1\\) degrees freedom: \\(df=106\\).p-value :course, function us.Notice degrees freedom integer, weighted average two different samples sizes standard deviations. method called Satterwaite approximation.","code":"\nHELP_sub %>%\n  gf_qq(~age|sex) %>%\n  gf_qqline(~age|sex) %>%\n  gf_theme(theme_bw())\nfavstats(age~sex,data=HELP_sub)##      sex min Q1 median   Q3 max     mean       sd   n missing\n## 1 female  21 31     35 40.5  58 36.25234 7.584858 107       0\n## 2   male  19 30     35 40.0  60 35.46821 7.750110 346       0\n(35.47-36.25)+c(-1,1)*qt(.975,106)*sqrt(7.58^2/107+7.75^2/346)## [1] -2.4512328  0.8912328\n2*pt(-0.92976,106)## [1] 0.3546079\nt_test(age~sex,data=HELP_sub)## \n##  Welch Two Sample t-test\n## \n## data:  age by sex\n## t = 0.92976, df = 179.74, p-value = 0.3537\n## alternative hypothesis: true difference in means between group female and group male is not equal to 0\n## 95 percent confidence interval:\n##  -0.8800365  2.4482932\n## sample estimates:\n## mean in group female   mean in group male \n##             36.25234             35.46821"},{"path":"BOOT.html","id":"pooled-standard-deviation","chapter":"25 Bootstrap","heading":"25.6.5.2 Pooled standard deviation","text":"Occasionally, two populations standard deviations similar can treated identical. assumption equal variance group. example, historical data well-understood biological mechanism may justify strong assumption. cases, can make \\(t\\) distribution approach slightly precise using pooled standard deviation.pooled standard deviation two groups way use data samples better estimate standard deviation standard error. \\(s_1^{}\\) \\(s_2^{}\\) standard deviations groups 1 2 good reasons believe population standard deviations equal, can obtain improved estimate group variances pooling data:\\[ s_{pooled}^2 = \\frac{s_1^2\\times (n_1-1) + s_2^2\\times (n_2-1)}{n_1 + n_2 - 2}\\]\\(n_1\\) \\(n_2\\) sample sizes, . use new statistic, substitute \\(s_{pooled}^2\\) place \\(s_1^2\\) \\(s_2^2\\) standard error formula, use updated formula degrees freedom:\n\\[df = n_1 + n_2 - 2\\]benefits pooling standard deviation realized obtaining better estimate standard deviation group using larger degrees freedom parameter \\(t\\) distribution. changes may permit accurate model sampling distribution \\(\\bar{x}_1 - \\bar{x}_2\\).Caution\nPooling standard deviations done careful researchA pooled standard deviation appropriate background research indicates population standard deviations nearly equal. sample size large condition may adequately checked data, benefits pooling standard deviations greatly diminishes.R can difference two means equal variance using var.equal.Since sample sizes large, big impact results.","code":"\nt_test(age~sex,data=HELP_sub,var.equal=TRUE)## \n##  Two Sample t-test\n## \n## data:  age by sex\n## t = 0.91923, df = 451, p-value = 0.3585\n## alternative hypothesis: true difference in means between group female and group male is not equal to 0\n## 95 percent confidence interval:\n##  -0.8922735  2.4605303\n## sample estimates:\n## mean in group female   mean in group male \n##             36.25234             35.46821"},{"path":"BOOT.html","id":"bootstrap-1","chapter":"25 Bootstrap","heading":"25.6.6 Bootstrap","text":"Finally, construct confidence interval use bootstrap distribution. problem careful sample within group. Compare following two sets samples.andNotice second line code, keeping samples size within sex variable.Let’s get bootstrap distribution.Figure 25.7 sampling distribution bootstrap.\nFigure 25.7: Sampling distribution difference means.\n, similar results.","code":"\nfavstats(age ~ sex, data = resample(HELP_sub))##      sex min Q1 median    Q3 max     mean       sd   n missing\n## 1 female  21 30     33 38.75  50 34.64706 6.267387 102       0\n## 2   male  19 29     33 39.00  59 34.74359 7.833066 351       0\nfavstats(age ~ sex, data = resample(HELP_sub,groups=sex))##      sex min Q1 median   Q3 max     mean       sd   n missing\n## 1 female  22 31     34 39.5  57 35.60748 6.901951 107       0\n## 2   male  20 31     35 41.0  60 35.94798 8.039227 346       0\nset.seed(2527)\nresults <- do(1000) * diffmean(age ~ sex, data = resample(HELP_sub, groups = sex))\nresults %>%\n  gf_histogram(~diffmean,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic) %>%\n  gf_labs(x=\"Difference in means\",y=\"\")\ncdata( ~ diffmean, p = 0.95, data = results)##          lower     upper central.p\n## 2.5% -2.394406 0.8563786      0.95"},{"path":"BOOT.html","id":"frequently-asked-questions","chapter":"25 Bootstrap","heading":"25.7 Frequently asked questions","text":"types bootstrap techniques, right?\nYes! many excellent bootstrap techniques. chosen present two bootstrap techniques explained single lesson also reasonably reliable. many adjustments can made speed improve accuracy. Packages resample boot appropriate situations.types bootstrap techniques, right?Yes! many excellent bootstrap techniques. chosen present two bootstrap techniques explained single lesson also reasonably reliable. many adjustments can made speed improve accuracy. Packages resample boot appropriate situations.’ve heard percentile bootstrap robust.\ncommonly held belief percentile bootstrap robust bootstrap method. false. percentile method one least reliable bootstrap methods. However, easy use understand can give first attempt solution accurate methods used.’ve heard percentile bootstrap robust.commonly held belief percentile bootstrap robust bootstrap method. false. percentile method one least reliable bootstrap methods. However, easy use understand can give first attempt solution accurate methods used.use 1000 replicates bootstrap permutation tests.\nrandomization bootstrap distributions involve random component sampling process thus p-values confidence intervals computed data vary. amount Monte Carlo variability depends number replicates used create randomization bootstrap distribution. important use introduce much random noise p-value confidence interval calculations. replicate costs time, marginal gain additional replicate decreases number replicates increases. little reason use millions replicates (unless goal estimate\nsmall p-values). generally use roughly 1000 routine preliminary work increase 10,000\nwant reduce effects Monte Carlo variability.use 1000 replicates bootstrap permutation tests.randomization bootstrap distributions involve random component sampling process thus p-values confidence intervals computed data vary. amount Monte Carlo variability depends number replicates used create randomization bootstrap distribution. important use introduce much random noise p-value confidence interval calculations. replicate costs time, marginal gain additional replicate decreases number replicates increases. little reason use millions replicates (unless goal estimate\nsmall p-values). generally use roughly 1000 routine preliminary work increase 10,000\nwant reduce effects Monte Carlo variability.","code":""},{"path":"BOOT.html","id":"homework-problems-24","chapter":"25 Bootstrap","heading":"25.8 Homework Problems","text":"Poker\naspiring poker player recorded winnings losses 50 evenings play, data data folder file poker.csv. poker player like better understand volatility long term play.Load data plot histogram.Load data plot histogram.Find summary statistics.Find summary statistics.Mean absolute deviation MAD intuitive measure spread variance. directly measures average distance mean. found formula:Mean absolute deviation MAD intuitive measure spread variance. directly measures average distance mean. found formula:\\[mad = \\sum_{=1}^{n}\\frac{\\left| x_{} - \\bar{x} \\right|}{n}\\]Find bootstrap distribution MAD using 1000 replicates.Find bootstrap distribution MAD using 1000 replicates.Plot histogram bootstrap distribution.Plot histogram bootstrap distribution.Report 95% confidence interval MAD.Report 95% confidence interval MAD.ADVANCED: think sample MAD unbiased estimator population MAD? ?ADVANCED: think sample MAD unbiased estimator population MAD? ?Bootstrap hypothesis testingBootstrap hypothesis testing relatively undeveloped, generally accurate permutation testing. Therefore general avoid . problem reading , may work. sample way consistent null hypothesis, calculate p-value tail probability like permutation tests. example generalize well applications like relative risk, correlation, regression, categorical data.Using HELPrct data, null hypothesis requires means group equal. Pick one group adjust, either male female. First zero mean selected group subtracting sample mean group data points group. add sample mean group data point selected group. Store new object called HELP_null. set, store observed value difference means male female.Using HELPrct data, null hypothesis requires means group equal. Pick one group adjust, either male female. First zero mean selected group subtracting sample mean group data points group. add sample mean group data point selected group. Store new object called HELP_null. set, store observed value difference means male female.null hypothesis requires means group equal. Pick one group adjust, either male female. First zero mean selected group subtracting sample mean group data points group. add sample mean group data point selected group. Store new object called HELP_null.null hypothesis requires means group equal. Pick one group adjust, either male female. First zero mean selected group subtracting sample mean group data points group. add sample mean group data point selected group. Store new object called HELP_null.Run favstats() check means equal.Run favstats() check means equal.new adjusted data set, generate bootstrap distribution difference sample means.new adjusted data set, generate bootstrap distribution difference sample means.Plot bootstrap distribution line observed difference sample means.Plot bootstrap distribution line observed difference sample means.Find p-value.Find p-value.p-value compare reading.p-value compare reading.Bootstrap hypothesis testing\nRepeat analysis MLB data reading time generate bootstrap distribution \\(F\\) statistic.Bootstrap hypothesis testingRepeat analysis MLB data reading time generate bootstrap distribution \\(F\\) statistic.Paired dataPaired dataAre textbooks actually cheaper online? compare price textbooks University California, Los Angeles (UCLA) bookstore Amazon.com. Seventy-three UCLA courses randomly sampled Spring 2010, representing less 10% UCLA courses. class multiple books, expensive text considered.data file textbooks.csv data folder.textbook two corresponding prices data set: one UCLA bookstore one Amazon. Therefore, textbook price UCLA bookstore natural correspondence textbook price Amazon. two sets observations special correspondence, said paired.analyze paired data, often useful look difference outcomes pair observations. textbooks, look difference prices, represented diff variable. important always subtract using consistent order; Amazon prices always subtracted UCLA prices.data tidy? Explain.data tidy? Explain.Make scatterplot UCLA price versus Amazon price. Add 45 degree line plot.Make scatterplot UCLA price versus Amazon price. Add 45 degree line plot.Make histogram differences price.\nhypotheses :\\(H_0\\): \\(\\mu_{diff}=0\\). difference average textbook price.\\(H_A\\): \\(\\mu_{diff} \\neq 0\\). difference average prices.Make histogram differences price.hypotheses :\\(H_0\\): \\(\\mu_{diff}=0\\). difference average textbook price.\\(H_A\\): \\(\\mu_{diff} \\neq 0\\). difference average prices.use \\(t\\) distribution, variable diff independent normally distributed. Since 73 books represent less 10% population, assumption random sample independent reasonable. Check normality using qqnorsim() openintro package. generates 8 qq plots simulated normal data can use judge diff variable.use \\(t\\) distribution, variable diff independent normally distributed. Since 73 books represent less 10% population, assumption random sample independent reasonable. Check normality using qqnorsim() openintro package. generates 8 qq plots simulated normal data can use judge diff variable.Run \\(t\\) test diff variable. Report p-value conclusion.Run \\(t\\) test diff variable. Report p-value conclusion.Create bootstrap distribution generate 95% confidence interval mean differences, diff column.Create bootstrap distribution generate 95% confidence interval mean differences, diff column.","code":"Write a function and find the *MAD* of the data.  "},{"path":"CS4.html","id":"CS4","chapter":"26 Case Study","heading":"26 Case Study","text":"","code":""},{"path":"CS4.html","id":"objectives-25","chapter":"26 Case Study","heading":"26.1 Objectives","text":"Using R, generate linear regression model use produce prediction model.Using plots, check assumptions linear regression model.","code":""},{"path":"CS4.html","id":"introduction-to-linear-regression","chapter":"26 Case Study","heading":"26.2 Introduction to linear regression","text":"Linear regression often one first methods taught machine learning course. excellent tool wide range applications. can used solely predict outcome interest, prediction model, /used inference. book, mainly focus use inference. Even , treatment barely scratches surface can done. entire courses devoted interpretation linear regression models.used predictive model, linear regression fits category function approximation method. parameters model, function, fit using objective, loss, function optimization procedure. linear regression book, loss function sum squared errors leads closed form solution using differentiation. machine learning courses learn different types loss functions include penalized regularized versions well different optimization engines. software tidymodels R scitkit-learn python, specify loss function optimization engine directly.","code":""},{"path":"CS4.html","id":"case-study-introduction","chapter":"26 Case Study","heading":"26.3 Case study introduction","text":"Human Freedom Index report attempts summarize idea “freedom” bunch different variables many countries around globe. serves rough objective measure relationships different types freedom - whether ’s political, religious, economical personal freedom - social economic circumstances. Human Freedom Index annually co-published report Cato Institute, Fraser Institute, Liberales Institut Friedrich Naumann Foundation Freedom.case study, ’ll analyzing data Human Freedom Index reports 2008-2016. aim summarize relationships within data graphically numerically order find variables can help tell story freedom. done using tool regression., like previous case studies, chapter introduce many ideas block. Don’t worry seem difficult feel overwhelmed bit, come back ideas following chapters.","code":""},{"path":"CS4.html","id":"load-packages","chapter":"26 Case Study","heading":"26.3.1 Load packages","text":"Let’s load packages.","code":"\nlibrary(tidyverse)\nlibrary(mosaic)"},{"path":"CS4.html","id":"the-data-and-exploratory-analysis","chapter":"26 Case Study","heading":"26.3.2 The data and exploratory analysis","text":"data ’re working file called hfi.csv data folder. name hfi \nshort Human Freedom Index.Exercise\nRead data R. dimensions dataset?1458 observations 123 variables data frame.Exercise\nCreate summaries first 10 variables data. just don’t want large printout.Exercise\nCreate scatter plot display relationship personal freedom score, pf_score, response pf_expression_control predictor. relationship look linear?\nFigure 26.1: scatterplot personal freedom versus expression control using ggformula package.\n\nFigure 26.2: scatterplot personal freedom versus expression control using ggplot2 package.\nFigures 26.1 26.2 scatterplots, included demonstrate ggformula ggplot2 packages. figures, relationship look linear. Although, uncomfortable using model end points. less points edge linear estimation larger variance endpoints, predictions endpoints suspect.Exercise\nrelationship looks linear, quantify strength \nrelationship correlation coefficient.sample correlation coefficient, indicates strong positive linear relationship variables.Note set use argument “complete.obs” since observations missing values, NA.","code":"\nhfi<-tibble(read_csv(\"data/hfi.csv\"))\ndim(hfi)## [1] 1458  123\ninspect(hfi[,1:10])## \n## categorical variables:  \n##        name     class levels    n missing\n## 1  ISO_code character    162 1458       0\n## 2 countries character    162 1458       0\n## 3    region character     10 1458       0\n##                                    distribution\n## 1 AGO (0.6%), ALB (0.6%), ARE (0.6%) ...       \n## 2 Albania (0.6%), Algeria (0.6%) ...           \n## 3 Sub-Saharan Africa (25.9%) ...               \n## \n## quantitative variables:  \n##                            name   class  min          Q1      median\n## ...1                       year numeric 2008 2010.000000 2012.000000\n## ...2          pf_rol_procedural numeric    0    4.133333    5.300000\n## ...3               pf_rol_civil numeric    0    4.549550    5.300000\n## ...4            pf_rol_criminal numeric    0    3.789724    4.575189\n## ...5                     pf_rol numeric    0    4.131746    4.910797\n## ...6             pf_ss_homicide numeric    0    6.386978    8.638278\n## ...7 pf_ss_disappearances_disap numeric    0   10.000000   10.000000\n##               Q3         max        mean       sd    n missing\n## ...1 2014.000000 2016.000000 2012.000000 2.582875 1458       0\n## ...2    7.389499    9.700000    5.589355 2.080957  880     578\n## ...3    6.410975    8.773533    5.474770 1.428494  880     578\n## ...4    6.400000    8.719848    5.044070 1.724886  880     578\n## ...5    6.513178    8.723094    5.309641 1.529310 1378      80\n## ...6    9.454402    9.926568    7.412980 2.832947 1378      80\n## ...7   10.000000   10.000000    8.341855 3.225902 1369      89\ngf_lm(pf_score~pf_expression_control,data=hfi,color=\"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_point(alpha=0.3) %>%\n  gf_labs(title=\"Personal freedom score versus Expression control\",\n          x=\"Political pressures and controls on media\",\n          y=\"Personal freedom score\")\nggplot(hfi,aes(x=pf_expression_control,y=pf_score))+\n  geom_point(alpha=0.3) +\n  theme_bw()+\n  geom_lm(color=\"black\")+\n  labs(title=\"Personal freedom score versus Expression control\",\n          x=\"Political pressures and controls on media\",\n          y=\"Personal freedom score\")\nhfi %>%\n  summarise(cor(pf_expression_control, pf_score, use = \"complete.obs\"))## # A tibble: 1 x 1\n##   `cor(pf_expression_control, pf_score, use = \"complete.obs\")`\n##                                                          <dbl>\n## 1                                                        0.796"},{"path":"CS4.html","id":"sum-of-squared-residuals","chapter":"26 Case Study","heading":"26.4 Sum of squared residuals","text":"section, use interactive function investigate mean “sum\nsquared residuals”. need run function console. Running function also requires hfi data set loaded environment, .Think back way described distribution single variable. Recall discussed characteristics center, spread, shape. ’s also useful able describe relationship two numerical variables, pf_expression_control pf_score .Exercise\nLooking scatterplot , describe relationship two variables. Make sure discuss form, direction, strength relationship well unusual observations.say strong positive linear relationship two variables.Just ’ve used mean standard deviation summarize single variable, can summarize relationship two variables finding \nline best follows association. Use following interactive function select line think best job going cloud points.First must remove missing values data set make visualization easier, just randomly sample 30 data points. included hf_score need later.used function slice_sample() ensure unique values pf_expression_control sample.Exercise\nR console, run code create object hfi_sub. going load packages read hfi data set. execute next lines code. Pick two locations plot create line. Record sum squares.First, run code chunk create function plot_ss() use next.Next run next code chunk us resulted Figure 26.3. pick points plot think gives best line.\nFigure 26.3: Plot selected line associated residuals.\n’ve picked two locations, line specified shown black residuals blue. Residuals difference observed values values predicted line:\\[\n  e_i = y_i - \\hat{y}_i\n\\]common way linear regression select line minimizes sum squared residuals. visualize squared residuals, can rerun\nplot command add argument showSquares = TRUE.Note output plot_ss function provides slope intercept line well sum squares.Exercise:\nUsing plot_ss, choose line good job minimizing sum squares. Run function several times. smallest sum squares got?","code":"\nset.seed(4011)\nhfi_sub <- hfi %>%\n  select(pf_expression_control,pf_score,hf_score) %>%\n  drop_na() %>%\n  group_by(pf_expression_control) %>%\n  slice_sample(n=1)\n# Function to create plot and residuals.\nplot_ss <- function (x, y, data, showSquares = FALSE, leastSquares = FALSE) \n{\n    x <- eval(substitute(x), data)\n    y <- eval(substitute(y), data)\n    plot(y ~ x, asp = 1, pch = 16)\n    if (leastSquares) {\n        m1 <- lm(y ~ x)\n        y.hat <- m1$fit\n    }\n    else {\n        cat(\"Click two points to make a line.\")\n        pt1 <- locator(1)\n        points(pt1$x, pt1$y, pch = 4)\n        pt2 <- locator(1)\n        points(pt2$x, pt2$y, pch = 4)\n        pts <- data.frame(x = c(pt1$x, pt2$x), y = c(pt1$y, pt2$y))\n        m1 <- lm(y ~ x, data = pts)\n        y.hat <- predict(m1, newdata = data.frame(x))\n    }\n    r <- y - y.hat\n    abline(m1)\n    oSide <- x - r\n    LLim <- par()$usr[1]\n    RLim <- par()$usr[2]\n    oSide[oSide < LLim | oSide > RLim] <- c(x + r)[oSide < LLim | \n        oSide > RLim]\n    n <- length(y.hat)\n    for (i in 1:n) {\n        lines(rep(x[i], 2), c(y[i], y.hat[i]), lty = 2, col = \"blue\")\n        if (showSquares) {\n            lines(rep(oSide[i], 2), c(y[i], y.hat[i]), lty = 3, \n                col = \"orange\")\n            lines(c(oSide[i], x[i]), rep(y.hat[i], 2), lty = 3, \n                col = \"orange\")\n            lines(c(oSide[i], x[i]), rep(y[i], 2), lty = 3, col = \"orange\")\n        }\n    }\n    SS <- round(sum(r^2), 3)\n    cat(\"\\r                                \")\n    print(m1)\n    cat(\"Sum of Squares: \", SS)\n}\nplot_ss(x = pf_expression_control, y = pf_score, data = hfi_sub)## Click two points to make a line.\n                                \n## Call:\n## lm(formula = y ~ x, data = pts)\n## \n## Coefficients:\n## (Intercept)            x  \n##      5.0272       0.4199  \n## \n## Sum of Squares:  19.121\nplot_ss(x = pf_expression_control, y = pf_score, data = hfi_sub, showSquares = TRUE)"},{"path":"CS4.html","id":"the-linear-model","chapter":"26 Case Study","heading":"26.5 The linear model","text":"rather cumbersome try get correct least squares line, .e. line minimizes sum squared residuals, trial error. Instead, can use lm() function R fit linear model (.k..\nregression line).first argument function lm formula takes form y ~ x. can read want make linear model pf_score function pf_expression_control. second argument specifies\nR look hfi_sub data frame find two variables. familiar us since used mosaic package.output lm object contains information need linear model just fit. can access information using summary() function.Let’s consider output piece piece. First, formula used describe model shown top. formula find five-number summary residuals. “Coefficients” table shown next key; first\ncolumn displays linear model’s y-intercept coefficient pf_expression_control. table, can write least squares regression line \nlinear model:\\[\n  \\hat{\\text{pf_score}} = 5.02721 + 0.41988 \\times \\text{pf_expression_control}\n\\]least values got machine using seed provided. may differ slightly. One last piece information discuss summary output Multiple R-squared, simply, \\(R^2\\). \\(R^2\\) value represents proportion variability response variable explained explanatory variable. model, 72.48% variability pf_score explained pr_expression_control.Exercise:\nFit new model uses pf_expression_control predict hf_score, total human freedom score. Using estimates R output, write equation regression line. slope tell us context relationship human freedom amount political pressure media content?\\[\n  \\hat{\\text{hf_score}} = 5.45660 + 0.30707 \\times \\text{pf_expression_control}\n\\]\npolitical pressure increases one unit, average human freedom score increases 0.307.","code":"\nm1 <- lm(pf_score ~ pf_expression_control, data = hfi_sub)\nsummary(m1)## \n## Call:\n## lm(formula = pf_score ~ pf_expression_control, data = hfi_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7559 -0.4512  0.1838  0.5369  1.2510 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)            5.02721    0.23186  21.682  < 2e-16 ***\n## pf_expression_control  0.41988    0.04312   9.736 1.26e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7288 on 36 degrees of freedom\n## Multiple R-squared:  0.7248, Adjusted R-squared:  0.7171 \n## F-statistic:  94.8 on 1 and 36 DF,  p-value: 1.262e-11\nm2<-lm(hf_score ~ pf_expression_control, data = hfi_sub)\nsummary(m2)## \n## Call:\n## lm(formula = hf_score ~ pf_expression_control, data = hfi_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.5151 -0.5776  0.2340  0.4622  1.0633 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)            5.45660    0.21585  25.279  < 2e-16 ***\n## pf_expression_control  0.30707    0.04015   7.649 4.72e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6785 on 36 degrees of freedom\n## Multiple R-squared:  0.6191, Adjusted R-squared:  0.6085 \n## F-statistic:  58.5 on 1 and 36 DF,  p-value: 4.718e-09"},{"path":"CS4.html","id":"prediction-and-prediction-errors","chapter":"26 Case Study","heading":"26.6 Prediction and prediction errors","text":"Let’s create scatterplot least squares line m1, first model, laid top, Figure 26.4.\nFigure 26.4: Scatterplot personal expression control personal freedom score.\n, literally adding layer top plot. stat_smooth() function creates line fitting linear model, use geom_lm() well. can also show us standard error se\nassociated line, ’ll suppress now.line can used predict \\(y\\) value \\(x\\). predictions made values \\(x\\) beyond range observed data, referred extrapolation usually recommended. However,\npredictions made within range data reliable. ’re also used compute residuals.Exercise:\nsomeone saw least squares regression line actual data, predict country’s personal freedom score one 6.75 rating pf_expression_control? overestimate underestimate, much? words, residual prediction?predict, use predict function, send new data data frame.thus predict value 7.86 average pf_score.observed value 8.628272.residual :underestimated actual value.Another way use broom package.","code":"\nggplot(data = hfi_sub, aes(x = pf_expression_control, y = pf_score)) +\n  geom_point() +\n#  geom_lm(color=\"black\") +\n  stat_smooth(method = \"lm\", se = FALSE,color=\"black\") +\n  theme_bw()+\n  labs(title=\"Personal freedom score versus Expression control\",\n          x=\"Political pressures and controls on media\",\n          y=\"Personal freedom score\")\npredict(m1,newdata=data.frame(pf_expression_control=6.75))##        1 \n## 7.861402\nhfi_sub %>%\n  filter(pf_expression_control==6.75)## # A tibble: 1 x 3\n## # Groups:   pf_expression_control [1]\n##   pf_expression_control pf_score hf_score\n##                   <dbl>    <dbl>    <dbl>\n## 1                  6.75     8.63     8.25\n8.628272 - 7.861402## [1] 0.76687\nlibrary(broom)\naugment(m1) %>%\n   filter(pf_expression_control==6.75)## # A tibble: 1 x 8\n##   pf_score pf_expression_control .fitted .resid   .hat .sigma .cooksd .std.resid\n##      <dbl>                 <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n## 1     8.63                  6.75    7.86  0.767 0.0421  0.727  0.0254       1.08"},{"path":"CS4.html","id":"model-diagnostics","chapter":"26 Case Study","heading":"26.7 Model diagnostics","text":"assess whether linear model reliable, need check forlinearity,independence,nearly normal residuals, andconstant variability.Linearity: already checked relationship pf_score pf_expression_control linear using scatterplot. also verify condition plot residuals vs. fitted (predicted) values, Figure 26.5.\nFigure 26.5: Scatterplot residuals fitted values used assess assumptions linearity constant variance.\nNotice m1 can also serve data set stored within fitted values (\\(\\hat{y}\\)) residuals. Also note ’re getting sophisticated code Figure 26.5. creating scatterplot first layer (first line code), overlay horizontal dashed line \\(y = 0\\) (help us check whether residuals distributed around 0), also rename axis labels informative add title.Exercise:\napparent pattern residuals plot? indicate linearity relationship two variables?width constant trend data. linearity assumption bad.Independence: difficult check residuals depends data collected.Nearly normal residuals: check condition, can look histogram, Figure 26.6.\nFigure 26.6: Histogram residuals linear regression model.\nnormal probability plot residuals, 26.7.\nFigure 26.7: quantile-quantile residual plot used assess normality assumption.\nNote syntax making normal probability plot bit different ’re used seeing: set sample equal residuals instead x, set statistical method qq, stands “quantile-quantile”,\nanother name commonly used normal probability plots.little difficult first understand qq plot indicated distribution skewed left. points indicate quantile sample, standardized mean zero standard deviation one, plotted quantiles standard normal. sample matched standard normal points align 45-degree line. plot, see theoretical quantile get larger, zero, sample . trajectory points upper right looks flat, 45-degree line. Thus distribution compressed right making skewed left.Exercise:\nBased histogram normal probability plot, nearly normal residuals condition appear met?, sample small appears residual skewed left.Constant variability:Exercise:\nBased residuals vs. fitted plot, constant variability condition appear met?Yes, width plot seems constant.","code":"\nggplot(data = m1, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x=\"Fitted values\",y=\"Residuals\",title=\"Residual analysis\") +\n  theme_bw()\nggplot(data = m1, aes(x = .resid)) +\n  geom_histogram(binwidth = .4,fill=\"cyan\",color=\"black\") +\n  xlab(\"Residuals\") +\n  theme_bw()\nggplot(data = m1, aes(sample = .resid)) +\n  stat_qq() +\n  theme_bw() +\n  geom_abline(slope=1,intercept = 0)"},{"path":"CS4.html","id":"brief-summary","chapter":"26 Case Study","heading":"26.8 Brief summary","text":"case study introduced simple linear regression. look criteria find best fit linear line two variables. also used R fit line. examined output R used explain predict model. remainder block develop ideas extend multiple predictors binary outcomes. perfect introduction Math 378.","code":""},{"path":"CS4.html","id":"homework-problems-25","chapter":"26 Case Study","heading":"26.9 Homework Problems","text":"HFIChoose another freedom variable variable think strongly correlate . openintro package contains data set hfi. Type ?openintro::hfi Console window RStudio learn variables.Produce scatterplot two variables.Quantify strength relationship correlation coefficient.Fit linear model. glance, seem linear relationship?relationship compare relationship pf_expression_control pf_score? Use \\(R^2\\) values two model summaries compare. independent variable seem predict dependent one better? ?Display model diagnostics regression model analyzing relationship.Predict response explanatory variable value median third quartile. overestimate underestimate, much? words, residual prediction?","code":""},{"path":"LRBASICS.html","id":"LRBASICS","chapter":"27 Linear Regression Basics","heading":"27 Linear Regression Basics","text":"","code":""},{"path":"LRBASICS.html","id":"objectives-26","chapter":"27 Linear Regression Basics","heading":"27.1 Objectives","text":"Obtain parameter estimates simple linear regression model given sample data.Interpret coefficients simple linear regression.Create scatterplot regression line.Explain check assumptions linear regression.Use able explain new terms.","code":""},{"path":"LRBASICS.html","id":"linear-regression-models","chapter":"27 Linear Regression Basics","heading":"27.2 Linear regression models","text":"rest block serve brief introduction linear models. general, model estimates relationship one variable (response) one variables (predictors). Models typically serve two purposes: prediction inference. model allows us predict value response given particular values predictors. Also, model allows us make inferences relationship response predictors.models used built principles. models better inference others better prediction. models better qualitative responses others better quantitative responses. Also, many models require making assumptions nature relationship variables. assumptions violated, model loses usefulness. machine learning course, wide array models discussed used purpose prediction.block, focus linear models use linear regression produce evaluate model. Linear regression powerful statistical technique. Many people familiarity regression just reading news, graphs straight lines overlaid scatterplots, much like last lesson. Linear models can used prediction evaluate whether linear relationship two numerical variables.Suppose interested exploring relationship one response variable (\\(Y\\)) one predictor variable (\\(X\\)). can postulate linear relationship two:\n\\[\nY=\\beta_0+\\beta_1 X\n\\]linear model can expanded include multiple predictor variables:\n\\[\nY=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p\n\\]model often referred linear regression model. (one predictor variable, often refer simple linear regression model.) \\(\\beta_j\\) terms referred coefficients. Note coefficients parameters thus represented Greek letters. typically don’t know true values \\(\\beta_j\\), estimate samples population \\(X\\) \\(Y\\), data. Estimating parameters using prediction inference majority work last block.consider models linear models linear parameters. means following model also linear model:\\[\nY=\\beta_0+\\beta_1 X^2\n\\]Technically, can write parameters vector explanatory variables matrix. response inner product vector matrix thus linear combination.Even expect two random variables \\(X\\) \\(Y\\) share linear relationship, don’t expect perfect. scatter around estimated line. example, consider head length total length 104 brushtail possums Australia. data file possum.csv data folder.Exercise:\nRead data data/possum.csv look first rows data.think head total length variables linearly associated. Possums average total length also tend average head lengths. visualize data, use scatterplot. used scatterplots multiple times book. Scatterplots graphical technique present two numerical variables simultaneously. plots permit relationship variables examined ease. following figure shows scatterplot head length total length possums. point represents single possum data.Exercise:\nCreate scatterplot head length total length.\nFigure 27.1: scatterplot possum total length head length.\nFigure 27.1, see relationship perfectly linear; however, helpful partially explain connection variables straight line. Since longer possums shorter heads shorter possums longer heads, straight line can go data points. expect deviation linear fit. deviation represented random variable \\(e\\) (Euler number), refer error term residual:\\[\nY=\\beta_0+\\beta_1X+e\n\\]problem, \\(Y\\) head length \\(X\\) total length.general :\\[ \\text{Data} = \\text{Fit} + \\text{Residual} \\]change modeling process specify \\(\\text{Fit}\\).error term assumed follow normal distribution mean 0 constant standard deviation \\(\\sigma\\). Note: assumption normality inference using \\(t\\) \\(F\\) distributions can relax assumption using bootstrap. Among things, assumptions imply linear model used response variable continuous nature. approaches non-continuous response variables (example logistic regression).","code":"\npossum<-read_csv(\"data/possum.csv\")\nglimpse(possum)## Rows: 104\n## Columns: 8\n## $ site    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n## $ pop     <chr> \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\", \"Vic\",~\n## $ sex     <chr> \"m\", \"f\", \"f\", \"f\", \"f\", \"f\", \"m\", \"f\", \"f\", \"f\", \"f\", \"f\", \"m~\n## $ age     <dbl> 8, 6, 6, 6, 2, 1, 2, 6, 9, 6, 9, 5, 5, 3, 5, 4, 1, 2, 5, 4, 3,~\n## $ head_l  <dbl> 94.1, 92.5, 94.0, 93.2, 91.5, 93.1, 95.3, 94.8, 93.4, 91.8, 93~\n## $ skull_w <dbl> 60.4, 57.6, 60.0, 57.1, 56.3, 54.8, 58.2, 57.6, 56.3, 58.0, 57~\n## $ total_l <dbl> 89.0, 91.5, 95.5, 92.0, 85.5, 90.5, 89.5, 91.0, 91.5, 89.5, 89~\n## $ tail_l  <dbl> 36.0, 36.5, 39.0, 38.0, 36.0, 35.5, 36.0, 37.0, 37.0, 37.5, 39~\nhead(possum)## # A tibble: 6 x 8\n##    site pop   sex     age head_l skull_w total_l tail_l\n##   <dbl> <chr> <chr> <dbl>  <dbl>   <dbl>   <dbl>  <dbl>\n## 1     1 Vic   m         8   94.1    60.4    89     36  \n## 2     1 Vic   f         6   92.5    57.6    91.5   36.5\n## 3     1 Vic   f         6   94      60      95.5   39  \n## 4     1 Vic   f         6   93.2    57.1    92     38  \n## 5     1 Vic   f         2   91.5    56.3    85.5   36  \n## 6     1 Vic   f         1   93.1    54.8    90.5   35.5\npossum %>%\n  gf_point(head_l~total_l) %>%\n  gf_labs(x=\"Total Length (cm)\",y=\"Head Length (mm)\") %>%\n  gf_theme(theme_classic())"},{"path":"LRBASICS.html","id":"estimation-1","chapter":"27 Linear Regression Basics","heading":"27.2.1 Estimation","text":"want describe relationship head length total length variables possum data set using line. example, use total length predictor variable, \\(x\\), predict possum’s head length, \\(y\\). Just side note, choice predictor response arbitrary. response typically want predict case experiments causal result predictor(s).possum data \\(n = 104\\) observations: \\((x_1,y_1), (x_2,y_2),...,(x_{104},y_{104})\\). observation implied linear model :\\[\ny_i=\\beta_0+\\beta x_i + e_i\n\\]fit linear relationship eye like case study obtain estimates slope intercept ad hoc.93 , given set data like possum, actually obtain estimates \\(\\beta_0\\) \\(\\beta_1\\)? best fit line?begin thinking mean ``best’’. Mathematically, want line small residuals. multiple methods, common method least squares. method, goal find values \\(\\beta_0\\) \\(\\beta_1\\) minimize squared vertical distance points resulting line, residuals. See 27.2 visual representation involving four observations made data.\nFigure 27.2: illustration least squares method.\ncriterion best estimates slope intercept minimize sum squared residuals:\\[e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\\]following three possible reasons choose least squares criterion criteria sum absolute value residuals:commonly used method.Computing line based least squares much easier hand computers available.many applications, residual twice large another residual twice bad. example, 4 usually twice bad 2. Squaring residuals accounts discrepancy.first two reasons largely tradition convenience; last reason explains least squares typically helpful.94So, need find \\(\\beta_0\\) \\(\\beta_1\\) minimize expression, observed minus expected:\n\\[\n\\sum_{=1}^n (y_i-\\beta_0-\\beta_1 x_i)^2\n\\]Using calculus-based optimization yields following estimates \\(\\beta_0\\) \\(\\beta_1\\):\\[\n\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}\n\\]Notice implies line always go point \\(\\left(\\bar{x},\\bar{y} \\right)\\). reminder \\(\\bar{x}\\) sample mean explanatory variable \\(\\bar{y}\\) sample mean response.\\[\n\\hat{\\beta}_1 = {\\sum x_i y_i - n\\bar{x}\\bar{y} \\{\\sum x_i^2 -n\\bar{x}^2}}\n\\]intuitive formula slope one links correlation linear regression :\\[\n\\hat{\\beta}_1 = \\frac{s_y}{s_x} R\n\\]\\(R\\) correlation two variables, \\(s_x\\) \\(s_y\\) sample standard deviations explanatory variable response, respectively. Thus slope proportional correlation.may also interested estimating \\(\\sigma\\), standard deviation error:\n\\[\n\\hat{\\sigma}=\\sqrt{{1\\{n-2}} \\sum_{=1}^n \\hat{e}_i^2}\n\\]\\(\\hat{e}_i\\) observed \\(\\)th residual (\\(\\hat{e}_i=y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i\\)). estimate based assumption constant variance.","code":""},{"path":"LRBASICS.html","id":"possum-example","chapter":"27 Linear Regression Basics","heading":"27.2.2 Possum example","text":"let R heavy work minimizing sum squares. function lm() learned case study. function needs formula data input. formula notation easy us since worked formulas much mosaic package.First create model:output model object minimal just estimated slope intercept.can get information using summary() function:model object, poss_mod, contains much information. Using function names() function model objects, gives list quantities available, residuals.Figure 27.3 plot data points least squares line together.\nFigure 27.3: scatterplot possum total length head length including regression line.\n","code":"\nposs_mod <- lm(head_l~total_l,data=possum)\nposs_mod## \n## Call:\n## lm(formula = head_l ~ total_l, data = possum)\n## \n## Coefficients:\n## (Intercept)      total_l  \n##     42.7098       0.5729\nsummary(poss_mod)## \n## Call:\n## lm(formula = head_l ~ total_l, data = possum)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1877 -1.5340 -0.3345  1.2788  7.3968 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 42.70979    5.17281   8.257 5.66e-13 ***\n## total_l      0.57290    0.05933   9.657 4.68e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.595 on 102 degrees of freedom\n## Multiple R-squared:  0.4776, Adjusted R-squared:  0.4725 \n## F-statistic: 93.26 on 1 and 102 DF,  p-value: 4.681e-16\nnames(poss_mod)##  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n##  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n##  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\npossum %>%\n  gf_point( head_l ~ total_l) %>%\n  gf_lm(color=\"black\") %>%\n  gf_labs(x=\"Total Length (cm)\",y=\"Head Length (mm)\") %>%\n  gf_labs(title=\"Possum data including regression line\") %>%\n  gf_theme(theme_classic()) "},{"path":"LRBASICS.html","id":"interpretation","chapter":"27 Linear Regression Basics","heading":"27.2.3 Interpretation","text":"Interpreting parameters regression model often one important steps analysis. intercept term, \\(\\beta_0\\), usually uninteresting. represents average value response predictor 0. Unless center predictor variable around 0, actual value intercept usually important; typically just gives slope flexibility fit data. slope term, \\(\\beta_1\\) represents average increase response variable per unit increase predictor variable. keep using word average discussion. assumption mean 0 residuals, least squares ensures line going point \\(\\left(\\bar{x},\\bar{y} \\right)\\), output model expected average response given input. Mathematically :\\[\nE(Y|X=x)=E(\\beta_0+\\beta_1x+e) = E(\\beta_0+\\beta_1x)+E(e)=\\beta_0+\\beta_1x\n\\]Predicting value response variable simply becomes matter substituting value predictor variable estimated regression equation. , important note given value \\(x\\), predicted response, \\(\\hat{y}\\) expect average value \\(y\\) given specific value predictor.Exercise: slope intercept estimates possum data 0.5729 42.7098. numbers really mean, interpret ?Interpreting slope parameter helpful almost application. additional 1 cm total length possum, expect possum’s head 0.5729 mm longer average. Note longer total length corresponds longer head slope coefficient positive. must cautious interpretation: real association, interpret causal connection variables data observational. , increasing possum’s total length may cause possum’s head longer.estimated intercept \\(b_0=42.7\\) describes average head length possum zero total length! meaning intercept irrelevant application since possum can practically total length 0.Earlier noted relationship slope estimate correlation coefficient estimate.Exercise\nFind slope correlation standard deviations.","code":"\npossum %>%\n  summarise(correlation=cor(head_l,total_l),sd_head=sd(head_l),\n            sd_total=sd(total_l),slope=correlation*sd_head/sd_total)## # A tibble: 1 x 4\n##   correlation sd_head sd_total slope\n##         <dbl>   <dbl>    <dbl> <dbl>\n## 1       0.691    3.57     4.31 0.573"},{"path":"LRBASICS.html","id":"extrapolation-is-dangerous","chapter":"27 Linear Regression Basics","heading":"27.2.4 Extrapolation is dangerous","text":"“blizzards hit East Coast winter, proved satisfaction global warming fraud. snow freezing cold. alarming trend, temperatures spring risen. Consider : February \\(6^{th}\\) 10 degrees. Today hit almost 80. rate, August 220 degrees. clearly folks climate debate rages .”\nStephen Colbert\nApril 6th, 201095Linear models can used approximate relationship two variables built observed random sample. models real limitations linear regression simply modeling framework. truth almost always much complex simple line. Extrapolation occurs one tries make prediction response given value predictor outside range values used build model. information relationship two variables region around observed data. know data outside limited window behave. careful extrapolating.","code":""},{"path":"LRBASICS.html","id":"reading-computer-output","chapter":"27 Linear Regression Basics","heading":"27.2.5 Reading computer output","text":"stored results linear regression model possum data object poss_mod provided bare minimum information. can get information using function summary().first line repeats model formula. second line descriptive summary residuals, plots residuals useful summary. table model fit. first column numbers provides estimates \\(\\beta_0\\) \\(\\beta_1\\), respectively. next columns, ’ll describe meaning columns using second row, corresponds information slope estimate. , first column provides point estimate \\(\\beta_1\\). second column standard error point estimate. third column \\(t\\) test statistic null hypothesis \\(\\beta_1 = 0\\): \\(T=9.657\\). last column p-value \\(t\\) test statistic null hypothesis \\(\\beta_1=0\\) two-sided alternative hypothesis. get details next chapters.row residual standard error estimate unexplained variance. next rows give summary model fit discuss next chapters.tidyverse may want table tibble. broom package, seen , helps effort.Exercise:\ncars dataset (built-R) contains 50 observations 2 variables. data give speed cars (mph) corresponding distance (feet) took stop. Attempt answer following questions.Build simple linear regression model fitting distance speed.Figure 27.4 scatterplot cars data set.\nFigure 27.4: scatterplot speed stopping distance.\nexpected, appears larger speeds, stopping distance greater.Report interpret estimated model coefficients.estimated intercept term, \\(\\hat{\\beta}_0\\) equal -17.6. estimate doesn’t helpful interpretation. Technically, estimated average stopping distance speed 0. However, “stopping distance” doesn’t make sense car speed. Also, negative stopping distance doesn’t make sense. Furthermore, speed 0 outside observed speeds data set, even speed 0 made sense, outside scope data thus extrapolation.estimated slope term, \\(\\hat{\\beta}_1\\) equal 3.9. means increase one mph, expect stopping distance increase 3.9 feet, average.Report estimated common standard deviation residuals.estimated standard deviation error (residual), \\(\\hat{\\sigma}\\) equal 15.4.","code":"\nsummary(poss_mod)## \n## Call:\n## lm(formula = head_l ~ total_l, data = possum)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1877 -1.5340 -0.3345  1.2788  7.3968 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 42.70979    5.17281   8.257 5.66e-13 ***\n## total_l      0.57290    0.05933   9.657 4.68e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.595 on 102 degrees of freedom\n## Multiple R-squared:  0.4776, Adjusted R-squared:  0.4725 \n## F-statistic: 93.26 on 1 and 102 DF,  p-value: 4.681e-16\nlibrary(broom)\ntidy(poss_mod)## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   42.7      5.17        8.26 5.66e-13\n## 2 total_l        0.573    0.0593      9.66 4.68e-16\ncars_mod <- lm(dist~speed,data=cars)\nsummary(cars_mod)## \n## Call:\n## lm(formula = dist ~ speed, data = cars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -29.069  -9.525  -2.272   9.215  43.201 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -17.5791     6.7584  -2.601   0.0123 *  \n## speed         3.9324     0.4155   9.464 1.49e-12 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 15.38 on 48 degrees of freedom\n## Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 \n## F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\ncars %>%\n  gf_point(dist~speed) %>%\n  gf_lm() %>%\n  gf_labs(x=\"Speed (mph)\",y=\"Stopping distance (ft)\") %>%\n  gf_theme(theme_bw())"},{"path":"LRBASICS.html","id":"assumptions","chapter":"27 Linear Regression Basics","heading":"27.2.6 Assumptions","text":"Anytime build model, assumptions behind , violated, invalidate conclusions model. description simple linear regression, briefly mentioned assumptions. Next chapter, discuss validate assumptions.Fit. build simple linear regression, assume relationship response predictor specify fit formula. simple linear regression often just linear relationship. Suppose two variables non-linearly related, see 27.5. build linear regression model two, resulting model useful. built model fit formulated quadratic, similar plot residuals look flat. discuss later chapter.\nFigure 27.5: example non-linear relationship two variables fitted linear regression line.\nIndependent Observations. Another assumption observations data set independent one another. common way assumption violated using time predictor variable. example, suppose interested individual’s weight changes time. may tempting plot fit regression line data, resulting model inappropriate, simple linear regression assumes observation independent. Figure 27.6 shows correlated data fitted linear regression line.\nFigure 27.6: scatterplot correlated data fit using linear regression model assumption independence.\nConstant Error Variance. simple linear regression, assume residuals come normal distribution mean 0 constant standard deviation \\(\\sigma\\). Violation assumption usually manifested “megaphone” pattern scatterplot. Specifically, value predictor increases, variance response also increases, resulting greater spread larger values predictor.Normality Errors. , assume residuals normally distributed. Normality residuals easy see graphically, use diagnostics check assumption.last three assumptions important necessarily estimating relationship, inferring relationship. future chapters, discuss use model prediction, build confidence/prediction interval around prediction. Also, discuss inference coefficient estimates model. Violation one last three assumptions impact ability conduct inference population parameters.","code":""},{"path":"LRBASICS.html","id":"residual-plots","chapter":"27 Linear Regression Basics","heading":"27.2.7 Residual plots","text":"One purpose residual plots identify characteristics patterns still apparent data fitting model. can help evaluate assumptions.Figure 27.7 shows three scatterplots linear models first row residual plots second row.\nFigure 27.7: Residual plots associated scatterplots.\nfirst data set (first column), residuals show obvious patterns. residuals appear scattered randomly around dashed line represents 0.second data set shows pattern residuals. curvature scatterplot, obvious residual plot. use straight line model data. Instead, advanced technique used.last plot spread, variance data, seems increase explanatory variable increases. can see clearly residual plot. make inference using \\(t\\) \\(F\\) distribution require transformation equalize variance.","code":""},{"path":"LRBASICS.html","id":"summary-1","chapter":"27 Linear Regression Basics","heading":"27.2.8 Summary","text":"introduced ideas linear regression lesson. many new terms well new R functions learn. continue use ideas remainder block material. Next learn inference prediction.","code":""},{"path":"LRBASICS.html","id":"homework-problems-26","chapter":"27 Linear Regression Basics","heading":"27.3 Homework Problems","text":"Nutrition StarbucksIn data folder file named starbucks.csv. Use answer questions .Create scatterplot number calories amount carbohydrates.Describe relationship graph.scenario, explanatory response variables?might want fit regression line data?Create scatterplot number calories amount carbohydrates regression line included.Using ’lm()` fit least squares line data.Report interpret slope coefficient.menu item 51 g carbs, estimated calorie count?use model menu item 100 g carbs?assumption constant variance seem reasonable problem?Verify line passes mean carb mean calories, mathematically.estimate standard deviation residuals? use information?","code":""},{"path":"LRINF.html","id":"LRINF","chapter":"28 Linear Regression Inference","heading":"28 Linear Regression Inference","text":"","code":""},{"path":"LRINF.html","id":"objectives-27","chapter":"28 Linear Regression Inference","heading":"28.1 Objectives","text":"Given simple linear regression model, conduct inference coefficients \\(\\beta_0\\) \\(\\beta_1\\).Given simple linear regression model, calculate predicted response given value predictor.Build interpret confidence prediction intervals values response variable.","code":""},{"path":"LRINF.html","id":"introduction-3","chapter":"28 Linear Regression Inference","heading":"28.2 Introduction","text":"chapter discuss uncertainty estimates slope y-intercept regression line. allow us perform inference predictions. Just identified standard errors point estimates previous chapters, first discuss standard errors new estimates. chapter classical chapter sense using normal distribution. assume errors normally distributed constant variance. Later book, relax assumptions.","code":""},{"path":"LRINF.html","id":"regression","chapter":"28 Linear Regression Inference","heading":"28.2.1 Regression","text":"Last chapter, introduced linear models using simple linear regression model:\n\\[\nY=\\beta_0+\\beta_1X+e\n\\]now assume error term follows normal distribution mean 0 constant standard deviation \\(\\sigma\\). Using method least squares, require assumption normality, obtain estimates \\(\\beta_0\\) \\(\\beta_1\\):\n\\[\n\\hat{\\beta}_1 = {\\sum x_i y_i - n\\bar{x}\\bar{y} \\\\sum x_i^2 -n\\bar{x}^2}\n\\]\n\\[\n\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}\n\\]assume probability distribution errors, also find point estimates using maximum likelihood methods. discussed book.Using estimates, given value predictor, \\(x_*\\), can obtain prediction response variable. using subscript \\(_*\\) denote new value explanatory variable. resulting prediction, denote \\(\\hat{Y}_*\\), average expected value response given predictor value \\(x_*\\):\\[\n\\hat{Y}_*=\\hat{\\beta}_0+\\hat{\\beta}_1x_*\n\\]reason model returns expected value response given value predictor error term expected value zero. review properties expectation well last chapter, :\\[\nE(Y|X=x)=E(\\beta_0+\\beta_1x+e)=Y=\\beta_0+\\beta_1x+E(e)=\\beta_0+\\beta_1x\n\\]\n\\(\\beta_0\\), \\(\\beta_1\\), \\(x\\) constants.abundantly clear now \\(\\hat{Y}_*\\), \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) estimators. estimators, dependent random sample, data. collect new random sample population, get new estimates estimators. Thus, can think \\(\\hat{Y}_*\\), \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) random variables. Like random variables, distributions. can use distribution estimator build confidence intervals conduct hypothesis tests true values parameter intended estimate. estimators based least squares unbiased, distributions centered around actual values \\(Y\\), \\(\\beta_0\\) \\(\\beta_1\\), respectively.","code":""},{"path":"LRINF.html","id":"review-of-assumptions","chapter":"28 Linear Regression Inference","heading":"28.2.2 Review of assumptions","text":"review assumptions least squares model important inference. Refer Figure 28.1, plots linear regression top row residuals second row. generally assume following:Fit. data show linear trend. nonlinear trend, transformation explanatory variable advanced regression method applied. looking residual plot, trend linear, see spread points flat. left column Figure 28.1 example nonlinear relationship. top plot regression plot can see looks like quadratic relationship instead linear one. residual plot, plot lower left corner Figure 28.1, also exhibits non-linear trend.Nearly normal residuals. Generally residuals must nearly normal use \\(t\\) \\(F\\) inference. assumption found unreasonable, usually outliers concerns influential points. example non-normal residuals shown second column Figure 28.1. qq plot also useful diagnostic tool seen. can still use bootstrap inference tool normality assumption unreasonable.Constant variability. variability points around least squares line remains roughly constant. example non-constant variability shown third panel Figure 28.1. constant variability assumption needed \\(t\\) \\(F\\) distributions. required bootstrap method.Independent observations. cautious applying regression data collected sequentially called time series. data may underlying structure considered model analysis. example time series independence violated shown fourth panel Figure 28.1. advanced methods required time series data even including using bootstrap.later chapter explore regression diagnostics.\nFigure 28.1: Plots linear regression residual illustrate assumptions model.\n","code":""},{"path":"LRINF.html","id":"distribution-of-our-estimators","chapter":"28 Linear Regression Inference","heading":"28.2.3 Distribution of our estimators","text":"assumption error term normally distributed, can find distributions estimates, turn normal:\\[\n\\hat{\\beta}_0\\sim N\\left(\\beta_0, \\sigma\\sqrt{{1\\n}+{\\bar{x}^2\\\\sum (x_i-\\bar{x})^2}}\\right)\n\\]\\[\n\\hat{\\beta}_1\\sim N\\left(\\beta_1, {\\sigma \\\\sqrt{ \\sum (x_i-\\bar{x})^2}}\\right)\n\\]\\[\n\\hat{Y}_* \\sim N\\left(\\beta_0+\\beta_1x_*, \\sigma\\sqrt{{1\\n}+{(x_*-\\bar{x})^2\\\\sum (x_i-\\bar{x})^2}}\\right)\n\\]Notice three unbiased, expected value equal parameter estimated. Looking variance slope estimate can see function underlying unexplained variance, \\(\\sigma^2\\) data. denominator increased larger spread explanatory variable. slope estimated line stable, less variable, independent variable high variance. interesting. designing experiment, gives insight select range values explanatory variable.","code":""},{"path":"LRINF.html","id":"inference","chapter":"28 Linear Regression Inference","heading":"28.3 Inference","text":"Now know coefficient estimates average predicted values behave, can perform inference true values. Let’s take \\(\\hat{\\beta}_1\\) demonstration:\\[\n\\hat{\\beta}_1\\sim N\\left(\\beta_1, {\\sigma \\\\sqrt{ \\sum (x_i-\\bar{x})^2}}\\right)\n\\]Thus,\\[\n{\\hat{\\beta}_1-\\beta_1 \\{\\sigma \\\\sqrt{ \\sum (x_i-\\bar{x})^2}}}\\sim N\\left(0, 1\\right)\n\\]However, note expression left depends error standard deviation, \\(\\sigma\\). reality, know value estimate \\[\n\\hat{\\sigma}=\\sqrt{{1\\n-2} \\sum_{=1}^n \\hat{e}_i^2}\n\\]\\(\\hat{e}_i\\) observed \\(\\)th residual (\\(\\hat{e}_i=y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i\\)).learned last block, replace population standard deviation (\\(\\sigma\\)) estimation, resulting random variable longer standard normal distribution. fact, can shown \\[\n{\\hat{\\beta}_1-\\beta_1 \\{\\hat \\sigma \\\\sqrt{ \\sum (x_i-\\bar{x})^2}}}\\sim \\textsf{t}\\left(n-2\\right)\n\\]\n\\(n-2\\) degrees freedom estimation \\(\\sigma^2\\) estimate two parameters, \\(\\beta_0\\) \\(\\beta_1\\).can use information build \\((1-\\alpha)*100\\%\\) confidence interval \\(\\beta_1\\). First, recognize \\[\n\\mbox{P}\\left(-t_{\\alpha/2,n-2} \\leq {\\hat{\\beta}_1-\\beta_1 \\{\\hat \\sigma \\\\sqrt{ \\sum (x_i-\\bar{x})^2}}}\\leq t_{\\alpha/2,n-2} \\right) = 1-\\alpha\n\\]Solving expression inside probability statement \\(\\beta_1\\) yields confidence interval \\[\n\\beta_1 \\\\left(\\hat{\\beta_1} \\pm t_{\\alpha/2,n-2}{\\hat \\sigma \\\\sqrt{\\sum(x_i-\\bar{x})^2}}\\right)\n\\]can also evaluate null hypothesis \\(H_0: \\beta_1 =\\beta^*_1\\). true value \\(\\beta_1\\) \\(\\beta^*_1\\), estimated \\(\\hat{\\beta_1}\\) around value. fact, \\(H_0\\) true, value\\[\n{\\hat{\\beta}_1-\\beta^*_1 \\{\\hat \\sigma \\\\sqrt{ \\sum (x_i-\\bar{x})^2}}}\n\\]\\(\\textsf{t}\\) distribution \\(n-2\\) degrees freedom. Thus, collect sample obtain observed \\(\\hat{\\beta_1}\\) \\(\\hat \\sigma\\), can calculate quantity determine whether far enough zero reject \\(H_0\\).Similarly, can use distribution \\(\\hat \\beta_0\\) build confidence interval conduct hypothesis test \\(\\beta_0\\), usually don’t. interpretation \\(\\beta_0\\).","code":""},{"path":"LRINF.html","id":"starbucks","chapter":"28 Linear Regression Inference","heading":"28.3.1 Starbucks","text":"great deal mathematics theory. Let’s put use example Starbucks. file data/starbucks.csv nutritional facts several Starbucks’ food items. used data homework last chapter. use data illustrate ideas introduced section.Read data.Exercise:\nSummarize explore data.Let’s look summary data.Let’s predict calories carbohydrate content.Exercise:\nCreate scatterplot calories carbohydrate, carbs, content.Figure 28.2 scatterplot.\nFigure 28.2: Scatterplot calories carbohydrate content Starbucks’ products.\nExercise:\nUse R fit linear regression model regressing calories carb.results fitting linear least squares model stored star_mod object.","code":"\nstarbucks <- read_csv(\"data/starbucks.csv\")  %>%\n  mutate(type=factor(type))\nglimpse(starbucks)## Rows: 77\n## Columns: 7\n## $ item     <chr> \"8-Grain Roll\", \"Apple Bran Muffin\", \"Apple Fritter\", \"Banana~\n## $ calories <dbl> 350, 350, 420, 490, 130, 370, 460, 370, 310, 420, 380, 320, 3~\n## $ fat      <dbl> 8, 9, 20, 19, 6, 14, 22, 14, 18, 25, 17, 12, 17, 21, 5, 18, 1~\n## $ carb     <dbl> 67, 64, 59, 75, 17, 47, 61, 55, 32, 39, 51, 53, 34, 57, 52, 7~\n## $ fiber    <dbl> 5, 7, 0, 4, 0, 5, 2, 0, 0, 0, 2, 3, 2, 2, 3, 3, 2, 3, 0, 2, 0~\n## $ protein  <dbl> 10, 6, 5, 7, 0, 6, 7, 6, 5, 7, 4, 6, 5, 5, 12, 7, 8, 6, 0, 10~\n## $ type     <fct> bakery, bakery, bakery, bakery, bakery, bakery, bakery, baker~\ninspect(starbucks)## \n## categorical variables:  \n##   name     class levels  n missing\n## 1 item character     77 77       0\n## 2 type    factor      7 77       0\n##                                    distribution\n## 1 8-Grain Roll (1.3%) ...                      \n## 2 bakery (53.2%), petite (11.7%) ...           \n## \n## quantitative variables:  \n##          name   class min  Q1 median  Q3 max       mean         sd  n missing\n## ...1 calories numeric  80 300    350 420 500 338.831169 105.368701 77       0\n## ...2      fat numeric   0   9     13  18  28  13.766234   7.095488 77       0\n## ...3     carb numeric  16  31     45  59  80  44.870130  16.551634 77       0\n## ...4    fiber numeric   0   0      2   4   7   2.220779   2.112764 77       0\n## ...5  protein numeric   0   5      7  15  34   9.480519   8.079556 77       0\nstarbucks %>%\n  gf_point(calories~carb) %>%\n  gf_labs(x=\"Carbohydrates\",y=\"Calories\") %>%\n  gf_theme(theme_classic())\nstar_mod <- lm(formula = calories ~ carb, data = starbucks)\nsummary(star_mod)## \n## Call:\n## lm(formula = calories ~ carb, data = starbucks)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -151.962  -70.556   -0.636   54.908  179.444 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 146.0204    25.9186   5.634 2.93e-07 ***\n## carb          4.2971     0.5424   7.923 1.67e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 78.26 on 75 degrees of freedom\n## Multiple R-squared:  0.4556, Adjusted R-squared:  0.4484 \n## F-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11"},{"path":"LRINF.html","id":"hypothesis-test","chapter":"28 Linear Regression Inference","heading":"28.3.1.1 Hypothesis test","text":"second row Coefficients portion table point estimate, standard error, test statistic, p-value slope.hypotheses output \\(H_0\\): \\(\\beta_1 = 0\\). true linear model slope zero. carb content impact calorie content.\\(H_A\\): \\(\\beta_1 \\neq 0\\). true linear model slope different zero. higher carb content, greater average calorie content vice-versa.estimate slope 4.297 standard error 0.5424. Just demonstration purposes, use R calculate test statistic p-value series steps. test statistic null hypothesis :\\[\n{\\hat{\\beta}_1-0 \\{\\hat \\sigma \\\\sqrt{ \\sum (x_i-\\bar{x})^2}}}\n\\]\ndenominator standard error estimate. estimate residual standard deviation reported last line 78.26. just square root sum squared residuals divided degrees freedom.standard error slope estimate , confirmed table:test statistic isAnd p-valueThis slightly different table value precision computer small p-value.reject \\(H_0\\) favor \\(H_A\\) data provide strong evidence true slope parameter greater zero.computer software uses zero null hypothesis, wanted test another value slope calculations step step like .way, tidy way calculation. broom package makes easier use tidy ideas regression model. used ideas last chapter.reminder:step step:","code":"\nsighat<-sqrt(sum((star_mod$residuals)^2)/75)\nsighat## [1] 78.25956\nstd_er<-sighat/sqrt(sum((starbucks$carb-mean(starbucks$carb))^2))\nstd_er## [1] 0.5423626\n(4.2971-0)/std_er## [1] 7.922928\n2*pt((4.2971-0)/std_er,73,lower.tail = FALSE)## [1] 1.965319e-11\nlibrary(broom)\ntidy(star_mod) ## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   146.      25.9        5.63 2.93e- 7\n## 2 carb            4.30     0.542      7.92 1.67e-11\ntidy(star_mod) %>%\n  filter(term==\"carb\") %>%\n  summarize(test_stat=(estimate-0)/std.error,p_value=2*pt(test_stat,df=73,lower.tail = FALSE))## # A tibble: 1 x 2\n##   test_stat  p_value\n##       <dbl>    <dbl>\n## 1      7.92 1.97e-11"},{"path":"LRINF.html","id":"confidence-interval-1","chapter":"28 Linear Regression Inference","heading":"28.3.1.2 Confidence interval","text":"calculate confidence interval point estimate, standard error, critical value let R us.confidence interval contain value 0. suggests value 0 probably feasible \\(\\beta_1\\).end, declare carbohydrate calorie content Starbucks’ menu items linearly correlated. However, prove causation. simply showed two variables correlated.","code":"\nconfint(star_mod)##                 2.5 %     97.5 %\n## (Intercept) 94.387896 197.652967\n## carb         3.216643   5.377526"},{"path":"LRINF.html","id":"inference-on-predictions","chapter":"28 Linear Regression Inference","heading":"28.4 Inference on Predictions","text":"Similarly, can take advantage distribution \\(\\hat Y_*\\) build confidence interval \\(Y_*\\) (average value \\(Y\\) value \\(x_*\\)):\\[\nY_*\\\\left(\\hat Y_* \\pm t_{\\alpha/2,n-2}\\hat \\sigma \\sqrt{{1\\n}+{(x_*-\\bar{x})^2\\\\sum (x_i-\\bar{x})^2}} \\right)\n\\]couple things point . First, note width confidence interval dependent far \\(x_*\\) average value \\(x\\). center data, wider interval .Second, note interval \\(Y_*\\) average value \\(Y\\) \\(x_*\\). want build interval single observation \\(Y\\) (\\(Y_{new}\\)), need build prediction interval, considerably wider confidence interval \\(Y_*\\):\\[\nY_{new}\\\\left(\\hat Y_* \\pm t_{\\alpha/2,n-2}\\hat \\sigma \\sqrt{1+{1\\n}+{(x_*-\\bar{x})^2\\\\sum (x_i-\\bar{x})^2}} \\right)\n\\]","code":""},{"path":"LRINF.html","id":"starbucks-1","chapter":"28 Linear Regression Inference","heading":"28.4.1 Starbucks","text":"Continuing Starbucks example. plotting data, can R plot confidence prediction bands, Figure 28.3. observe width intervals increase move away center data also prediction intervals wider confidence interval.\nFigure 28.3: Confidence predictions bands linear regression model calories carbs Starbucks’ products.\ndone diagnostics yet may using linear regression model data may appropriate. sake learning continue. find confidence intervals need value carb let’s use 60 70.create data frame new values carb . use predict function find confidence interval. Using option interval set confidence return confidence interval average calorie content value new data frame.using broom package.example, 95% confident average calories Starbucks’ menu item 60 grams carbs 379.7 428.0.Exercise:\nGive 95% confidence interval average calories 70 grams carbohydrates.95% confident average calories Starbucks’ menu item 70 grams carbs 414.4 479.3.prediction interval, simply need change option interval:95% confident next Starbucks’ menu item 60 grams carbs calorie content 246 561. Notice prediction intervals wider since intervals individual observations averages.Exercise:\nGive 90% prediction interval average calories 70 grams carbohydrates.changed confidence level. Since less confident, interval narrower 95% prediction interval just calculated.90% confident next Starbucks’ menu item 70 grams carbs calorie content 313.7 579.9.","code":"\nstarbucks %>%\n  gf_point(calories~carb) %>%\n  gf_labs(x=\"Carbohydrates\",y=\"Calories\") %>%\n  gf_lm(stat=\"lm\",interval=\"confidence\") %>%\n  gf_lm(stat=\"lm\",interval=\"prediction\") %>%\n  gf_theme(theme_classic())\nnew_carb <- data.frame(carb=c(60,70))\npredict(star_mod, newdata = new_carb, interval = 'confidence')##        fit      lwr      upr\n## 1 403.8455 379.7027 427.9883\n## 2 446.8163 414.3687 479.2640\naugment(star_mod,newdata=new_carb,interval=\"confidence\")## # A tibble: 2 x 4\n##    carb .fitted .lower .upper\n##   <dbl>   <dbl>  <dbl>  <dbl>\n## 1    60    404.   380.   428.\n## 2    70    447.   414.   479.\nnew_carb <- data.frame(carb=c(60,70))\npredict(star_mod, newdata = new_carb, interval = 'prediction')##        fit      lwr      upr\n## 1 403.8455 246.0862 561.6048\n## 2 446.8163 287.5744 606.0582\npredict(star_mod, newdata = new_carb, level=0.9, interval = 'prediction')##        fit      lwr      upr\n## 1 403.8455 271.9565 535.7345\n## 2 446.8163 313.6879 579.9448"},{"path":"LRINF.html","id":"summary-2","chapter":"28 Linear Regression Inference","heading":"28.4.2 Summary","text":"chapter introduced process inference simple linear regression model. tested slope estimate well generated confidence intervals average individual predicted values.","code":""},{"path":"LRINF.html","id":"homework-problems-27","chapter":"28 Linear Regression Inference","heading":"28.5 Homework Problems","text":"chapter reading, noticed 95% prediction interval much wider 95% confidence interval. words, explain .chapter reading, noticed 95% prediction interval much wider 95% confidence interval. words, explain .Beer blood alcohol contentBeer blood alcohol contentMany people believe gender, weight, drinking habits, many factors much important predicting blood alcohol content (BAC) simply considering number drinks person consumed. examine data sixteen student volunteers Ohio State University drank randomly assigned number cans beer. students evenly divided men women, differed weight drinking habits. Thirty minutes later, police officer measured blood alcohol content (BAC) grams alcohol per deciliter blood. data bac.csv file data folder.Create scatterplot cans beer blood alcohol level.Describe relationship number cans beer BAC.Write equation regression line. Interpret slope intercept context.data provide strong evidence drinking cans beer associated increase blood alcohol? State null alternative hypotheses, report p-value, state conclusion.Build 95% confidence interval slope interpret context hypothesis test part d.Suppose visit bar, ask people many drinks , also take BAC. think relationship number drinks BAC strong relationship found Ohio State study?Predict average BAC two beers build 90% confidence interval around prediction.Repeat except build 90% prediction interval interpret.Plot data points regression line, confidence band, prediction band.Suppose build regression fitting response variable one predictor variable. build 95% confidence interval \\(\\beta_1\\) find contains 0, meaning slope 0 feasible. mean response predictor independent?","code":""},{"path":"LRDIAG.html","id":"LRDIAG","chapter":"29 Regression Diagnostics","heading":"29 Regression Diagnostics","text":"","code":""},{"path":"LRDIAG.html","id":"objectives-28","chapter":"29 Regression Diagnostics","heading":"29.1 Objectives","text":"Obtain interpret \\(R\\)-squared \\(F\\)-statistic.Use R evaluate assumptions linear model.Identify explain outliers leverage points.","code":""},{"path":"LRDIAG.html","id":"introduction-4","chapter":"29 Regression Diagnostics","heading":"29.2 Introduction","text":"last two chapters, detailed simple linear regression. First, described model underlying assumptions. Next, obtained parameter estimates using method least squares. Finally, obtained distributions parameter estimates used information conduct inference parameters predictions. Implementation relatively straightforward; obtained expressions interest, used R find parameters estimates, interval estimates, etc. chapter explore tools assess quality simple linear regression model. tools generalize move multiple predictors.","code":""},{"path":"LRDIAG.html","id":"assessing-our-model---understanding-the-output-from-lm","chapter":"29 Regression Diagnostics","heading":"29.3 Assessing our model - understanding the output from lm","text":"can output lm() function just estimating parameters predicting responses. metrics allow us assess fit model. explore ideas let’s use Starbucks example .First load data:Next build summarize model:may noticed information appeared summary model. discussed output previous chapter let’s go little depth.","code":"\nstarbucks <- read_csv(\"data/starbucks.csv\")\nstar_mod <- lm(calories~carb,data=starbucks)\nsummary(star_mod)## \n## Call:\n## lm(formula = calories ~ carb, data = starbucks)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -151.962  -70.556   -0.636   54.908  179.444 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 146.0204    25.9186   5.634 2.93e-07 ***\n## carb          4.2971     0.5424   7.923 1.67e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 78.26 on 75 degrees of freedom\n## Multiple R-squared:  0.4556, Adjusted R-squared:  0.4484 \n## F-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11"},{"path":"LRDIAG.html","id":"residual-standard-error","chapter":"29 Regression Diagnostics","heading":"29.3.1 Residual Standard Error","text":"“residual standard error” estimate \\(\\sigma\\), unexplained variance response. example, turned 78.26. assumptions normality constant variance valid, expect majority, 68%, observed values given input within \\(\\pm 78.26\\) mean value.want extract just value model object, first recognize summary(.model) list several components:expected, sigma component shows estimated value \\(\\sigma\\)., value smaller closer points regression fit. measure unexplained variance response variable.","code":"\nnames(summary(star_mod))##  [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n##  [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n##  [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"\nsummary(star_mod)$sigma## [1] 78.25956"},{"path":"LRDIAG.html","id":"r-squared","chapter":"29 Regression Diagnostics","heading":"29.3.2 R-squared","text":"Another quantity appears \\(R\\)-squared, also know coefficient determination. \\(R\\)-squared one measure goodness fit. Essentially \\(R\\)-squared ratio variance (response) explained model overall variance response. helps describe decomposition variance:\\[\n\\underbrace{\\sum_{=1}^n (y_i-\\bar{y})^2}_{SS_{\\text{Total}}} = \\underbrace{\\sum_{=1}^n (\\hat{y}_i-\\bar y)^2}_{SS_{\\text{Regression}}}+\\underbrace{\\sum_{=1}^n(y_i-\\hat{y}_i)^2}_{SS_{\\text{Error}}}\n\\]words, overall variation \\(y\\) can separated two parts: variation due linear relationship \\(y\\) predictor variable(s), called \\(SS_\\text{Regression}\\), residual variation (due random scatter perhaps poorly chosen model), called \\(SS_\\text{Error}\\). Note: \\(SS_\\text{Error}\\) used estimate residual standard error previous section.96\\(R\\)-squared simply measures ratio \\(SS_\\text{Regression}\\) \\(SS_\\text{Total}\\). common definition \\(R\\)-squared proportion overall variation response explained linear model. \\(R\\)-squared can 0 1. Values \\(R\\)-squared close 1 indicate tight fit (little scatter) around estimated regression line. Value close 0 indicate opposite (large remaining scatter).can obtain \\(R\\)-squared “hand” using output lm() function:simple linear regression, \\(R\\)-squared related correlation. can compute correlation using formula, just sample mean standard deviation. However, formula rather complex,97 let R heavy lifting us.review, Figure 29.1 shows eight plots corresponding correlations. relationship perfectly linear correlation either -1 1. relationship strong positive, correlation near +1. strong negative, near -1. apparent linear relationship variables, correlation near zero.\nFigure 29.1: Scatterplots demonstrating different correlations.\nExercise\nlinear model strong negative relationship correlation -0.97, much variation response explained explanatory variable?98Note one components summary(lm()) function adj.r.squared. value \\(R\\)-squared adjusted number predictors. idea covered depth machine learning course.","code":"\nsummary(star_mod)$r.squared## [1] 0.4556237\nstarbucks %>%\n  summarize(correlation=cor(carb,calories),correlation_squared=correlation^2)## # A tibble: 1 x 2\n##   correlation correlation_squared\n##         <dbl>               <dbl>\n## 1       0.675               0.456"},{"path":"LRDIAG.html","id":"f-statistic","chapter":"29 Regression Diagnostics","heading":"29.3.3 F-Statistic","text":"Another quantity appears summary model \\(F\\)-statistic. value evaluates null hypothesis non-intercept coefficients equal 0. Rejecting hypothesis implies model useful sense least one predictors shares significant linear relationship response.\\(H_0\\): \\(\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\)\\(H_a\\): least one coefficient equal 0.\\(p\\) number predictors model. Just like ANOVA, simultaneous test coefficients inform us one(s) different 0.\\(F\\)-statistic given \n\\[\n{n-p-1 \\p}{\\sum (\\hat{y}_i-\\bar{y})^2\\\\sum e_i^2}\n\\]null hypothesis, \\(F\\)-statistic follows \\(F\\) distribution parameters \\(p\\) \\(n-p-1\\).example, \\(F\\)-statistic redundant since one predictor. fact, \\(p\\)-value associated \\(F\\)-statistic equal \\(p\\)-value associated estimate \\(\\beta_1\\). However, move cases predictor variables, may interested \\(F\\)-statistic.","code":"\nsummary(star_mod)$fstatistic##    value    numdf    dendf \n## 62.77234  1.00000 75.00000"},{"path":"LRDIAG.html","id":"regression-diagnostics","chapter":"29 Regression Diagnostics","heading":"29.4 Regression diagnostics","text":"Finally, can use lm object check assumptions model. discussed assumptions chapter use R generate visual checks. also numeric diagnostic measures.several potential problems regression model:Assumptions error structure. assume:errors normally distributedthe errors independentthe errors constant variance, homoskedasticAssumptions fit. assume fit model correct. simple linear regression, means fit specified formula lm correct.Assumptions fit. assume fit model correct. simple linear regression, means fit specified formula lm correct.Problems outliers leverage points. case small number points data unusually large impact parameter estimates. points may give mistaken sense model great fit conversely relationship variables.Problems outliers leverage points. case small number points data unusually large impact parameter estimates. points may give mistaken sense model great fit conversely relationship variables.Missing predictors. can potentially improve fit predictive performance model including predictors. spend one chapter topic, machine learning courses devote time discussing build complex models. case multivariate linear regression, many diagnostic tools discussed next also applicable.Missing predictors. can potentially improve fit predictive performance model including predictors. spend one chapter topic, machine learning courses devote time discussing build complex models. case multivariate linear regression, many diagnostic tools discussed next also applicable.","code":""},{"path":"LRDIAG.html","id":"residual-plots-1","chapter":"29 Regression Diagnostics","heading":"29.4.1 Residual plots","text":"assumptions error structure can checked residual plots. already done , let’s review provide little depth.Applying plot() function “lm” object provides several graphs allow us visually evaluate linear model’s assumptions. actually six plots (selected option) available:plot residuals fitted values,Scale-Location plot \\(\\sqrt(| \\text{residuals} |)\\) fitted values,Normal Q-Q plot,plot Cook’s distances versus row labels,plot residuals leverages,plot Cook’s distances leverage/(1-leverage).default, first three fifth provided applying plot() “lm” object. obtain four , simply use plot(.model) command line, Figure 29.2.\nFigure 29.2: Regression diagnostic plots.\nHowever, ’s best walk four plots Starbucks example.","code":"\nplot(star_mod)"},{"path":"LRDIAG.html","id":"residuals-vs-fitted","chapter":"29 Regression Diagnostics","heading":"29.4.2 Residuals vs Fitted","text":"providing number option, can select plot want, Figure 29.3 first diagnostic plot.\nFigure 29.3: diagnostic residual plot.\nplot assesses linearity model homoscedasticity (constant variance). red line smoothed estimate fitted values versus residuals. Ideally, red line coincide dashed horizontal line residuals centered around dashed line. indicate linear fit appropriate. Furthermore, scatter around dashed line relatively constant across plot, homoscedasticity. case, looks like minor concern linearity non-constant error variance. noted earlier cluster points lower left hand corner scatterplot.Note: points labeled points high residual value. extreme. discuss outliers leverage points shortly.","code":"\nplot(star_mod,which = 1)"},{"path":"LRDIAG.html","id":"normal-q-q-plot","chapter":"29 Regression Diagnostics","heading":"29.4.3 Normal Q-Q Plot","text":"’s name suggests, plot evaluates normality residuals. seen used plot several times book. Remember number data points small, plot greatly reduced effectiveness.\nFigure 29.4: quantile-quantile plot checking normality.\nAlong \\(y\\)-axis actual standardized residuals. Along \\(x\\)-axis points residuals actually normally distributed. Ideally, dots fall along diagonal dashed line. Figure 29.4, appears skewness right just longer tails normal distribution. can tell smaller residuals, don’t increase match normal distribution, points line. concerning.","code":"\nplot(star_mod,which = 2)"},{"path":"LRDIAG.html","id":"scale-location-plot","chapter":"29 Regression Diagnostics","heading":"29.4.4 Scale-Location Plot","text":"scale-location plot better indicator non-constant error variance. plot fitted values versus square root absolute value standardized residuals. standardized residual residual divided standard deviation\\[\ne^{'}_i=\\frac{e_i}{s}\n\\]plot illustrates spread residuals entire range predictor. using fitted values generalize well one predictor.\nFigure 29.5: scale-location diagnostic residual plot.\nstraight horizontal red line indicates constant error variance. case, Figure 29.5, indication error variance higher lower carb counts.","code":"\nplot(star_mod,which=3)"},{"path":"LRDIAG.html","id":"outliers-and-leverage","chapter":"29 Regression Diagnostics","heading":"29.5 Outliers and leverage","text":"discussing last plot, need spend time discussing outliers. Outliers regression observations fall far “cloud” points. points especially important can strong influence least squares line.regression, two types outliers:\n- outlier response variable one predicted well model. either problem data model. residuals outlier large absolute value.\n- outlier explanatory variable. typically called leverage points can undue impact parameter estimates. multiple predictors, can leverage point unusual combination predictors.outlier influential point drastically alters regression output. example causing large changes estimated slope hypothesis p-values, omitted.Exercise:\nsix plots shown Figure 29.6 along least squares line residual plots. scatterplot residual plot pair, identify obvious outliers note influence least squares line. Recall outlier point doesn’t appear belong vast majority points.\nFigure 29.6: Examples outliers leverage points.\none outlier far points, though appears slightly influence line. outlier response large residual magnitude.one outlier right, though quite close least squares line, suggests wasn’t influential although leverage point.one point far away cloud, leverage point appears pull least squares line right; examine line around primary cloud doesn’t appear fit well. point high influence estimated slope.primary cloud small secondary cloud four outliers. secondary cloud appears influencing line somewhat strongly, making least square line fit poorly almost everywhere. might interesting explanation dual clouds, something investigated.obvious trend main cloud points outlier right appears largely control slope least squares line. point outlier response predictor. highly influential point.one outlier response predictor, thus leverage point, far cloud, however, falls quite close least squares line appear influential.Examining residual plots Figure 29.6, probably find trend main clouds (3) (4). cases, outliers influenced slope least squares lines. (5), data clear trend assigned line large trend simply due one outlier!Leverage\nPoints fall horizontally away center cloud tend pull harder line, call points high leverage.Points fall horizontally far line points high leverage; points can strongly influence slope least squares line. one high leverage points appear actually invoke influence slope line – cases (3), (4), (5) – call influential point. Usually can say point influential , fitted line without , influential point unusually far least squares line. Leverage can calculated called hat matrix, actual mathematics beyond scope book.point can outlier leverage point already discussed. tempting remove outliers data set. Don’t without good reason. Models ignore exceptional (interesting) cases often perform poorly. instance, financial firm ignored largest market swings – ``outliers’’ – soon go bankrupt making poorly thought-investments.","code":""},{"path":"LRDIAG.html","id":"residuals-vs-leverage-plot","chapter":"29 Regression Diagnostics","heading":"29.5.1 Residuals vs Leverage Plot","text":"residuals vs leverage plot good way identify influential observations. Sometimes, influential observations representative population, also indicate error recording data, otherwise unrepresentative outlier. worth looking cases.\nFigure 29.7: Diagnostic plots Starbucks regression model.\nFigure 29.7 helps us find influential cases, leverage points impact estimated slope. Unlike plots, patterns relevant. watch outlying values upper right corner lower right corner. spots places cases can influential regression line. Look cases outside dashed line, Cook’s distance. particular plot, dotted line Cook’s distance outside bounds plot thus come play. cases outside Cook’s distance (meaning high Cook’s distance scores), cases influential regression results. regression results altered exclude cases. example, points tend undue influence.","code":"\nplot(star_mod,5)"},{"path":"LRDIAG.html","id":"what-if-our-assumptions-are-violated","chapter":"29 Regression Diagnostics","heading":"29.5.2 What If Our Assumptions Are Violated","text":"assumptions model violated /influential points, linear regression model normality assumptions appropriate. Sometimes appropriate transform data (either response predictor), assumptions met transformed data. times, appropriate explore models. entire courses regression blocks material devoted diagnostics transformations reduce impact violations assumptions. go methods book. Instead, confronted clear violated assumptions, use resampling possible solution. learn next chapter assume normality residuals. limited solution, introductory text, excellent first step.","code":""},{"path":"LRDIAG.html","id":"homework-problems-28","chapter":"29 Regression Diagnostics","heading":"29.6 Homework Problems","text":"Identify relationshipsFor six plots Figure 29.8, identify strength relationship (e.g. weak, moderate, strong) data whether fitting linear model reasonable. ask strength relationship, mean:relationship \\(x\\) \\(y\\) anddoes relationship explain variance?\nFigure 29.8: Homework problem 1.\nBeer blood alcohol contentWe use blood alcohol content data . reminder description data: Many people believe gender, weight, drinking habits, many factors much important predicting blood alcohol content (BAC) simply considering number drinks person consumed. examine data sixteen student volunteers Ohio State University drank randomly assigned number cans beer. students evenly divided men women, differed weight drinking habits. Thirty minutes later, police officer measured blood alcohol content (BAC) grams alcohol per deciliter blood.data bac.csv file data folder.Obtain interpret \\(R\\)-squared model.Evaluate assumptions model. anything concerned ?OutliersIdentify outliers scatterplots shown Figure 29.9 determine type outliers . Explain reasoning.\nFigure 29.9: Homework problem 3.\n","code":""},{"path":"LRSIM.html","id":"LRSIM","chapter":"30 Simulation Based Linear Regression","heading":"30 Simulation Based Linear Regression","text":"","code":""},{"path":"LRSIM.html","id":"objectives-29","chapter":"30 Simulation Based Linear Regression","heading":"30.1 Objectives","text":"Using bootstrap, generate confidence intervals estimates standard error parameter estimates linear regression model.Generate interpret bootstrap confidence intervals predicted values.Generate bootstrap samples sampling rows data sampling residuals. Explain might prefer one .Interpret regression coefficients linear model categorical explanatory variable.","code":""},{"path":"LRSIM.html","id":"introduction-5","chapter":"30 Simulation Based Linear Regression","heading":"30.2 Introduction","text":"last couple chapters examined perform inference simple linear regression model assuming errors independent normally distributed random variables. examined diagnostic tools check assumptions look outliers. chapter use bootstrap create confidence prediction intervals.least two ways can consider creating bootstrap distribution linear model. can easily fit linear model resampled data set. situations may undesirable features. Influential observations, example, appear duplicated resamples missing entirely resamples.Another option use “residual resampling”. residual resampling, new data set predictor values original data set new response created adding fitted function resampled residual.summary, suppose \\(n\\) observations, \\(Y\\) number \\(X\\)’s, observation stored row data set. two basic procedures bootstrapping regression :\n. bootstrap observations, \nb. bootstrap residuals.\nlatter special case general rule: sample \\(Y\\) estimated conditional distribution given \\(X\\).bootstrapping observations, sample replacement rows data; \\(Y\\) comes corresponding \\(X\\)’s. bootstrap sample observations may repeated multiple times, others included. idea used used bootstrap hypothesis testing.bootstrapping residuals, fit regression model, compute predicted values \\(\\hat{Y}_i\\) residuals \\(e_i = Y_i - \\hat{Y}_i\\), create bootstrap sample using \\(X\\) values original data, new \\(Y\\) values obtained using prediction plus random residual, \\(Y^{*}_i = \\hat{Y}_i + e^{*}_i\\), residuals \\(e^{*}_i\\) sampled randomly replacement original residuals. still chance selecting large residual outlier, paired \\(x\\) value near \\(\\bar{x}\\), little leverage.Bootstrapping residuals corresponds designed experiment, \\(x\\) values fixed \\(Y\\) random. bootstrap observations, essentially \\(X\\) \\(Y\\) sampled. principle sampling way data drawn, second method implies \\(Y\\) \\(X\\) random.","code":""},{"path":"LRSIM.html","id":"confidence-intervals-for-parameters","chapter":"30 Simulation Based Linear Regression","heading":"30.3 Confidence intervals for parameters","text":"build confidence interval slope parameter, resample data residuals generate new regression model. process assume normality residuals. use functions mosaic package complete work. However, know tidymodels purrr sophisticated tools work.","code":""},{"path":"LRSIM.html","id":"resampling","chapter":"30 Simulation Based Linear Regression","heading":"30.3.1 Resampling","text":"make ideas salient, let’s use Starbucks example .First read data R:Build model:Let’s see output model:preparation resampling, let’s see () treats linear model object.Nice. resample data use () resample(). sample rows, referring first method.Perfect, ready scale .Now let’s look first 6 rows results.plot slopes, red lines Figure 30.1, get sense variability estimated slope intercept. also gives us idea width confidence interval estimated mean response. plotted confidence interval gray shade can see matches red shaded region bootstrap slopes. can see confidence interval wider extreme values predictor.\nFigure 30.1: Plot slopes resampled regression.\ndata results, can generate confidence intervals slope, \\(R\\)-squared (\\(R^2\\)), \\(F\\) statistic. Figure 30.2 histogram slope values resampling.\nFigure 30.2: Histogram slopes resampled regression.\nconfidence interval found using cdata().95% confident true slope 3.17 5.37. reminder, using normality assumption 95% confidence interval \\((3.21,5.38)\\):bootstrap confidence interval \\(R^2\\) :bootstrap sampling distribution \\(R^2\\) displayed Figure 30.3.\nFigure 30.3: histogram \\(R^2\\) values resampled regression.\nnice work. powerful.Let’s see accomplish work using infer package.check can use package, let’s find slope estimate.Good, let’s get bootstrap sampling distribution regression slope.Next confidence interval.matches work already done. Finally, let’s visualize results, Figure 30.4.\nFigure 30.4: Sampling distribution slope using resampling. (Black line estimate slope original data blue lines confidence bounds.)\n","code":"\nstarbucks <- read_csv(\"data/starbucks.csv\")\nstar_mod <- lm(calories~carb,data=starbucks)\nsummary(star_mod)## \n## Call:\n## lm(formula = calories ~ carb, data = starbucks)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -151.962  -70.556   -0.636   54.908  179.444 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 146.0204    25.9186   5.634 2.93e-07 ***\n## carb          4.2971     0.5424   7.923 1.67e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 78.26 on 75 degrees of freedom\n## Multiple R-squared:  0.4556, Adjusted R-squared:  0.4484 \n## F-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11\nset.seed(401)\nobs<-do(1)*star_mod\nobs##   Intercept     carb    sigma r.squared        F numdf dendf .row .index\n## 1  146.0204 4.297084 78.25956 0.4556237 62.77234     1    75    1      1\ndo(2)*lm(calories~carb,data=resample(starbucks))##   Intercept     carb    sigma r.squared        F numdf dendf .row .index\n## 1  145.6345 4.089065 73.32243 0.4980692 74.42299     1    75    1      1\n## 2  160.0193 3.828742 66.81016 0.4298148 56.53621     1    75    1      2\nset.seed(532)\nresults <- do(1000)*lm(calories~carb,data=resample(starbucks))\nhead(results)##   Intercept     carb    sigma r.squared        F numdf dendf .row .index\n## 1  154.7670 4.176327 78.94717 0.4127581 52.71568     1    75    1      1\n## 2  166.8589 3.807697 72.09482 0.4032196 50.67437     1    75    1      2\n## 3  105.3658 4.899956 77.62517 0.5310212 84.92195     1    75    1      3\n## 4  227.4138 2.805156 79.97902 0.2467094 24.56317     1    75    1      4\n## 5  194.9190 3.457191 83.74624 0.2670279 27.32313     1    75    1      5\n## 6  183.1159 3.549460 73.90153 0.3931691 48.59292     1    75    1      6\nggplot(starbucks, aes(x=carb, y=calories)) +\n  geom_abline(data = results,\n              aes(slope =  carb, intercept = Intercept), \n              alpha = 0.01,color=\"red\") +\n  geom_point() +\n  theme_classic() +\n  labs(x=\"Carbohydrates (g)\",y=\"Calories\",title=\"Bootstrap Slopes\",subtitle =\"1000 Slopes\") +\n  geom_lm(interval=\"confidence\")\nresults %>%\n  gf_histogram(~carb,fill=\"cyan\",color = \"black\") %>%\n  gf_vline(xintercept = obs$carb,color=\"red\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Carbohydrate regression slope.\",y=\"\")\ncdata(~carb,data=results,p=0.95)##         lower    upper central.p\n## 2.5% 3.166546 5.377743      0.95\nconfint(star_mod)##                 2.5 %     97.5 %\n## (Intercept) 94.387896 197.652967\n## carb         3.216643   5.377526\ncdata(~r.squared,data=results)##          lower     upper central.p\n## 2.5% 0.2837033 0.6234751      0.95\nresults %>%\n  gf_histogram(~r.squared,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept = obs$r.squared,color=\"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(y=\"\",x=expression(R^2))\nlibrary(infer)\nslope_estimate <- starbucks %>%\n  specify(calories ~ carb) %>%\n  calculate(stat=\"slope\")\nslope_estimate## Response: calories (numeric)\n## Explanatory: carb (numeric)\n## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1  4.30\nresults2 <- starbucks %>%\n  specify(calories~carb) %>%\n  generate(reps=1000,type=\"bootstrap\") %>%\n  calculate(stat=\"slope\")\nhead(results2)## Response: calories (numeric)\n## Explanatory: carb (numeric)\n## # A tibble: 6 x 2\n##   replicate  stat\n##       <int> <dbl>\n## 1         1  3.75\n## 2         2  5.43\n## 3         3  3.76\n## 4         4  4.69\n## 5         5  4.38\n## 6         6  3.63\nslope_ci<-results2 %>%\n  get_confidence_interval(level=0.95)\nslope_ci## # A tibble: 1 x 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1     3.14     5.26\nresults2 %>%\n  visualize() +\n  shade_confidence_interval(slope_ci,color=\"blue\",fill=\"lightblue\") +\n  geom_vline(xintercept = slope_estimate$stat,color=\"black\",size=2) +\n  labs(x=\"Estimated Slope\") +\n  theme_bw()"},{"path":"LRSIM.html","id":"resample-residuals","chapter":"30 Simulation Based Linear Regression","heading":"30.3.2 Resample residuals","text":"also resample residuals instead data. makes stronger assumption linear model appropriate. However, guarantees every \\(X\\) value resample data frame. lm function, send model instead data resample residuals. Since R object oriented programming language, sending model object resample() function, code automatically resample residuals.Next plot bootstrap sampling distribution, Figure 30.5.\nFigure 30.5: Histogram estimated regression slope using resampling residuals.\nfinally confidence interval slope.Similar previous bootstrap confidence interval just little narrower.","code":"\nresults_resid <- do(1000)*lm( calories~carb, data = resample(star_mod)) # resampled residuals\nhead(results_resid)##   Intercept     carb    sigma r.squared        F numdf dendf .row .index\n## 1  151.9999 4.356740 73.07024 0.4967052 74.01804     1    75    1      1\n## 2  101.6226 5.280410 82.24346 0.5336627 85.82779     1    75    1      2\n## 3  152.4453 4.346918 82.64249 0.4344055 57.60383     1    75    1      3\n## 4  159.1311 3.846912 84.42784 0.3656236 43.22634     1    75    1      4\n## 5  167.9957 3.981328 67.50240 0.4912807 72.42905     1    75    1      5\n## 6  198.5458 3.239953 86.11143 0.2821237 29.47482     1    75    1      6\nresults_resid %>%\n  gf_histogram(~carb,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept = obs$carb,color=\"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Estimated slope of carbs\",y=\"\")\ncdata(~carb,data=results_resid)##        lower    upper central.p\n## 2.5% 3.24622 5.323031      0.95"},{"path":"LRSIM.html","id":"confidence-intervals-for-prediction","chapter":"30 Simulation Based Linear Regression","heading":"30.4 Confidence intervals for prediction","text":"now want generate confidence interval average calories 60 grams carbohydrates.Using normal assumption, hadWe bootstrap slope intercept estimates results object. can use tidyverse functions find confidence interval predicting response slope intercept estimate.similar interval found last chapter. 95% confident average calorie content menu item 60 grams carbohydrates 380.8 425.7.","code":"\npredict(star_mod,newdata = data.frame(carb=60),interval=\"confidence\")##        fit      lwr      upr\n## 1 403.8455 379.7027 427.9883\nhead(results)##   Intercept     carb    sigma r.squared        F numdf dendf .row .index\n## 1  154.7670 4.176327 78.94717 0.4127581 52.71568     1    75    1      1\n## 2  166.8589 3.807697 72.09482 0.4032196 50.67437     1    75    1      2\n## 3  105.3658 4.899956 77.62517 0.5310212 84.92195     1    75    1      3\n## 4  227.4138 2.805156 79.97902 0.2467094 24.56317     1    75    1      4\n## 5  194.9190 3.457191 83.74624 0.2670279 27.32313     1    75    1      5\n## 6  183.1159 3.549460 73.90153 0.3931691 48.59292     1    75    1      6\nresults %>%\n  mutate(pred=Intercept+carb*60) %>%\n  cdata(~pred,data=.)##         lower    upper central.p\n## 2.5% 385.2706 423.6689      0.95"},{"path":"LRSIM.html","id":"prediction-interval","chapter":"30 Simulation Based Linear Regression","heading":"30.4.1 Prediction interval","text":"prediction interval difficult perform bootstrap. account variability slope also residual variability since individual observation. can’t just add residual predicted value. Remember variance sum independent variables sum variances. standard deviations can’t just add .Let’s look happen try. First reminder, prediction interval 60 grams carb using assumption normally distributed errors last lesson :generating bootstrap size 1000, resample residuals 1000 times.prediction interval appears biased. Thus generating prediction interval beyond scope book.","code":"\npredict(star_mod,newdata = data.frame(carb=60),interval=\"prediction\")##        fit      lwr      upr\n## 1 403.8455 246.0862 561.6048\nresults %>%\n  mutate(pred=Intercept+carb*60) %>% \n  cbind(resid=sample(star_mod$residuals,size=1000,replace = TRUE)) %>%\n  mutate(pred_ind=pred+resid) %>%\n  cdata(~pred_ind,data=.)##         lower    upper central.p\n## 2.5% 277.4886 577.0957      0.95"},{"path":"LRSIM.html","id":"categorical-predictor","chapter":"30 Simulation Based Linear Regression","heading":"30.5 Categorical predictor","text":"want finish simple linear regression discussing categorical predictor. somewhat changes interpretation regression model.Thus far, discussed regression context quantitative, continuous, response quantitative, continuous, predictor. can build linear models categorical predictor variables well.case binary covariate, nothing linear model changes. two levels binary covariate typically coded 1 0, model built, evaluated interpreted analogous fashion . difference continuous predictor categorical two values predictor can take regression model simply predict average value response within value predictor.case categorical covariate \\(k\\) levels, \\(k>2\\), need include \\(k-1\\) dummy variables model. dummy variables takes value 0 1. example, covariate \\(k=3\\) categories levels (say , B C), create two dummy variables, \\(X_1\\) \\(X_2\\), can take values 1 0. arbitrarily state \\(X_1=1\\), represents covariate value . Likewise \\(X_2=1\\), state covariate takes value B. \\(X_1=0\\) \\(X_2=0\\), known reference category, case covariate takes value C. arrangement levels categorical covariate arbitrary can adjusted user. coding covariate dummy variables called contrasts typically taught advanced course linear models.case \\(k=3\\), linear model \\(Y=\\beta_0 + \\beta_1X_1 + \\beta_2X_2+e\\).covariate takes value , \\(\\mbox{E}(Y|X=)=\\beta_0 + \\beta_1\\).covariate takes value B, \\(\\mbox{E}(Y|X=B)=\\beta_0 + \\beta_2\\).covariate takes value C, \\(\\mbox{E}(Y|X=C)=\\beta_0\\).Based , think interpret coefficients \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\).","code":""},{"path":"LRSIM.html","id":"lending-club","chapter":"30 Simulation Based Linear Regression","heading":"30.5.1 Lending Club","text":"Let’s example data.Lending Club data set represents thousands loans made Lending Club platform, platform allows individuals lend individuals. course, loans created equal. Someone essentially sure bet pay back loan easier time getting loan low interest rate someone appears riskier. people risky? may even get loan offer, may accepted loan offer due high interest rate. important keep last part mind, since data set represents loans actually made, .e. mistake data loan applications! data set loans.csv data folder.Let’s look size data:big data set. educational purposes, sample 100 points original data. need variables interest_rate homeownership. First let’s break homeownership variable.want sample data level home ownership proportion original, stratified sample.quite 100 observations, preserved proportion homeownership.Let’s look data boxplot, Figure 30.6.\nFigure 30.6: Boxplot loan interest rates Lending Club based homeownership status.\nappears evidence home ownership impacts interest rate. can build linear model explore whether difference significant. can use lm() function R, order include categorical predictor, need make sure variable stored “factor” type. , ’ll need convert .Now can build model:Note default, R set MORTGAGE level reference category. first value sorted alphabetically. can control changing order factor levels. package forcats helps effort.interpret output? Since MORTGAGE reference category, intercept effectively estimated, average, interest rate home owners mortgage.terms represent expected difference average interest rates ownership types.Specifically, average, loan interest rates home owners home 2.61 percentage points higher mortgage. rent 2.36 percent higher average. highest interest rate home. seems odd interesting.Exercise:\nUsing coefficient regression model, find difference average interest rates home owners renters?first coefficient\n\\[\\beta_\\text{homeownershipOWN} = \\mu_\\text{} - \\mu_\\text{MORTGAGE}\\]\n\\[\\beta_\\text{homeownershipRENT} = \\mu_\\text{RENT} - \\mu_\\text{MORTGAGE}.\\]\nThus \\[\\mu_\\text{} -\\mu_\\text{RENT} = \\beta_\\text{homeownershipOWN} - \\beta_\\text{homeownershipRENT},\\] difference coefficients.model fitting line data just estimating average coefficients representing difference reference level.Std.Error, t value, Pr(>|t|) values can used conduct inference respective estimates. p-values significant. similar ANOVA analysis conducted last block except hypothesis test simultaneously testing coefficients testing pairwise.","code":"\nloans <- read_csv(\"data/loans.csv\")\ndim(loans)## [1] 10000    55\ntally(~homeownership,data=loans,format=\"proportion\")## homeownership\n## MORTGAGE      OWN     RENT \n##   0.4789   0.1353   0.3858\nset.seed(905)\nloans100 <- loans %>%\n  select(interest_rate,homeownership) %>%\n  mutate(homeownership=factor(homeownership)) %>%\n  group_by(homeownership) %>%\n  slice_sample(prop=0.01) %>%\n  ungroup()\ndim(loans100)## [1] 98  2\ntally(~homeownership,data=loans100,format=\"proportion\")## homeownership\n##  MORTGAGE       OWN      RENT \n## 0.4795918 0.1326531 0.3877551\nloans100 %>%\n  gf_boxplot(interest_rate~homeownership) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Lending Club\",x=\"Homeownership\",y=\"Interest Rate\")\nstr(loans100)## tibble [98 x 2] (S3: tbl_df/tbl/data.frame)\n##  $ interest_rate: num [1:98] 19.03 9.44 6.07 7.96 10.9 ...\n##  $ homeownership: Factor w/ 3 levels \"MORTGAGE\",\"OWN\",..: 1 1 1 1 1 1 1 1 1 1 ...\nloan_mod<-lm(interest_rate ~ homeownership,data=loans100)\nsummary(loan_mod)## \n## Call:\n## lm(formula = interest_rate ~ homeownership, data = loans100)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.5442 -3.1472  0.1628  2.1228 12.9658 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        10.4972     0.5889  17.825  < 2e-16 ***\n## homeownershipOWN    2.6135     1.2652   2.066  0.04158 *  \n## homeownershipRENT   2.3570     0.8808   2.676  0.00878 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.037 on 95 degrees of freedom\n## Multiple R-squared:  0.08517,    Adjusted R-squared:  0.06591 \n## F-statistic: 4.422 on 2 and 95 DF,  p-value: 0.01458\nloans100 %>%\n  filter(homeownership == \"MORTGAGE\") %>%\n  summarise(average=mean(interest_rate))## # A tibble: 1 x 1\n##   average\n##     <dbl>\n## 1    10.5\nloans100 %>%\n  group_by(homeownership) %>%\n  summarise(average=mean(interest_rate),std_dev=sd(interest_rate))## # A tibble: 3 x 3\n##   homeownership average std_dev\n##   <fct>           <dbl>   <dbl>\n## 1 MORTGAGE         10.5    3.44\n## 2 OWN              13.1    2.89\n## 3 RENT             12.9    4.94"},{"path":"LRSIM.html","id":"bootstrap-2","chapter":"30 Simulation Based Linear Regression","heading":"30.5.2 Bootstrap","text":"boxplots, biggest difference means home owners mortgage. However, regression output p-value test difference owners renters. easy solution change reference level many levels? know ones test? next section look multiple comparisons can use bootstrap help us.Let’s bootstrap regression.course can generate confidence interval either coefficients results object.Figure 30.7 histogram estimated coefficient home.\nFigure 30.7: Distribution estimated regression coefficent homeownership.\nsimilar results assuming normality.However, want confidence interval difference home owners renters.Done! interval can infer home owners home rent significantly different interest rates.","code":"\nset.seed(532)\nresults <- do(1000)*lm(interest_rate ~ homeownership,data=resample(loans100))\nhead(results)##   Intercept homeownershipOWN homeownershipRENT    sigma  r.squared        F\n## 1   9.98300         2.088250          2.110000 3.832758 0.07223701 3.698421\n## 2  11.04875         3.868250          1.449750 3.421090 0.11065485 5.910085\n## 3  10.52865         3.200096          2.065557 3.958047 0.08231134 4.260474\n## 4  11.10000         2.572000          2.163000 4.752496 0.05494338 2.761539\n## 5  10.52939         2.459703          1.214033 4.157461 0.03970813 1.964128\n## 6  10.08280         4.100533          2.531745 3.650730 0.16435823 9.342539\n##   numdf dendf .row .index\n## 1     2    95    1      1\n## 2     2    95    1      2\n## 3     2    95    1      3\n## 4     2    95    1      4\n## 5     2    95    1      5\n## 6     2    95    1      6\nobs<-do(1)*loan_mod\nobs##   Intercept homeownershipOWN homeownershipRENT    sigma  r.squared        F\n## 1  10.49723         2.613535          2.356976 4.037396 0.08516582 4.421978\n##   numdf dendf .row .index\n## 1     2    95    1      1\nresults %>%\n  gf_histogram(~homeownershipOWN,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept = obs$homeownershipOWN,color=\"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(y=\"\",x=\"Homeownership (Own).\")## Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided.\ncdata(~homeownershipOWN,data=results)##          lower    upper central.p\n## 2.5% 0.7674164 4.489259      0.95\nconfint(loan_mod)##                       2.5 %    97.5 %\n## (Intercept)       9.3280904 11.666378\n## homeownershipOWN  0.1018118  5.125259\n## homeownershipRENT 0.6083964  4.105557\nresults %>%\n  mutate(own_rent=homeownershipOWN - homeownershipRENT) %>%\n  cdata(~own_rent,data=.)##          lower    upper central.p\n## 2.5% -1.943183 2.536296      0.95"},{"path":"LRSIM.html","id":"anova-table","chapter":"30 Simulation Based Linear Regression","heading":"30.5.3 ANOVA Table","text":"reminder, also report results loans analysis using analysis variance, ANOVA, table.table lays variation observations broken . simultaneous test equality three means. Using \\(F\\)-statistic, reject null hypothesis differences mean response across levels categorical variable. Notice p-value reported \\(F\\) distribution regression summary.","code":"\nanova(loan_mod)## Analysis of Variance Table\n## \n## Response: interest_rate\n##               Df  Sum Sq Mean Sq F value  Pr(>F)  \n## homeownership  2  144.16  72.081   4.422 0.01458 *\n## Residuals     95 1548.55  16.301                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"LRSIM.html","id":"pairwise-comparisons","chapter":"30 Simulation Based Linear Regression","heading":"30.5.4 Pairwise Comparisons","text":"ANOVA table (along summary linear model output ) merely tells whether difference exists mean response across levels categorical predictor. tell difference lies. case using regression can compare MORTGAGE two levels can’t conduct hypothesis vs RENT. order make pairwise comparisons, need another tool. common one Tukey method. Essentially, Tukey method conducts three hypothesis tests (null difference mean) corrects \\(p\\)-values based understanding conducting three simultaneous hypothesis tests set data don’t want inflate Type 1 error.can obtain pairwise comparisons using TukeyHSD() function R. “HSD” stands “Honest Significant Differences”. function requires anova object, obtained using aov() function:According output, average interest rate mortgage different renters.","code":"\nTukeyHSD(aov(interest_rate~homeownership, data=loans100))##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = interest_rate ~ homeownership, data = loans100)\n## \n## $homeownership\n##                     diff        lwr      upr     p adj\n## OWN-MORTGAGE   2.6135352 -0.3988868 5.625957 0.1025289\n## RENT-MORTGAGE  2.3569765  0.2598263 4.454127 0.0236346\n## RENT-OWN      -0.2565587 -3.3453062 2.832189 0.9786730"},{"path":"LRSIM.html","id":"assumptions-1","chapter":"30 Simulation Based Linear Regression","heading":"30.6 Assumptions","text":"Keep mind ANOVA special case simple linear model. Therefore, assumptions remain except linearity. order levels irrelevant thus line need go three levels. order evaluate assumptions, need obtain appropriate diagnostic plots.\nFigure 30.8: Q-Q normality plot.\nFigure 30.8 shows normality suspect large sample size thus get much difference results bootstrap assume normality.\nFigure 30.9: Scale-location residual diagnostic plot.\nassumption equal variance also suspect, Figure 30.9. variance homeowners might less two.\nFigure 30.10: Residual plot outliers leverage points.\nthree points might outliers extreme, Figure 30.10. general, nothing plot concerning us.","code":"\nplot(loan_mod,2)\nplot(loan_mod,3)\nplot(loan_mod,5)"},{"path":"LRSIM.html","id":"homework-problems-29","chapter":"30 Simulation Based Linear Regression","heading":"30.7 Homework Problems","text":"use loans data set create linear models. Remember data set represents thousands loans made Lending Club platform, platform allows individuals lend individuals.LoansIn exercise examine relationship interest rate loan amount.Read data loans.csv data folder.Create subset data 200 observations following three variables interest_rate, loan_amount, term. Change term factor use stratified sample keep proportion loan terms roughly original data.Plot interest_rate versus loan_amount. think interest_rate response.Fit linear model data regressing interest_rate loan_amount. significant relationship interest_rate loan_amount?Using \\(t\\) distribution:\nFind 95% confidence interval slope.\nFind interpret 90% confidence interval loan amount $20000\nFind 95% confidence interval slope.Find interpret 90% confidence interval loan amount $20000Repeat part e using bootstrap.Check assumptions linear regression.Loans IIUsing loans data set 200 observations previous exercise, use variable term determine difference interest rates two different loan lengths.Build set side--side boxplots summarize interest rate term. Describe relationship see. Note: convert term variable factor prior continuing.Build linear model fitting interest rate term. appear significant difference mean interest rates term?Write estimated linear model. words, interpret coefficient estimate.Construct bootstrap confidence interval coefficient.Check model assumptions.","code":""},{"path":"LRMULTI.html","id":"LRMULTI","chapter":"31 Multiple Linear Regression","heading":"31 Multiple Linear Regression","text":"","code":""},{"path":"LRMULTI.html","id":"objectives-30","chapter":"31 Multiple Linear Regression","heading":"31.1 Objectives","text":"Create interpret model multiple predictors check assumptions.Generate interpret confidence intervals estimates.Explain adjusted \\(R^2\\) multi-collinearity.Interpret regression coefficients linear model multiple predictors.Build interpret models higher order terms.","code":""},{"path":"LRMULTI.html","id":"introduction-to-multiple-regression","chapter":"31 Multiple Linear Regression","heading":"31.2 Introduction to multiple regression","text":"principles simple linear regression lay foundation sophisticated regression methods used wide range challenging settings. last two chapters, explore multiple regression, introduces possibility one predictor.","code":""},{"path":"LRMULTI.html","id":"multiple-regression","chapter":"31 Multiple Linear Regression","heading":"31.3 Multiple regression","text":"Multiple regression extends simple two-variable regression case still one response many predictors (denoted \\(x_1\\), \\(x_2\\), \\(x_3\\), …). method motivated scenarios many variables may simultaneously connected output.explore explain ideas, consider Ebay auctions video game called Mario Kart Nintendo Wii. outcome variable interest total price auction, highest bid plus shipping cost. try determine total price related characteristic auction simultaneously controlling variables. instance, characteristics held constant, longer auctions associated higher lower prices? , average, much buyers tend pay additional Wii wheels (plastic steering wheels attach Wii controller) auctions? Multiple regression help us answer questions.data set file mariokart.csv data folder. data set includes results 141 auctions.99 Ten observations data set shown R code . Note force first column interpreted character string since identification code sale numeric meaning. Just case simple linear regression, multiple regression also allows categorical variables many levels. Although type variable data set, leave discussion types variables multiple regression advanced regression machine learning courses.interested total_pr, cond, stock_photo, duration, wheels. variables described following list:total_pr: final auction price plus shipping costs, US dollarscond: two-level categorical factor variablestock_photo: two-level categorical factor variableduration: length auction, days, taking values 1 10wheels: number Wii wheels included auction (Wii wheel plastic racing wheel holds Wii controller optional helpful accessory playing Mario Kart)","code":"\nmariokart <-read_csv(\"data/mariokart.csv\", col_types = list(col_character()))\nhead(mariokart,n=10)## # A tibble: 10 x 12\n##    id        duration n_bids cond  start_pr ship_pr total_pr ship_sp seller_rate\n##    <chr>        <dbl>  <dbl> <chr>    <dbl>   <dbl>    <dbl> <chr>         <dbl>\n##  1 15037742~        3     20 new       0.99    4        51.6 standa~        1580\n##  2 26048337~        7     13 used      0.99    3.99     37.0 firstC~         365\n##  3 32043234~        3     16 new       0.99    3.5      45.5 firstC~         998\n##  4 28040522~        3     18 new       0.99    0        44   standa~           7\n##  5 17039222~        1     20 new       0.01    0        71   media           820\n##  6 36019515~        3     19 new       0.99    4        45   standa~      270144\n##  7 12047772~        1     13 used      0.01    0        37.0 standa~        7284\n##  8 30035550~        1     15 new       1       2.99     54.0 upsGro~        4858\n##  9 20039206~        3     29 used      0.99    4        47   priori~          27\n## 10 33036416~        7      8 used     20.0     4        50   firstC~         201\n## # ... with 3 more variables: stock_photo <chr>, wheels <dbl>, title <chr>"},{"path":"LRMULTI.html","id":"a-single-variable-model-for-the-mario-kart-data","chapter":"31 Multiple Linear Regression","heading":"31.3.1 A single-variable model for the Mario Kart data","text":"Let’s fit linear regression model game’s condition predictor auction price. start let’s change cond stock_photo factors.Next let’s summarize data.Finally, let’s plot data.\nFigure 31.1: Total price Mario Kart Ebay condition.\nseveral outliers may impact analysis, Figure 31.1.Now let’s build model.model may written \\[\n\\hat{\\text{totalprice}} = 53.771 - 6.623 \\times \\text{condused}\n\\]scatterplot price versus game condition shown Figure 31.2. Since predictor binary, scatterplot appropriate look reference.\nFigure 31.2: Scatterplot total price Mario Kart Ebay versus condition.\nlargest outlier probably significantly impacting relationship model. find mean median two groups, see .appears used items right skewed distribution average higher least one outliers.least two outliers plot. Let’s gather information .look variable title additional items sale two observations. Let’s remove two outliers run model . Note reason removing annoying us messing model. don’t think representative population interest. Figure 31.3 boxplot data outliers dropped.\nFigure 31.3: Boxplot total price condition outliers removed.\nNotice much residual standard error decreased likewise \\(R\\)-squared increased.model may written :\\[\n\\hat{total price} = 53.771 - 10.90 \\times condused\n\\]Now see average price used items $10.90 less average new items.Exercise:\nlinear model seem reasonable? assumptions check?model seem reasonable sense assumptions errors plausible. residuals indicate skewness right may driven predominantly skewness new items, Figure 31.4.\nFigure 31.4: Check normality using quantile-quantile plot.\nnormality assumption somewhat suspect 100 data points short tails distribution concern. shape curve indicates positive skew.\nFigure 31.5: Residual plot assess equal variance assumption.\nFigure 31.5, equal variance seems reasonable.\nFigure 31.6: Residual plot checking leverage points.\nhigh leverage points, Figure 31.6.need check linearity, two different values explanatory variable.Example: Interpretation\nInterpret coefficient game’s condition model. coefficient significantly different 0?Note cond two-level categorical variable reference level new. - 10.90 means model predicts extra $10.90 average games new versus used. Examining regression output, can see p-value cond close zero, indicating strong evidence coefficient different zero using simple one-variable model.","code":"\nmariokart <- mariokart %>%\n  mutate(cond=factor(cond),stock_photo=factor(stock_photo))\ninspect(mariokart)## \n## categorical variables:  \n##          name     class levels   n missing\n## 1          id character    143 143       0\n## 2        cond    factor      2 143       0\n## 3     ship_sp character      8 143       0\n## 4 stock_photo    factor      2 143       0\n## 5       title character     80 142       1\n##                                    distribution\n## 1 110439174663 (0.7%) ...                      \n## 2 used (58.7%), new (41.3%)                    \n## 3 standard (23.1%), upsGround (21.7%) ...      \n## 4 yes (73.4%), no (26.6%)                      \n## 5  (%) ...                                     \n## \n## quantitative variables:  \n##             name   class   min      Q1 median      Q3       max         mean\n## ...1    duration numeric  1.00   1.000    3.0    7.00     10.00     3.769231\n## ...2      n_bids numeric  1.00  10.000   14.0   17.00     29.00    13.538462\n## ...3    start_pr numeric  0.01   0.990    1.0   10.00     69.95     8.777203\n## ...4     ship_pr numeric  0.00   0.000    3.0    4.00     25.51     3.143706\n## ...5    total_pr numeric 28.98  41.175   46.5   53.99    326.51    49.880490\n## ...6 seller_rate numeric  0.00 109.000  820.0 4858.00 270144.00 15898.419580\n## ...7      wheels numeric  0.00   0.000    1.0    2.00      4.00     1.146853\n##                sd   n missing\n## ...1 2.585693e+00 143       0\n## ...2 5.878786e+00 143       0\n## ...3 1.506745e+01 143       0\n## ...4 3.213179e+00 143       0\n## ...5 2.568856e+01 143       0\n## ...6 5.184032e+04 143       0\n## ...7 8.471829e-01 143       0\nmariokart %>% \n  gf_boxplot(total_pr~cond) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title=\"Ebay Auction Prices\",x=\"Condition\", y=\"Total Price\")\nmario_mod <- lm(total_pr~cond,data=mariokart)\nsummary(mario_mod)## \n## Call:\n## lm(formula = total_pr ~ cond, data = mariokart)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.168  -7.771  -3.148   1.857 279.362 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   53.771      3.329  16.153   <2e-16 ***\n## condused      -6.623      4.343  -1.525     0.13    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 25.57 on 141 degrees of freedom\n## Multiple R-squared:  0.01622,    Adjusted R-squared:  0.009244 \n## F-statistic: 2.325 on 1 and 141 DF,  p-value: 0.1296\nmariokart %>%\n  gf_point(total_pr~cond) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Ebay Auction Prices\",x=\"Condition\", y=\"Total Price\")\nmariokart %>%\n  group_by(cond) %>%\n  summarize(xbar=mean(total_pr), stand_dev=sd(total_pr),xmedian=median(total_pr))## # A tibble: 2 x 4\n##   cond   xbar stand_dev xmedian\n##   <fct> <dbl>     <dbl>   <dbl>\n## 1 new    53.8      7.44    54.0\n## 2 used   47.1     32.7     42.8\nmariokart %>%\n  filter(total_pr > 100)## # A tibble: 2 x 12\n##   id         duration n_bids cond  start_pr ship_pr total_pr ship_sp seller_rate\n##   <chr>         <dbl>  <dbl> <fct>    <dbl>   <dbl>    <dbl> <chr>         <dbl>\n## 1 110439174~        7     22 used      1       25.5     327. parcel          115\n## 2 130335427~        3     27 used      6.95     4       118. parcel           41\n## # ... with 3 more variables: stock_photo <fct>, wheels <dbl>, title <chr>\nmariokart_new <- mariokart %>%\n  filter(total_pr <= 100) %>% \n  select(total_pr,cond,stock_photo,duration,wheels)\nsummary(mariokart_new)##     total_pr       cond    stock_photo    duration          wheels     \n##  Min.   :28.98   new :59   no : 36     Min.   : 1.000   Min.   :0.000  \n##  1st Qu.:41.00   used:82   yes:105     1st Qu.: 1.000   1st Qu.:0.000  \n##  Median :46.03                         Median : 3.000   Median :1.000  \n##  Mean   :47.43                         Mean   : 3.752   Mean   :1.149  \n##  3rd Qu.:53.99                         3rd Qu.: 7.000   3rd Qu.:2.000  \n##  Max.   :75.00                         Max.   :10.000   Max.   :4.000\nmariokart_new %>% \n  gf_boxplot(total_pr~cond) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title=\"Ebay Auction Prices\",subtitle=\"Outliers removed\",x=\"Condition\", y=\"Total Price\")\nmario_mod2 <- lm(total_pr~cond,data=mariokart_new)\nsummary(mario_mod2)## \n## Call:\n## lm(formula = total_pr ~ cond, data = mariokart_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.8911  -5.8311   0.1289   4.1289  22.1489 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  53.7707     0.9596  56.034  < 2e-16 ***\n## condused    -10.8996     1.2583  -8.662 1.06e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.371 on 139 degrees of freedom\n## Multiple R-squared:  0.3506, Adjusted R-squared:  0.3459 \n## F-statistic: 75.03 on 1 and 139 DF,  p-value: 1.056e-14\nplot(mario_mod2,2)\nplot(mario_mod2,3)\nplot(mario_mod2,5)"},{"path":"LRMULTI.html","id":"including-and-assessing-many-variables-in-a-model","chapter":"31 Multiple Linear Regression","heading":"31.3.2 Including and assessing many variables in a model","text":"Sometimes underlying structures relationships predictor variables. instance, new games sold Ebay tend come Wii wheels, may led higher prices auctions. like fit model includes potentially important variables simultaneously. help us evaluate relationship predictor variable outcome controlling potential influence variables. strategy used multiple regression. remain cautious making causal interpretations using multiple regression, models common first step providing evidence causal connection.want construct model accounts game condition, simultaneously accounts three variables: stock_photo, duration, wheels. model can represented :\\[\n\\widehat{\\text{totalprice}}\n    = \\beta_0 + \\beta_1 \\times \\text{cond} + \\beta_2 \\times \\text{stockphoto}\n    + \\beta_3 \\times  \\text{duration} +\n        \\beta_4 \\times  \\text{wheels}\n\\]:\\[\\begin{equation}\n\\hat{y}\n    = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\n        \\beta_3 x_3 + \\beta_4 x_4\n  \\tag{31.1}\n\\end{equation}\\]Equation (31.1), \\(y\\) represents total price, \\(x_1\\) indicates whether game new, \\(x_2\\) indicates whether stock photo used, \\(x_3\\) duration auction, \\(x_4\\) number Wii wheels included game. Just single predictor case, multiple regression model may missing important components might precisely represent relationship outcome available explanatory variables. model perfect, wish explore possibility model may fit data reasonably well.estimate parameters \\(\\beta_0\\), \\(\\beta_1\\), …, \\(\\beta_4\\) way case single predictor. select \\(b_0\\), \\(b_1\\), …, \\(b_4\\) minimize sum squared residuals:\\[\n\\text{SSE} = e_1^2 + e_2^2 + \\dots + e_{141}^2\n    = \\sum_{=1}^{141} e_i^2\n     = \\sum_{=1}^{141} \\left(y_i - \\hat{y}_i\\right)^2\n\\]problem, 141 residuals, one observation. use computer minimize sum compute point estimates.formula total_pr~. uses dot. means want use predictors. also used following code:Recall, + symbol mean literally add predictors together. mathematical operation formula operation means include predictor.can view summary model using summmary() function.can summarize tibble using broom package.\nTable 31.1: Multiple regression coefficients.\nUsing output, Table 31.1, identify point estimates \\(b_i\\) \\(\\beta_i\\), just one-predictor case.Multiple regression model\nmultiple regression model linear model many predictors.general, write model \\[\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k %+ \\epsilon\n\\]\\(k\\) predictors. often estimate \\(\\beta_i\\) parameters using computer.Exercise:\nWrite multiple regression model using point estimates regression output. many predictors model?100Exercise:\n\\(\\beta_4\\), coefficient variable \\(x_4\\) (Wii wheels), represent? point estimate \\(\\beta_4\\)?101Exercise:\nCompute residual first observation dataframe using regression equation.broom package function augment() calculate predicted residuals.\\(e_i = y_i - \\hat{y_i} = 51.55 - 49.62 = 1.93\\)Example:\nestimated coefficient cond \\(b_1 = - 10.90\\) standard error \\(SE_{b_1} = 1.26\\) using simple linear regression. might difference estimate one multiple regression setting?examined data carefully, see predictors correlated. instance, estimated connection outcome total_pr predictor cond using simple linear regression, unable control variables like number Wii wheels included auction. model biased confounding variable wheels. use variables, particular underlying unintentional bias reduced eliminated (though bias confounding variables may still remain).previous example describes common issue multiple regression: correlation among predictor variables. say two predictor variables collinear (pronounced co-linear) correlated, collinearity complicates model estimation. impossible prevent collinearity arising observational data, experiments usually designed prevent predictors collinear.Exercise:\nestimated value intercept 41.34, one might tempted make interpretation coefficient, , model’s predicted price variables take value zero: game new, primary image stock photo, auction duration zero days, wheels included. value gained making interpretation?102","code":"\nmario_mod_multi <- lm(total_pr~., data=mariokart_new)\nmario_mod_multi <- lm(total_pr~cond+stock_photo+duration+wheels, data=mariokart_new)\nsummary(mario_mod_multi)## \n## Call:\n## lm(formula = total_pr ~ ., data = mariokart_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -11.3788  -2.9854  -0.9654   2.6915  14.0346 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    41.34153    1.71167  24.153  < 2e-16 ***\n## condused       -5.13056    1.05112  -4.881 2.91e-06 ***\n## stock_photoyes  1.08031    1.05682   1.022    0.308    \n## duration       -0.02681    0.19041  -0.141    0.888    \n## wheels          7.28518    0.55469  13.134  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.901 on 136 degrees of freedom\n## Multiple R-squared:  0.719,  Adjusted R-squared:  0.7108 \n## F-statistic: 87.01 on 4 and 136 DF,  p-value: < 2.2e-16\nmario_mod_multi$residuals[1]##        1 \n## 1.923402\nlibrary(broom)\naugment(mario_mod_multi) %>%\n  head(1)## # A tibble: 1 x 11\n##   total_pr cond  stock_photo duration wheels .fitted .resid   .hat .sigma\n##      <dbl> <fct> <fct>          <dbl>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>\n## 1     51.6 new   yes                3      1    49.6   1.92 0.0215   4.92\n## # ... with 2 more variables: .cooksd <dbl>, .std.resid <dbl>"},{"path":"LRMULTI.html","id":"inference-1","chapter":"31 Multiple Linear Regression","heading":"31.3.3 Inference","text":"printout model summary, can see stock_photo duration variables significantly different zero. Thus may want drop model. machine learning course, explore different ways determine best model.Likewise, generate confidence intervals coefficients:confirms stock_photo duration may impact total price.","code":"\nconfint(mario_mod_multi)##                     2.5 %     97.5 %\n## (Intercept)    37.9566036 44.7264601\n## condused       -7.2092253 -3.0519030\n## stock_photoyes -1.0096225  3.1702442\n## duration       -0.4033592  0.3497442\n## wheels          6.1882392  8.3821165"},{"path":"LRMULTI.html","id":"adjusted-r2-as-a-better-estimate-of-explained-variance","chapter":"31 Multiple Linear Regression","heading":"31.3.4 Adjusted \\(R^2\\) as a better estimate of explained variance","text":"first used \\(R^2\\) simple linear regression determine amount variability, used sum squares mean squared errors, response explained model:\n\\[\nR^2 = 1 - \\frac{\\text{sum squares residuals}}{\\text{sum squares outcome}}\n\\]\nequation remains valid multiple regression framework, small enhancement can often even informative.Exercise:\nvariance residuals model \\(4.901^2\\), variance total price auctions 83.06. Estimate \\(R^2\\) model.103To get \\(R^2\\) need sum squares variance, multiply appropriate degrees freedom.strategy estimating \\(R^2\\) acceptable just single variable. However, becomes less helpful many variables. regular \\(R^2\\) actually biased estimate amount variability explained model. get better estimate, use adjusted \\(R^2\\).Adjusted \\(\\mathbf{R^2}\\) tool model assessment:\nadjusted \\(\\mathbf{R^2}\\) computed :\n\\[\nR_{adj}^{2} = 1-\\frac{\\text{sum squares residuals} / (n-k-1)}{\\text{sum squares outcome} / (n-1)}\n\\]\n\\(n\\) number cases used fit model \\(k\\) number predictor variables model.\\(k\\) never negative, adjusted \\(R^2\\) smaller – often times just little smaller – unadjusted \\(R^2\\). reasoning behind adjusted \\(R^2\\) lies degrees freedom associated variance. 104Exercise:\nSuppose added another predictor model, variance errors didn’t go . happen \\(R^2\\)? happen adjusted \\(R^2\\)?105Again, machine learning course, spend time select models. Using internal metrics performance p-values adjusted \\(R\\) squared one way using external measures predictive performance cross validation hold sets introduced.","code":"\n1-(24.0198*136)/(83.05864*140)## [1] 0.7190717\nsummary(mario_mod_multi)$r.squared## [1] 0.7190261"},{"path":"LRMULTI.html","id":"reduced-model","chapter":"31 Multiple Linear Regression","heading":"31.3.5 Reduced model","text":"Now let’s drop duration model compare previous model:summary:reminder, previous model summary :Notice adjusted \\(R^2\\) improved dropping duration. Finally, let’s drop stock_photo.Though adjusted \\(R^2\\) dropped little, fourth decimal place thus essentially value. therefore go model.","code":"\nmario_mod_multi2 <- lm(total_pr~cond+stock_photo+wheels, data=mariokart_new)\nsummary(mario_mod_multi2)## \n## Call:\n## lm(formula = total_pr ~ cond + stock_photo + wheels, data = mariokart_new)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -11.454  -2.959  -0.949   2.712  14.061 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     41.2245     1.4911  27.648  < 2e-16 ***\n## condused        -5.1763     0.9961  -5.196 7.21e-07 ***\n## stock_photoyes   1.1177     1.0192   1.097    0.275    \n## wheels           7.2984     0.5448  13.397  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.884 on 137 degrees of freedom\n## Multiple R-squared:  0.719,  Adjusted R-squared:  0.7128 \n## F-statistic: 116.8 on 3 and 137 DF,  p-value: < 2.2e-16\nsummary(mario_mod_multi)## \n## Call:\n## lm(formula = total_pr ~ ., data = mariokart_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -11.3788  -2.9854  -0.9654   2.6915  14.0346 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    41.34153    1.71167  24.153  < 2e-16 ***\n## condused       -5.13056    1.05112  -4.881 2.91e-06 ***\n## stock_photoyes  1.08031    1.05682   1.022    0.308    \n## duration       -0.02681    0.19041  -0.141    0.888    \n## wheels          7.28518    0.55469  13.134  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.901 on 136 degrees of freedom\n## Multiple R-squared:  0.719,  Adjusted R-squared:  0.7108 \n## F-statistic: 87.01 on 4 and 136 DF,  p-value: < 2.2e-16\nmario_mod_multi3 <- lm(total_pr~cond+wheels, data=mariokart_new)\nsummary(mario_mod_multi3)## \n## Call:\n## lm(formula = total_pr ~ cond + wheels, data = mariokart_new)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -11.0078  -3.0754  -0.8254   2.9822  14.1646 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\n## condused     -5.5848     0.9245  -6.041 1.35e-08 ***\n## wheels        7.2328     0.5419  13.347  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.887 on 138 degrees of freedom\n## Multiple R-squared:  0.7165, Adjusted R-squared:  0.7124 \n## F-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16"},{"path":"LRMULTI.html","id":"confidence-and-prediction-intervals","chapter":"31 Multiple Linear Regression","heading":"31.3.6 Confidence and prediction intervals","text":"Let’s suppose want predict average total price Mario Kart sale 2 wheels new condition. can use predict() function.95% confident average price Mario Kart sale new item 2 wheels 55.50 58.17.Exercise:\nFind interpret prediction interval new Mario Kart 2 wheels.95% confident price Mario Kart sale new item 2 wheels 47.07 66.59.","code":"\npredict(mario_mod_multi3,newdata=data.frame(cond=\"new\",wheels=2),interval = \"confidence\")##        fit      lwr      upr\n## 1 56.83544 55.49789 58.17299\npredict(mario_mod_multi3,newdata=data.frame(cond=\"new\",wheels=2),interval = \"prediction\")##        fit      lwr      upr\n## 1 56.83544 47.07941 66.59147"},{"path":"LRMULTI.html","id":"diagnostics","chapter":"31 Multiple Linear Regression","heading":"31.3.7 Diagnostics","text":"diagnostics model similar previous lesson. Nothing plots gives us concern; however, one leverage point, Figure 31.7.\nFigure 31.7: Diagnostic residual plots multiple regression model.\n","code":"\nplot(mario_mod_multi3)"},{"path":"LRMULTI.html","id":"interaction-and-higher-order-terms","chapter":"31 Multiple Linear Regression","heading":"31.4 Interaction and Higher Order Terms","text":"final short topic want explore feature engineering. Thus far done transformation predictors data set except maybe making categorical variables factors. data analysis competitions, Kaggle, feature engineering often one important steps. machine learning course, look different tools book look simple transformations higher order terms interactions.make section relevant, going switch different data set. Load library ISLR.data set interest Credit. Use help menu read variables. simulated data set credit card debt.Notice ID treated integer. change character since label, work chapter bother.Suppose suspected relationship Balance, response, predictors Income Student. Note: actually using model educational purposes go model selection process.first model simply predictors model.Let’s plot data regression line. impact putting categorical variable Student just shift intercept. slope remains , Figure 31.8.\nFigure 31.8: Scatterplot credit card balance income student status.\nExercise:\nWrite equation regression model.\\[\n\\mbox{E}(Balance)=\\beta_0 + \\beta_1*\\text{Income}+ \\beta_2*\\text{(Student=Yes)}\n\\]\\[\n\\mbox{E}(Balance)=211.14 + 5.98*\\text{Income}+ 382.67*\\text{(Student=Yes)}\n\\]observation student, intercept increased 382.67.next case, want include interaction term model: interaction term allows slope change well. include interaction term building model R, use *.\nFigure 31.9: Scatterplot credit card balance income student status interaction term.\nNow different slope intercept case Student variable, Figure 31.9. Thus synergy interaction variables. student status changes impact Income Balance. student, every increase income 1 balance increase 4.219 average. student, every increase income 1 increases average balance 6.2182.Furthermore, suspect perhaps curved relationship exists two variables, include higher order term. example, let’s add quadratic term Income model (without interaction). R, need wrap higher order term (). include higher order term, usually want include lower order terms well; better approach make decision include using predictive performance.\nFigure 31.10: Scatterplot credit card balance income quadratic fit.\nmuch quadratic relationship, Figure 31.10.","code":"\nlibrary(ISLR)\nglimpse(Credit)## Rows: 400\n## Columns: 12\n## $ ID        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1~\n## $ Income    <dbl> 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7~\n## $ Limit     <int> 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, ~\n## $ Rating    <int> 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, ~\n## $ Cards     <int> 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, ~\n## $ Age       <int> 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, ~\n## $ Education <int> 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,~\n## $ Gender    <fct>  Male, Female,  Male, Female,  Male,  Male, Female,  Male, F~\n## $ Student   <fct> No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No~\n## $ Married   <fct> Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye~\n## $ Ethnicity <fct> Caucasian, Asian, Asian, Asian, Caucasian, Caucasian, Africa~\n## $ Balance   <int> 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,~\ncredit_mod1<-lm(Balance~Income+Student,data=Credit)\nsummary(credit_mod1)## \n## Call:\n## lm(formula = Balance ~ Income + Student, data = Credit)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -762.37 -331.38  -45.04  323.60  818.28 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 211.1430    32.4572   6.505 2.34e-10 ***\n## Income        5.9843     0.5566  10.751  < 2e-16 ***\n## StudentYes  382.6705    65.3108   5.859 9.78e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 391.8 on 397 degrees of freedom\n## Multiple R-squared:  0.2775, Adjusted R-squared:  0.2738 \n## F-statistic: 76.22 on 2 and 397 DF,  p-value: < 2.2e-16\naugment(credit_mod1) %>%\n  gf_point(Balance~Income,color=~Student) %>%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod1), Student == \"Yes\"),color=~Student)%>%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod1), Student == \"No\"),color=~Student) %>%\n  gf_theme(theme_bw())\ncredit_mod2<-lm(Balance~Income*Student,data=Credit)\nsummary(credit_mod2)## \n## Call:\n## lm(formula = Balance ~ Income * Student, data = Credit)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -773.39 -325.70  -41.13  321.65  814.04 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       200.6232    33.6984   5.953 5.79e-09 ***\n## Income              6.2182     0.5921  10.502  < 2e-16 ***\n## StudentYes        476.6758   104.3512   4.568 6.59e-06 ***\n## Income:StudentYes  -1.9992     1.7313  -1.155    0.249    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 391.6 on 396 degrees of freedom\n## Multiple R-squared:  0.2799, Adjusted R-squared:  0.2744 \n## F-statistic:  51.3 on 3 and 396 DF,  p-value: < 2.2e-16\naugment(credit_mod2) %>%\n  gf_point(Balance~Income,color=~Student) %>%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod2), Student == \"Yes\"),color=~Student)%>%\n  gf_line(.fitted~Income,data=subset(augment(credit_mod2), Student == \"No\"),color=~Student) %>%\n  gf_theme(theme_bw())\ncredit_mod3<-lm(Balance~Income+I(Income^2),data=Credit)\nsummary(credit_mod3)## \n## Call:\n## lm(formula = Balance ~ Income + I(Income^2), data = Credit)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -782.88 -361.40  -54.98  316.26 1104.39 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 285.3973    54.1720   5.268 2.26e-07 ***\n## Income        4.3972     1.9078   2.305   0.0217 *  \n## I(Income^2)   0.0109     0.0120   0.908   0.3642    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 408 on 397 degrees of freedom\n## Multiple R-squared:  0.2166, Adjusted R-squared:  0.2127 \n## F-statistic: 54.88 on 2 and 397 DF,  p-value: < 2.2e-16\naugment(credit_mod3) %>%\n  gf_point(Balance~Income) %>%\n  gf_line(.fitted~Income) %>%\n  gf_theme(theme_bw())"},{"path":"LRMULTI.html","id":"summary-3","chapter":"31 Multiple Linear Regression","heading":"31.4.1 Summary","text":"chapter extended linear regression model allowing multiple predictors. allows us account confounding variables make sophisticated models. interpretation evaluation model changes.","code":""},{"path":"LRMULTI.html","id":"homework-problems-30","chapter":"31 Multiple Linear Regression","heading":"31.5 Homework Problems","text":"mtcars data set contains average mileage (mpg) information specific makes models cars. (data set built-R; information data set, reference documentation ?mtcars).Build interpret coefficients model fitting mpg displacement (disp), horsepower (hp), rear axle ratio (drat), weight 1000 lbs (wt).Given model, expected mpg vehicle displacement 170, horsepower 100, drat 3.80 wt 2,900 lbs. Construct 95% confidence interval prediction interval expected mpg.Repeat part (b) bootstrap confidence interval.best model predicting mpg? Try variety different models. explore higher order terms even interactions. One place start using pairs() function mtcars plot large pairwise scatterplot. high get adjusted \\(R\\)-squared? Keep mind one measure fit.","code":""},{"path":"LOGREG.html","id":"LOGREG","chapter":"32 Logistic Regression","heading":"32 Logistic Regression","text":"","code":""},{"path":"LOGREG.html","id":"objectives-31","chapter":"32 Logistic Regression","heading":"32.1 Objectives","text":"Using R, conduct logistic regression interpret output perform model selection.Write logistic regression model predict outputs given inputs.Find confidence intervals parameter estimates predictions.Create interpret confusion matrix.","code":""},{"path":"LOGREG.html","id":"logistic-regression-introduction","chapter":"32 Logistic Regression","heading":"32.2 Logistic regression introduction","text":"lesson introduce logistic regression tool building models categorical response variable two levels. Logistic regression type generalized linear model (GLM) response variables assumptions normally distributed errors appropriate. prepping advanced statistical models machine learning, explore predictive models many different types response variables including ones don’t assume underlying functional relationship inputs outputs. cool!GLMs can thought two-stage modeling approach. first model response variable using probability distribution, binomial Poisson distribution. Second, model parameter distribution using collection predictors special form multiple regression.explore explain ideas, use Ebay auctions video game called Mario Kart Nintendo Wii. Remember, data set file mariokart.csv includes results 141 auctions.106In chapter, want outcome variable interest game condition, cond. Chapter 31 used total price auction response. moving quantitative response binary qualitative variable. interested determining association exists variables cond total_pr, use linear regression total_pr response. However, problem want predict game condition. start reviewing previous models introduce logistic regression. finish multiple logistic regression model, one predictor.","code":""},{"path":"LOGREG.html","id":"mario-kart-data","chapter":"32 Logistic Regression","heading":"32.2.1 Mario Kart data","text":"Read data summarize.interested total_pr, cond, stock_photo, duration, wheels. variables described following list:total_pr: final auction price plus shipping costs, US dollarscond: two-level categorical factor variablestock_photo: two-level categorical factor variableduration: length auction, days, taking values 1 10wheels: number Wii wheels included auction (Wii wheel plastic racing wheel holds Wii controller optional helpful accessory playing Mario Kart)Remember removed couple outlier sales included multiple items. start let’s clean data include removing outliers.Next let’s summarize data.","code":"\nmariokart <- read_csv(\"data/mariokart.csv\", col_types = list(col_character()))\nhead(mariokart, n = 10)## # A tibble: 10 x 12\n##    id        duration n_bids cond  start_pr ship_pr total_pr ship_sp seller_rate\n##    <chr>        <dbl>  <dbl> <chr>    <dbl>   <dbl>    <dbl> <chr>         <dbl>\n##  1 15037742~        3     20 new       0.99    4        51.6 standa~        1580\n##  2 26048337~        7     13 used      0.99    3.99     37.0 firstC~         365\n##  3 32043234~        3     16 new       0.99    3.5      45.5 firstC~         998\n##  4 28040522~        3     18 new       0.99    0        44   standa~           7\n##  5 17039222~        1     20 new       0.01    0        71   media           820\n##  6 36019515~        3     19 new       0.99    4        45   standa~      270144\n##  7 12047772~        1     13 used      0.01    0        37.0 standa~        7284\n##  8 30035550~        1     15 new       1       2.99     54.0 upsGro~        4858\n##  9 20039206~        3     29 used      0.99    4        47   priori~          27\n## 10 33036416~        7      8 used     20.0     4        50   firstC~         201\n## # ... with 3 more variables: stock_photo <chr>, wheels <dbl>, title <chr>\ninspect(mariokart)## \n## categorical variables:  \n##          name     class levels   n missing\n## 1          id character    143 143       0\n## 2        cond character      2 143       0\n## 3     ship_sp character      8 143       0\n## 4 stock_photo character      2 143       0\n## 5       title character     80 142       1\n##                                    distribution\n## 1 110439174663 (0.7%) ...                      \n## 2 used (58.7%), new (41.3%)                    \n## 3 standard (23.1%), upsGround (21.7%) ...      \n## 4 yes (73.4%), no (26.6%)                      \n## 5  (%) ...                                     \n## \n## quantitative variables:  \n##             name   class   min      Q1 median      Q3       max         mean\n## ...1    duration numeric  1.00   1.000    3.0    7.00     10.00     3.769231\n## ...2      n_bids numeric  1.00  10.000   14.0   17.00     29.00    13.538462\n## ...3    start_pr numeric  0.01   0.990    1.0   10.00     69.95     8.777203\n## ...4     ship_pr numeric  0.00   0.000    3.0    4.00     25.51     3.143706\n## ...5    total_pr numeric 28.98  41.175   46.5   53.99    326.51    49.880490\n## ...6 seller_rate numeric  0.00 109.000  820.0 4858.00 270144.00 15898.419580\n## ...7      wheels numeric  0.00   0.000    1.0    2.00      4.00     1.146853\n##                sd   n missing\n## ...1 2.585693e+00 143       0\n## ...2 5.878786e+00 143       0\n## ...3 1.506745e+01 143       0\n## ...4 3.213179e+00 143       0\n## ...5 2.568856e+01 143       0\n## ...6 5.184032e+04 143       0\n## ...7 8.471829e-01 143       0\nmariokart <- mariokart %>%\n  filter(total_pr <= 100) %>% \n  mutate(cond = factor(cond),\n         stock_photo = factor(stock_photo)) %>% \n  select(cond, stock_photo, total_pr, duration, wheels)\ninspect(mariokart)## \n## categorical variables:  \n##          name  class levels   n missing\n## 1        cond factor      2 141       0\n## 2 stock_photo factor      2 141       0\n##                                    distribution\n## 1 used (58.2%), new (41.8%)                    \n## 2 yes (74.5%), no (25.5%)                      \n## \n## quantitative variables:  \n##          name   class   min Q1 median    Q3 max      mean        sd   n missing\n## ...1 total_pr numeric 28.98 41  46.03 53.99  75 47.431915 9.1136514 141       0\n## ...2 duration numeric  1.00  1   3.00  7.00  10  3.751773 2.5888663 141       0\n## ...3   wheels numeric  0.00  0   1.00  2.00   4  1.148936 0.8446146 141       0"},{"path":"LOGREG.html","id":"analyzing-contingency-table","chapter":"32 Logistic Regression","heading":"32.2.2 Analyzing contingency table","text":"review introduction logistic regression, let’s analyze relationship game condition stock photo.analyze comparing proportion new condition games stock photo value using randomization, empirical p-values, central limit theorem. just use exact permutation test, Fisher Exact Test, just uses hypergeometric distribution.Clearly, variables independent . model gives us much information let’s move logistic regression.","code":"\ntally(cond ~ stock_photo, data = mariokart, margins = TRUE, \n      format = \"proportion\")##        stock_photo\n## cond           no       yes\n##   new   0.1111111 0.5238095\n##   used  0.8888889 0.4761905\n##   Total 1.0000000 1.0000000\nfisher.test(tally(~cond + stock_photo, data = mariokart))## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tally(~cond + stock_photo, data = mariokart)\n## p-value = 9.875e-06\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  0.02766882 0.35763723\n## sample estimates:\n## odds ratio \n##  0.1152058"},{"path":"LOGREG.html","id":"modeling-the-probability-of-an-event","chapter":"32 Logistic Regression","heading":"32.2.3 Modeling the probability of an event","text":"outcome variable GLM denoted \\(Y_i\\), index \\(\\) used represent observation \\(\\). Mario Kart application, \\(Y_i\\) used represent whether game condition \\(\\) new (\\(Y_i=1\\)) used (\\(Y_i=0\\)).predictor variables represented follows: \\(x_{1,}\\) value variable 1 observation \\(\\), \\(x_{2,}\\) value variable 2 observation \\(\\), .Logistic regression generalized linear model outcome two-level categorical variable. outcome, \\(Y_i\\), takes value 1 (application, represents game new condition easily switch make outcome interest used game) probability \\(p_i\\) value 0 probability \\(1-p_i\\). probability \\(p_i\\) model relation predictor variables.logistic regression model relates probability game new (\\(p_i\\)) values predictors \\(x_{1,}\\), \\(x_{2,}\\), …, \\(x_{k,}\\) framework much like multiple regression:\\[\n\\text{transformation}(p_{}) = \\beta_0 + \\beta_1x_{1,} + \\beta_2 x_{2,} + \\cdots \\beta_k x_{k,}\n\\]want choose transformation makes practical mathematical sense. example, want transformation makes range possibilities left hand side equation equal range possibilities right hand side. transformation equation, left hand side take values 0 1, right hand side take values outside range. common transformation \\(p_i\\) logit transformation, may written \\[\n\\text{logit}(p_i) = \\log_{e}\\left( \\frac{p_i}{1-p_i} \\right)\n\\], expand equation using logit transformation \\(p_i\\):\\[\n\\log_{e}\\left( \\frac{p_i}{1-p_i} \\right)\n    = \\beta_0 + \\beta_1 x_{1,} + \\beta_2 x_{2,} + \\cdots + \\beta_k x_{k,}\n\\]Solving \\(p_i\\) get logistic function:\\[\np_i = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_{1,} + \\beta_2 x_{2,} + \\cdots + \\beta_k x_{k,})}}\n\\]logistic function shown Figure 32.1.\nFigure 32.1: Logitstic function example points plotted.\nNotice output logistic function restricts values 0 1. curve fairly flat edges sharp rise center. functions achieve result. However, reasons beyond scope book, logit function desirable mathematical properties relate making sure common GLMs fall within exponential family distributions. topic graduate school level needed studies.Mario Kart example, 4 predictor variables, \\(k = 4\\). nonlinear model isn’t intuitive, still resemblance multiple regression, can fit model using software. fact, look results software, start feel like ’re back multiple regression, even interpretation coefficients complex.","code":""},{"path":"LOGREG.html","id":"first-model---intercept-only","chapter":"32 Logistic Regression","heading":"32.2.4 First model - intercept only","text":"create model just intercept.R use glm() function fit logistic regression model. formula format lm also requires family argument. Since response binary, use binomial. wanted use glm() linear regression assuming normally distributed residual, family argument gaussian. implies multiple linear regression assumption normally distributed errors special case generalized linear model. R, response 0/1 variable, can control outcome interest, 1, using logical argument formula.First understand output logistic regression, let’s just run model intercept term. Notice code chunk left hand side formula logical argument, gives 0/1 output 1 value want predict.Let’s get regression output using summary() function.looks similar regression output saw previous chapters. However, model different, nonlinear, form. Remember, Equation (32.1) general form model.\\[\\begin{equation}\n  \\log_{e}\\left( \\frac{p_i}{1-p_i} \\right)\n    = \\beta_0 + \\beta_1 x_{1,} + \\beta_2 x_{2,} + \\cdots + \\beta_k x_{k,}\n  \\tag{32.1}\n\\end{equation}\\]Thus using output R, Equation (32.2) estimated model.\\[\\begin{equation}\n\\log\\left( \\frac{p_i}{1-p_i} \\right) = -0.329\n  \\tag{32.2}\n\\end{equation}\\]Solving Equation (32.2) \\(p_i\\): \\(\\frac{e^{-0.329}}{1 + e^{-0.329}} = 0.418\\). estimated probability game condition new. point plotted Figure 32.1. can also check result using summary table.","code":"\nmario_mod1 <- glm(cond == \"new\" ~ 1, data = mariokart, family = \"binomial\")\nsummary(mario_mod1)## \n## Call:\n## glm(formula = cond == \"new\" ~ 1, family = \"binomial\", data = mariokart)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.041  -1.041  -1.041   1.320   1.320  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)  -0.3292     0.1707  -1.928   0.0538 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 191.7  on 140  degrees of freedom\n## Residual deviance: 191.7  on 140  degrees of freedom\n## AIC: 193.7\n## \n## Number of Fisher Scoring iterations: 4\ntally(~cond, data = mariokart, format = \"proportion\")## cond\n##       new      used \n## 0.4184397 0.5815603"},{"path":"LOGREG.html","id":"second-model---stock_photo","chapter":"32 Logistic Regression","heading":"32.2.5 Second model - stock_photo","text":"Now starting understand logistic regression model. Let’s add predictor variable, stock_photo. , many methods determine relationship two categorical variables exists, logistic regression another method.Examining p-value associated coefficient stock_photo, can see significant. Thus reject null hypothesis coefficient zero. relationship cond stock_photo, found Fisher’s test.can use broom package summarize output generate model fits.Let’s convert coefficients estimated probabilities using augment() function. need specify output response, returns probability, else get logit probability, link value.conditional probability new condition based status stock_photo. can see using tally() function.model coefficients.Exercise:\nFit logistic regression model cond used stock_photo predictor.repeat code ., let’s convert coefficients estimated probabilities using augment() function.matches output tally() function observed .Notice important whether select new used condition desired outcome. either case, logistic regression model returns conditional probability given value predictor.","code":"\nmario_mod2 <- glm(cond == \"new\" ~ stock_photo, data = mariokart,\n                 family = \"binomial\")\nsummary(mario_mod2)## \n## Call:\n## glm(formula = cond == \"new\" ~ stock_photo, family = \"binomial\", \n##     data = mariokart)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.2181  -1.2181  -0.4854   1.1372   2.0963  \n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)     -2.0794     0.5303  -3.921 8.81e-05 ***\n## stock_photoyes   2.1748     0.5652   3.848 0.000119 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 191.70  on 140  degrees of freedom\n## Residual deviance: 170.44  on 139  degrees of freedom\n## AIC: 174.44\n## \n## Number of Fisher Scoring iterations: 4\ntidy(mario_mod2)## # A tibble: 2 x 5\n##   term           estimate std.error statistic   p.value\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)       -2.08     0.530     -3.92 0.0000881\n## 2 stock_photoyes     2.17     0.565      3.85 0.000119\naugment(mario_mod2,\n        newdata = tibble(stock_photo = c(\"yes\", \"no\")),\n        type.predict = \"response\")## # A tibble: 2 x 2\n##   stock_photo .fitted\n##   <chr>         <dbl>\n## 1 yes           0.524\n## 2 no            0.111\ntally(cond ~ stock_photo, data = mariokart, margins = TRUE, \n      format = \"proportion\")##        stock_photo\n## cond           no       yes\n##   new   0.1111111 0.5238095\n##   used  0.8888889 0.4761905\n##   Total 1.0000000 1.0000000\nexp(-2.079442) / (1 + exp(-2.079442))## [1] 0.1111111\nexp(-2.079442 + 2.174752) / (1 + exp(-2.079442 + 2.174752))## [1] 0.5238095\nmario_mod3 <- glm(cond == \"used\" ~ stock_photo, data = mariokart,\n                 family = \"binomial\")\ntidy(mario_mod3)## # A tibble: 2 x 5\n##   term           estimate std.error statistic   p.value\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)        2.08     0.530      3.92 0.0000881\n## 2 stock_photoyes    -2.17     0.565     -3.85 0.000119\naugment(mario_mod3,\n        newdata = tibble(stock_photo = c(\"yes\", \"no\")),\n        type.predict = \"response\")## # A tibble: 2 x 2\n##   stock_photo .fitted\n##   <chr>         <dbl>\n## 1 yes           0.476\n## 2 no            0.889"},{"path":"LOGREG.html","id":"interpreting-the-coefficients","chapter":"32 Logistic Regression","heading":"32.2.6 Interpreting the coefficients","text":"point seems created great deal work just get results methods. However, logistic regression model allows us add predictors also gives us standard errors parameter estimates.Let’s first discuss interpretation coefficients. reminder, fitted coefficients reported model summary.variable stock_photo takes values 0 1, value 1 indicates sale stock photo. logistic regression model fitting Equation (32.3).\\[\\begin{equation}\n  \\log_{e}\\left( \\frac{p_{new}}{1-p_{new}} \\right)\n    = \\beta_0 + \\beta_1 \\mbox{stock_photo}  \n  \\tag{32.3}\n\\end{equation}\\]photo stock photo model Equation (32.4). left-hand side natural logarithm odds, odds ratio probability success divided probability failure.\\[\n\\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right)\n    = \\beta_0 + \\beta_1   \n\\]\\[\\begin{equation}\n  \\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right)\n    = \\beta_0   \n  \\tag{32.4}\n\\end{equation}\\]stock photo, variable stock_photo 1. Equation (32.5) resulting model.\\[\\begin{equation}\n\\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right)\n    = \\beta_0 + \\beta_1   \n  \\tag{32.5}\n\\end{equation}\\]Thus difference gives interpretation \\(\\beta_1\\) coefficient, log odds ratio shown derivation follows.\\[\n\\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right)\n-\n\\log_{e}\\left( \\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}} \\right) = \\beta_1\n\\]\n\\[\n\\log_{e}\\left(\\frac{\\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}}}{\\frac{p_{\\mbox{new|stock photo}}}{1-p_{\\mbox{new|stock photo}}}} \\right)\n= \\beta_1\n\\]problem, log odds double photo stock photo. easier interpret odds ratios, often analysts use \\(e^{\\beta_1}\\) odds ratio. , problem, odds new condition game increase factor 8.8 stock photo used. Note odds ratio relative risk. Relative risk ratio probability new game stock photo probability new game without stock photo. careful interpretation.\\[\n\\text{Relative Risk} = \\left(\\frac{p_{\\mbox{new|stock photo}}}{p_{\\mbox{new|stock photo}}} \\right)\n\\]","code":"\ntidy(mario_mod2)## # A tibble: 2 x 5\n##   term           estimate std.error statistic   p.value\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)       -2.08     0.530     -3.92 0.0000881\n## 2 stock_photoyes     2.17     0.565      3.85 0.000119"},{"path":"LOGREG.html","id":"comparing-models","chapter":"32 Logistic Regression","heading":"32.2.7 Comparing models","text":"Just case linear regression, can compare nested models. examine output model line residual deviance. model fit using least squares using maximum likelihood. Deviance 2 times negative log likelihood. negate log likelihood maximizing log likelihood equivalent minimizing negation. allows thought process minimizing deviance minimizing residual sum squares. multiplication 2 asymptotic argument shows 2 times negative log likelihood approximately distributed Chi-square random variable.Similar linear regression, can use anova() function compare nested models.Adding, stock_photo statistically significant result. p-value different summary() function, assumes coefficient follows normal distribution. Different assumptions, conclusion.use p-value pick best model uses statistical assumptions select features. Another approach use predictive measure. machine learning contexts, use many different predictive performance measures model selection many based confusion matrix.confusion matrix generates 2 2 matrix predicted outcomes versus actual outcomes. logistic regression, output probability success. convert 0/1 outcome pick threshold. common use 0.5 threshold. Probabilities 0.5 considered success, context problem new game. Let’s generate confusion matrix.One single number summary metric accuracy. case model correct \\(32 + 55\\) 141 cases, 61.7% correct.looks like table get comparing cond stock_photo. case binary nature predictor. two probability values prediction.change threshold get different accuracy. machine learning course, learn metrics area ROC curve. Back problem, let’s add another variable see can improve model.","code":"\nsummary(mario_mod2)## \n## Call:\n## glm(formula = cond == \"new\" ~ stock_photo, family = \"binomial\", \n##     data = mariokart)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.2181  -1.2181  -0.4854   1.1372   2.0963  \n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)     -2.0794     0.5303  -3.921 8.81e-05 ***\n## stock_photoyes   2.1748     0.5652   3.848 0.000119 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 191.70  on 140  degrees of freedom\n## Residual deviance: 170.44  on 139  degrees of freedom\n## AIC: 174.44\n## \n## Number of Fisher Scoring iterations: 4\nanova(mario_mod1, mario_mod2, test = \"Chisq\")## Analysis of Deviance Table\n## \n## Model 1: cond == \"new\" ~ 1\n## Model 2: cond == \"new\" ~ stock_photo\n##   Resid. Df Resid. Dev Df Deviance Pr(>Chi)    \n## 1       140     191.70                         \n## 2       139     170.44  1    21.26 4.01e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\naugment(mario_mod2, type.predict = \"response\") %>%\n  rename(actual= starts_with('cond')) %>%\n  transmute(result = as.integer(.fitted > 0.5),\n            actual = as.integer(actual)) %>%\n  table()##       actual\n## result  0  1\n##      0 32  4\n##      1 50 55\ntally(~cond + stock_photo, data = mariokart)##       stock_photo\n## cond   no yes\n##   new   4  55\n##   used 32  50"},{"path":"LOGREG.html","id":"multiple-logistic-regression","chapter":"32 Logistic Regression","heading":"32.3 Multiple logistic regression","text":"Let’s add total_pr model. model something done previous models learned .Notice use formula syntax done linear regression.summary, stock_photo total_pr statistically significant.Exercise:\nInterpret coefficient associated predictor total_pr.one dollar increase total price auction, odds ratio increases \\(exp(\\beta_2)\\), 1.21, given condition stock photo variable.similar interpretation multiple linear regression. specify predictors held constant increased variable interest one unit.Besides using individual predictor p-values assess model, can also use confusion matrix.new model, accuracy improved \\(71 + 43\\) 141 cases, 80.9.7%. Without measure variability, don’t know significant improvement just variability modeling procedure. surface, appears improvement.experiment improve model, let’s use quadratic term model.Using individual p-values, appears quadratic term significant marginal.get similar result use anova() function.Finally, confusion matrix results slight improvement accuracy 82.3%.Almost classifier error. model , decided okay allow 9%, 13 141, games sale classified new really used. wanted make little harder classify item new, use cutoff, threshold, 0.75. two effects. raises standard can classified new, reduces number used games classified new. However, also fail correctly classify increased fraction new games new, see code . matter complexity confidence might model, practical considerations absolutely crucial making helpful classification model. Without , actually harm good using statistical model. tradeoff similar one found Type 1 Type 2 errors. Notice accuracy also dropped slightly.machine learning course, learn better methods assess predictive accuracy well sophisticated methods transform adapt predictor variables.Exercise Find probability auctioned game new total price 50 uses stock photo.clear use coefficients regression output since R performing transformation total_pr variable. Let’s approach two ways. First use augment() function hard work.predict probability game new uses stock photo total price 50 69.3%.want recreate calculation, need use raw polynomial.can calculate link linear combination, inner product coefficients values.\\[\n-30.67 + 2.04 + 0.969 * 50 -0.007*50^2 = 0.814\n\\]Using inverse transform logit function, find probability game new given predictor values.\\[\n\\frac{\\ e^{.814}\\ }{\\ 1\\ +\\ e^{.814}\\ } = 0.693\n\\]","code":"\nmario_mod4 <- glm(cond == \"new\" ~ stock_photo + total_pr,\n                  data = mariokart, family = \"binomial\")\nsummary(mario_mod4)## \n## Call:\n## glm(formula = cond == \"new\" ~ stock_photo + total_pr, family = \"binomial\", \n##     data = mariokart)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.3699  -0.6479  -0.2358   0.6532   2.5794  \n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)    -11.31951    1.88333  -6.010 1.85e-09 ***\n## stock_photoyes   2.11633    0.68551   3.087  0.00202 ** \n## total_pr         0.19348    0.03562   5.431 5.60e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 191.70  on 140  degrees of freedom\n## Residual deviance: 119.21  on 138  degrees of freedom\n## AIC: 125.21\n## \n## Number of Fisher Scoring iterations: 5\naugment(mario_mod4, type.predict = \"response\") %>%\n  rename(actual = starts_with('cond')) %>%\n  transmute(result = as.integer(.fitted > 0.5),\n            actual = as.integer(actual)) %>%\n  table()##       actual\n## result  0  1\n##      0 71 16\n##      1 11 43\nmario_mod5 <- glm(cond == \"new\" ~ stock_photo + poly(total_pr, 2),\n                  data = mariokart, family = \"binomial\")\nsummary(mario_mod5)## \n## Call:\n## glm(formula = cond == \"new\" ~ stock_photo + poly(total_pr, 2), \n##     family = \"binomial\", data = mariokart)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1555  -0.6511  -0.1200   0.5987   2.6760  \n## \n## Coefficients:\n##                    Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)         -2.4407     0.6347  -3.845  0.00012 ***\n## stock_photoyes       2.0411     0.6494   3.143  0.00167 ** \n## poly(total_pr, 2)1  23.7534     4.5697   5.198 2.01e-07 ***\n## poly(total_pr, 2)2  -9.9724     4.1999  -2.374  0.01758 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 191.70  on 140  degrees of freedom\n## Residual deviance: 114.05  on 137  degrees of freedom\n## AIC: 122.05\n## \n## Number of Fisher Scoring iterations: 6\nanova(mario_mod4, mario_mod5, test = \"Chi\")## Analysis of Deviance Table\n## \n## Model 1: cond == \"new\" ~ stock_photo + total_pr\n## Model 2: cond == \"new\" ~ stock_photo + poly(total_pr, 2)\n##   Resid. Df Resid. Dev Df Deviance Pr(>Chi)  \n## 1       138     119.21                       \n## 2       137     114.05  1   5.1687    0.023 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\naugment(mario_mod5, type.predict = \"response\") %>%\n  rename(actual = starts_with('cond')) %>%\n  transmute(result = as.integer(.fitted > 0.5),\n            actual = as.integer(actual)) %>%\n  table()##       actual\n## result  0  1\n##      0 69 12\n##      1 13 47\naugment(mario_mod5, type.predict = \"response\") %>%\n  rename(actual = starts_with('cond')) %>%\n  transmute(result = as.integer(.fitted > 0.75),\n            actual = as.integer(actual)) %>%\n  table()##       actual\n## result  0  1\n##      0 78 22\n##      1  4 37\naugment(mario_mod5,\n        newdata = tibble(stock_photo = \"yes\", total_pr = 50),\n        type.predict = \"response\")## # A tibble: 1 x 3\n##   stock_photo total_pr .fitted\n##   <chr>          <dbl>   <dbl>\n## 1 yes               50   0.693\nmario_mod6 <- glm(cond == \"new\" ~ stock_photo + total_pr + I(total_pr^2),\n                  data = mariokart, family = \"binomial\")\ntidy(mario_mod6)## # A tibble: 4 x 5\n##   term            estimate std.error statistic  p.value\n##   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    -30.7       9.08        -3.38 0.000732\n## 2 stock_photoyes   2.04      0.649        3.14 0.00167 \n## 3 total_pr         0.969     0.343        2.83 0.00470 \n## 4 I(total_pr^2)   -0.00760   0.00320     -2.37 0.0176\ntidy(mario_mod6) %>%\n  select(estimate) %>% \n  pull() %*% c(1, 1, 50, 50^2)##           [,1]\n## [1,] 0.8140013\nexp(.814) / (1 + exp(.814))## [1] 0.6929612"},{"path":"LOGREG.html","id":"diagnostics-for-logistic-regression","chapter":"32 Logistic Regression","heading":"32.3.1 Diagnostics for logistic regression","text":"assumptions logistic regression diagnostic tools similar found linear regression. However, binary nature outcome, often need large data sets check. devote much time performing diagnostics logistic regression interested using predictive model. assumptions :predictor \\(x_i\\) linearly related logit\\((p_i)\\) predictors held constant. similar linear fit diagnostic linear multiple regression.outcome \\(Y_i\\) independent outcomes.influential data points.Multicollinearity minimal.","code":""},{"path":"LOGREG.html","id":"confidence-intervals-2","chapter":"32 Logistic Regression","heading":"32.4 Confidence intervals","text":"section generate confidence intervals. section experimental since sure () mosaic package work glm() function, let’s experiment.","code":""},{"path":"LOGREG.html","id":"confidence-intervals-for-a-parameter","chapter":"32 Logistic Regression","heading":"32.4.1 Confidence intervals for a parameter","text":"First, let’s use R built-function confint() find confidence interval simple logistic regression model coefficients.symmetric around estimate method using profile-likelihood method. can get symmetric intervals based central limit theorem using function confint.default().results close. recommend using profile-likelihood method.Now, let’s work () function determine can get similar results.looks like () performing expected. Let’s now perform one resample see happens., looks like expect. Now let’s bootstrap coefficients summarize results.Now plot bootstrap sampling distribution parameter associated total_pr.printout logistic regression model assumes normality sampling distribution total_pr coefficient, appears positively skewed, skewed right. 95% confidence interval found using cdata().result closer result profile-likelihood. Since interval include value zero, can 95% confident zero. close found using R function confint().","code":"\nconfint(mario_mod4)## Waiting for profiling to be done...##                      2.5 %     97.5 %\n## (Intercept)    -15.4048022 -7.9648042\n## stock_photoyes   0.8888216  3.6268545\n## total_pr         0.1297024  0.2705395\nconfint.default(mario_mod4)##                      2.5 %     97.5 %\n## (Intercept)    -15.0107641 -7.6282654\n## stock_photoyes   0.7727450  3.4599054\n## total_pr         0.1236583  0.2632982\ndo(1)*mario_mod4##   Intercept stock_photoyes  total_pr .row .index\n## 1 -11.31951       2.116325 0.1934783    1      1\ntidy(mario_mod4)## # A tibble: 3 x 5\n##   term           estimate std.error statistic       p.value\n##   <chr>             <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)     -11.3      1.88       -6.01 0.00000000185\n## 2 stock_photoyes    2.12     0.686       3.09 0.00202      \n## 3 total_pr          0.193    0.0356      5.43 0.0000000560\nset.seed(23)\ndo(1)*glm(cond == \"new\" ~ stock_photo + total_pr,\n          data = resample(mariokart), family = \"binomial\")##   Intercept stock_photoyes  total_pr .row .index\n## 1 -14.06559       4.945683 0.1940279    1      1\nset.seed(5011)\nresults <- do(1000)*glm(cond == \"new\" ~ stock_photo + total_pr, \n                        data = resample(mariokart), family = \"binomial\")## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nhead(results)##   Intercept stock_photoyes  total_pr .row .index\n## 1 -11.22155       1.665492 0.1986654    1      1\n## 2 -13.25708       1.889510 0.2371109    1      2\n## 3 -11.54544       2.871460 0.1867757    1      3\n## 4 -19.25785       5.816050 0.2829247    1      4\n## 5 -10.86631       3.255767 0.1672335    1      5\n## 6 -13.62425       1.842765 0.2533934    1      6\nresults %>%\n  gf_histogram(~total_pr, fill = \"cyan\", color = \"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title = \"Bootstrap sampling distribtuion\",\n          x = \"total price paramater estimate\")\ncdata(~total_pr, data = results)##          lower     upper central.p\n## 2.5% 0.1388783 0.3082659      0.95"},{"path":"LOGREG.html","id":"confidence-intervals-for-probability-of-success","chapter":"32 Logistic Regression","heading":"32.4.2 Confidence intervals for probability of success","text":"can use results bootstrap get confidence interval probability success. calculate confidence game stock photo total price $50. reminder, probability game new 0.69.key use coefficient resampled data set calculate probability success.95% confident expected probability game stock photo total price $50 50.4% 74.4%.","code":"\naugment(mario_mod5,\n        newdata = tibble(stock_photo = \"yes\", total_pr = 50),\n        type.predict = \"response\")## # A tibble: 1 x 3\n##   stock_photo total_pr .fitted\n##   <chr>          <dbl>   <dbl>\n## 1 yes               50   0.693\nhead(results)##   Intercept stock_photoyes  total_pr .row .index\n## 1 -11.22155       1.665492 0.1986654    1      1\n## 2 -13.25708       1.889510 0.2371109    1      2\n## 3 -11.54544       2.871460 0.1867757    1      3\n## 4 -19.25785       5.816050 0.2829247    1      4\n## 5 -10.86631       3.255767 0.1672335    1      5\n## 6 -13.62425       1.842765 0.2533934    1      6\nresults_pred <- results %>% \n  mutate(pred = 1 / (1 + exp(-1*(Intercept + stock_photoyes + 50*total_pr))))\ncdata(~pred, data = results_pred)##        lower     upper central.p\n## 2.5% 0.50388 0.7445598      0.95"},{"path":"LOGREG.html","id":"summary-4","chapter":"32 Logistic Regression","heading":"32.5 Summary","text":"chapter, learned extend linear models outcomes binary. built interpreted models. also used resampling find confidence intervals.","code":""},{"path":"LOGREG.html","id":"homework-problems-31","chapter":"32 Logistic Regression","heading":"32.6 Homework Problems","text":"Possum classificationLet’s investigate possum data set . time want model binary outcome variable. reminder, common brushtail possum Australia region bit cuter distant cousin, American opossum. consider 104 brushtail possums two regions Australia, possums may considered random sample population. first region Victoria, eastern half Australia traverses southern coast. second region consists New South Wales Queensland, make eastern northeastern Australia.use logistic regression differentiate possums two regions. outcome variable, called pop, takes value Vic possum Victoria New South Wales Queensland. consider five predictors: sex, head_l, skull_w, total_l, tail_l.Explore data making histograms boxplots quantitative variables, bar charts discrete variables.\noutliers likely large influence logistic regression model?Build logistic regression model variables. Report summary model.Using p-values decide want remove variable(s) build model.variable decide remove, build 95% confidence interval parameter.Explain remaining parameter estimates change two models.Write form model. Also identify following variables positively associated (controlling variables) possum Victoria: head_l, skull_w, total_l, tail_l.Suppose see brushtail possum zoo US, sign says possum captured wild Australia, doesn’t say part Australia. However, sign indicate possum male, skull 63 mm wide, tail 37 cm long, total length 83 cm. reduced model’s computed probability possum Victoria? confident model’s accuracy probability calculation?Medical school admissionThe file MedGPA.csv data folder information medical school admission status GPA standardized test scores gathered 55 medical school applicants liberal arts college Midwest.variables :Accept Status: =accepted medical school D=denied admissionAcceptance: Indicator Accept: 1=accepted 0=deniedSex: F=female M=maleBCPM: Bio/Chem/Physics/Math grade point averageGPA: College grade point averageVR: Verbal reasoning (subscore)PS: Physical sciences (subscore)WS: Writing sample (subcore)BS: Biological sciences (subscore)MCAT: Score MCAT exam (sum CR+PS+WS+BS)Apps: Number medical schools applied toBuild logistic regression model predict student denied admission GPA Sex.Generate 95% confidence interval coefficient associated GPA.Fit model polynomial degree 2 GPA. Drop Sex model. quadratic fit improve model?Fit model just GPA interpret coefficient.Try add different predictors come best model.Generate confusion matrix best model developed.Find 95% confidence interval probability female student 3.5 GPA, BCPM 3.8, verbal reasoning score 10, physical sciences score 9, writing sample score 8, biological score 10, MCAT score 40, applied 5 medical schools.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
