<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 27 Regression Diagnostics | Probability and Statistics for Scientists and Engineers</title>
<meta name="author" content="Matthew Davis">
<meta name="author" content="Brianna Hitt">
<meta name="author" content="Ken Horton">
<meta name="author" content="Kris Pruitt">
<meta name="author" content="Bradley Warner">
<meta name="description" content="27.1 Objectives Obtain and interpret \(R\)-squared and the \(F\)-statistic. Use R to evaluate the assumptions of a linear model. Identify and explain outliers and leverage points.  27.2...">
<meta name="generator" content="bookdown 0.31 with bs4_book()">
<meta property="og:title" content="Chapter 27 Regression Diagnostics | Probability and Statistics for Scientists and Engineers">
<meta property="og:type" content="book">
<meta property="og:image" content="/figures/Cover_Engineers.png">
<meta property="og:description" content="27.1 Objectives Obtain and interpret \(R\)-squared and the \(F\)-statistic. Use R to evaluate the assumptions of a linear model. Identify and explain outliers and leverage points.  27.2...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 27 Regression Diagnostics | Probability and Statistics for Scientists and Engineers">
<meta name="twitter:description" content="27.1 Objectives Obtain and interpret \(R\)-squared and the \(F\)-statistic. Use R to evaluate the assumptions of a linear model. Identify and explain outliers and leverage points.  27.2...">
<meta name="twitter:image" content="/figures/Cover_Engineers.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Probability and Statistics for Scientists and Engineers</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="objectives.html">Objectives</a></li>
<li class="book-part">Descriptive Statistical Modeling</li>
<li><a class="" href="CS1.html"><span class="header-section-number">1</span> Data Case Study</a></li>
<li><a class="" href="DB.html"><span class="header-section-number">2</span> Data Basics</a></li>
<li><a class="" href="ODCP.html"><span class="header-section-number">3</span> Overview of Data Collection Principles</a></li>
<li><a class="" href="STUDY.html"><span class="header-section-number">4</span> Studies</a></li>
<li><a class="" href="NUMDATA.html"><span class="header-section-number">5</span> Numerical Data</a></li>
<li><a class="" href="CATDATA.html"><span class="header-section-number">6</span> Categorical Data</a></li>
<li class="book-part">Probability Modeling</li>
<li><a class="" href="CS2.html"><span class="header-section-number">7</span> Probability Case Study</a></li>
<li><a class="" href="PROBRULES.html"><span class="header-section-number">8</span> Probability Rules</a></li>
<li><a class="" href="CONDPROB.html"><span class="header-section-number">9</span> Conditional Probability</a></li>
<li><a class="" href="RANDVAR.html"><span class="header-section-number">10</span> Random Variables</a></li>
<li><a class="" href="CONRANDVAR.html"><span class="header-section-number">11</span> Continuous Random Variables</a></li>
<li><a class="" href="DISCRETENAMED.html"><span class="header-section-number">12</span> Named Discrete Distributions</a></li>
<li><a class="" href="CONTNNAMED.html"><span class="header-section-number">13</span> Named Continuous Distributions</a></li>
<li><a class="" href="MULTIDISTS.html"><span class="header-section-number">14</span> Multivariate Distributions</a></li>
<li><a class="" href="MULTIEXP.html"><span class="header-section-number">15</span> Multivariate Expectation</a></li>
<li class="book-part">Inferential Statistical Modeling</li>
<li><a class="" href="CS3.html"><span class="header-section-number">16</span> Hypothesis Testing Case Study</a></li>
<li><a class="" href="HYPTESTSIM.html"><span class="header-section-number">17</span> Hypothesis Testing with Simulation</a></li>
<li><a class="" href="HYPTESTDIST.html"><span class="header-section-number">18</span> Hypothesis Testing with Known Distributions</a></li>
<li><a class="" href="HYPTESTCLT.html"><span class="header-section-number">19</span> Hypothesis Testing with the Central Limit Theorem</a></li>
<li><a class="" href="ADDTESTS.html"><span class="header-section-number">20</span> Additional Hypothesis Tests</a></li>
<li><a class="" href="ANOVA.html"><span class="header-section-number">21</span> Analysis of Variance</a></li>
<li><a class="" href="CI.html"><span class="header-section-number">22</span> Confidence Intervals</a></li>
<li><a class="" href="BOOT.html"><span class="header-section-number">23</span> Bootstrap</a></li>
<li class="book-part">Predictive Statistical Modeling</li>
<li><a class="" href="CS4.html"><span class="header-section-number">24</span> Linear Regression Case Study</a></li>
<li><a class="" href="LRBASICS.html"><span class="header-section-number">25</span> Linear Regression Basics</a></li>
<li><a class="" href="LRINF.html"><span class="header-section-number">26</span> Linear Regression Inference</a></li>
<li><a class="active" href="LRDIAG.html"><span class="header-section-number">27</span> Regression Diagnostics</a></li>
<li><a class="" href="LRSIM.html"><span class="header-section-number">28</span> Simulation-Based Linear Regression</a></li>
<li><a class="" href="LRMULTI.html"><span class="header-section-number">29</span> Multiple Linear Regression</a></li>
<li><a class="" href="LOGREG.html"><span class="header-section-number">30</span> Logistic Regression</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/DS-USAFA/Computational-Probability-and-Statistics">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="LRDIAG" class="section level1" number="27">
<h1>
<span class="header-section-number">27</span> Regression Diagnostics<a class="anchor" aria-label="anchor" href="#LRDIAG"><i class="fas fa-link"></i></a>
</h1>
<div id="objectives-27" class="section level2" number="27.1">
<h2>
<span class="header-section-number">27.1</span> Objectives<a class="anchor" aria-label="anchor" href="#objectives-27"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>Obtain and interpret <span class="math inline">\(R\)</span>-squared and the <span class="math inline">\(F\)</span>-statistic.</p></li>
<li><p>Use <code>R</code> to evaluate the assumptions of a linear model.</p></li>
<li><p>Identify and explain <em>outliers</em> and <em>leverage points</em>.</p></li>
</ol>
</div>
<div id="introduction-5" class="section level2" number="27.2">
<h2>
<span class="header-section-number">27.2</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-5"><i class="fas fa-link"></i></a>
</h2>
<p>Over the last two chapters, we have detailed simple linear regression. First, we described the model and its underlying assumptions. Next, we obtained parameter estimates using the method of least squares. Finally, we obtained the distributions of parameter estimates and used that information to conduct inference on parameters and predictions. Implementation was relatively straightforward; once we obtained the expressions of interest, we used <code>R</code> to find parameters estimates, interval estimates, etc. In this chapter we will explore more tools to assess the quality of our simple linear regression model. Some of these tools will generalize when we move to multiple predictors.</p>
</div>
<div id="assessing-our-model---understanding-the-output-from-lm" class="section level2" number="27.3">
<h2>
<span class="header-section-number">27.3</span> Assessing our model - understanding the output from <code>lm</code><a class="anchor" aria-label="anchor" href="#assessing-our-model---understanding-the-output-from-lm"><i class="fas fa-link"></i></a>
</h2>
<p>There is more that we can do with the output from the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function than just estimating parameters and predicting responses. There are metrics that allow us to assess the fit of our model. To explore some of these ideas let’s use the Starbucks example again.</p>
<p>First load the data:</p>
<div class="sourceCode" id="cb875"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">starbucks</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span><span class="st">"data/starbucks.csv"</span><span class="op">)</span></span></code></pre></div>
<p>Next build and summarize the model:</p>
<div class="sourceCode" id="cb876"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">star_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">calories</span><span class="op">~</span><span class="va">carb</span>,data<span class="op">=</span><span class="va">starbucks</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb877"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">star_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = calories ~ carb, data = starbucks)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -151.962  -70.556   -0.636   54.908  179.444 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 146.0204    25.9186   5.634 2.93e-07 ***
## carb          4.2971     0.5424   7.923 1.67e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 78.26 on 75 degrees of freedom
## Multiple R-squared:  0.4556, Adjusted R-squared:  0.4484 
## F-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11</code></pre>
<p>You may have noticed some other information that appeared in the summary of our model. We discussed the output in a previous chapter but let’s go a little more in depth.</p>
<div id="residual-standard-error" class="section level3" number="27.3.1">
<h3>
<span class="header-section-number">27.3.1</span> Residual Standard Error<a class="anchor" aria-label="anchor" href="#residual-standard-error"><i class="fas fa-link"></i></a>
</h3>
<p>The “residual standard error” is the estimate of <span class="math inline">\(\sigma\)</span>, the unexplained variance in our response. In our example, this turned out to be 78.26. If the assumptions of normality and constant variance are valid, we would expect the majority, 68%, of the observed values at a given input to be within <span class="math inline">\(\pm 78.26\)</span> (one standard deviation) of the mean value. We would expect 95% of the observed values at a given input to be within <span class="math inline">\(\pm 156.52\)</span> (two standard deviations) of the mean value.</p>
<p>If we want to extract just this value from the model object, first recognize that <code>summary(my.model)</code> is a list with several components:</p>
<div class="sourceCode" id="cb879"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">star_mod</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##  [1] "call"          "terms"         "residuals"     "coefficients" 
##  [5] "aliased"       "sigma"         "df"            "r.squared"    
##  [9] "adj.r.squared" "fstatistic"    "cov.unscaled"</code></pre>
<p>As expected, the <code>sigma</code> component shows the estimated value of <span class="math inline">\(\sigma\)</span>.</p>
<div class="sourceCode" id="cb881"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">star_mod</span><span class="op">)</span><span class="op">$</span><span class="va">sigma</span></span></code></pre></div>
<pre><code>## [1] 78.25956</code></pre>
<p>Again, this value is smaller the closer the points are to the regression fit. It is a measure of unexplained variance in the response variable.</p>
</div>
<div id="r-squared" class="section level3" number="27.3.2">
<h3>
<span class="header-section-number">27.3.2</span> R-squared<a class="anchor" aria-label="anchor" href="#r-squared"><i class="fas fa-link"></i></a>
</h3>
<p>Another quantity that appears is <span class="math inline">\(R\)</span>-squared, also know as the coefficient of determination. <span class="math inline">\(R\)</span>-squared is one measure of goodness of fit. Essentially <span class="math inline">\(R\)</span>-squared is a ratio of variance (in the response) explained by the model to overall variance of the response. It helps to describe the decomposition of variance:</p>
<p><span class="math display">\[
\underbrace{\sum_{i=1}^n (y_i-\bar{y})^2}_{SS_{\text{Total}}} = \underbrace{\sum_{i=1}^n (\hat{y}_i-\bar y)^2}_{SS_{\text{Regression}}}+\underbrace{\sum_{i=1}^n(y_i-\hat{y}_i)^2}_{SS_{\text{Error}}}
\]</span></p>
<p>In other words, the overall variation in <span class="math inline">\(y\)</span> can be separated into two parts: variation due to the linear relationship between <span class="math inline">\(y\)</span> and the predictor variable(s), called <span class="math inline">\(SS_\text{Regression}\)</span>, and residual variation (due to random scatter or perhaps a poorly chosen model), called <span class="math inline">\(SS_\text{Error}\)</span>. Note: <span class="math inline">\(SS_\text{Error}\)</span> is used to estimate residual standard error in the previous section.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="math inline"&gt;\(\hat{\sigma}=\sqrt\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{\text{degrees of freedom}}\)&lt;/span&gt;&lt;/p&gt;'><sup>110</sup></a></p>
<p><span class="math inline">\(R\)</span>-squared simply measures the ratio between <span class="math inline">\(SS_\text{Regression}\)</span> and <span class="math inline">\(SS_\text{Total}\)</span>. A common definition of <span class="math inline">\(R\)</span>-squared is the proportion of overall variation in the response that is explained by the linear model. <span class="math inline">\(R\)</span>-squared can be between 0 and 1. Values of <span class="math inline">\(R\)</span>-squared close to 1 indicate a tight fit (little scatter) around the estimated regression line. Value close to 0 indicate the opposite (large remaining scatter).</p>
<p>We can obtain <span class="math inline">\(R\)</span>-squared “by hand” or by using the output of the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function:</p>
<div class="sourceCode" id="cb883"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">star_mod</span><span class="op">)</span><span class="op">$</span><span class="va">r.squared</span></span></code></pre></div>
<pre><code>## [1] 0.4556237</code></pre>
<p>For simple linear regression, <span class="math inline">\(R\)</span>-squared is related to <strong>correlation</strong>. We can compute the correlation using a formula, just as we did with the sample mean and standard deviation. However, this formula is rather complex,<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Formally, we can compute the correlation for observations &lt;span class="math inline"&gt;\((x_1, y_1)\)&lt;/span&gt;, &lt;span class="math inline"&gt;\((x_2, y_2)\)&lt;/span&gt;, …, &lt;span class="math inline"&gt;\((x_n, y_n)\)&lt;/span&gt; using the formula
&lt;span class="math display"&gt;\[
R = \frac{1}{n-1}\sum_{i=1}^{n} \frac{x_i-\bar{x}}{s_x}\frac{y_i-\bar{y}}{s_y}
\]&lt;/span&gt;
where &lt;span class="math inline"&gt;\(\bar{x}\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\bar{y}\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(s_x\)&lt;/span&gt;, and &lt;span class="math inline"&gt;\(s_y\)&lt;/span&gt; are the sample means and standard deviations for each variable.&lt;/p&gt;'><sup>111</sup></a> so we let <code>R</code> do the heavy lifting for us.</p>
<div class="sourceCode" id="cb885"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">starbucks</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>correlation<span class="op">=</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/aggregating.html">cor</a></span><span class="op">(</span><span class="va">carb</span>,<span class="va">calories</span><span class="op">)</span>,correlation_squared<span class="op">=</span><span class="va">correlation</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   correlation correlation_squared
##         &lt;dbl&gt;               &lt;dbl&gt;
## 1       0.675               0.456</code></pre>
<p>As a review, Figure <a href="LRDIAG.html#fig:cor-fig">27.1</a> below shows eight plots and their corresponding correlations. Only when the relationship is perfectly linear is the correlation either -1 or 1. If the relationship is strong and positive, the correlation will be near +1. If it is strong and negative, it will be near -1. If there is no apparent linear relationship between the variables, then the correlation will be near zero.</p>
<div class="figure">
<span style="display:block;" id="fig:cor-fig"></span>
<img src="29-Linear-Regression-Diagnostics_files/figure-html/cor-fig-1.png" alt="Scatterplots demonstrating different correlations." width="672"><p class="caption">
Figure 27.1: Scatterplots demonstrating different correlations.
</p>
</div>
<blockquote>
<p><strong>Exercise</strong><br>
If a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the response is explained by the explanatory variable?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;About &lt;span class="math inline"&gt;\(R^2 = (-0.97)^2 = 0.94\)&lt;/span&gt; or 94% of the variation is explained by the linear model.&lt;/p&gt;'><sup>112</sup></a></p>
</blockquote>
<p>Note that one of the components of <code>summary(lm())</code> function is <code>adj.r.squared</code>. This is a value of <span class="math inline">\(R\)</span>-squared adjusted for number of predictors. This idea is covered more in depth in a machine learning course.</p>
</div>
<div id="f-statistic" class="section level3" number="27.3.3">
<h3>
<span class="header-section-number">27.3.3</span> F-Statistic<a class="anchor" aria-label="anchor" href="#f-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>Another quantity that appears in the summary of the model is the <span class="math inline">\(F\)</span>-statistic. This value evaluates the null hypothesis that all of the non-intercept coefficients are equal to 0. Rejecting this hypothesis implies that the model is useful in the sense that at least one of the predictors shares a significant linear relationship with the response.</p>
<p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span><br><span class="math inline">\(H_a\)</span>: At least one coefficient not equal to 0.</p>
<p>where <span class="math inline">\(p\)</span> is the number of predictors in the model. Just like in ANOVA, this is a simultaneous test of all coefficients and does not inform us which one(s) are different from 0.</p>
<p>The <span class="math inline">\(F\)</span>-statistic is given by
<span class="math display">\[
{n-p-1 \over p}{\sum (\hat{y}_i-\bar{y})^2\over \sum e_i^2}
\]</span></p>
<p>Under the null hypothesis, the <span class="math inline">\(F\)</span>-statistic follows the <span class="math inline">\(F\)</span> distribution with parameters <span class="math inline">\(p\)</span> and <span class="math inline">\(n-p-1\)</span>.</p>
<p>In our example, the <span class="math inline">\(F\)</span>-statistic is redundant since there is only one predictor. In fact, the <span class="math inline">\(p\)</span>-value associated with the <span class="math inline">\(F\)</span>-statistic is equal to the <span class="math inline">\(p\)</span>-value associated with the estimate of <span class="math inline">\(\beta_1\)</span>. However, when we move to cases with more predictor variables, we may be interested in the <span class="math inline">\(F\)</span>-statistic.</p>
<div class="sourceCode" id="cb887"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">star_mod</span><span class="op">)</span><span class="op">$</span><span class="va">fstatistic</span></span></code></pre></div>
<pre><code>##    value    numdf    dendf 
## 62.77234  1.00000 75.00000</code></pre>
</div>
</div>
<div id="regression-diagnostics" class="section level2" number="27.4">
<h2>
<span class="header-section-number">27.4</span> Regression diagnostics<a class="anchor" aria-label="anchor" href="#regression-diagnostics"><i class="fas fa-link"></i></a>
</h2>
<p>Finally, we can use the <code>lm</code> object to check the assumptions of the model. We have discussed the assumptions before but in this chapter we will use <code>R</code> to generate visual checks. There are also numeric diagnostic measures.</p>
<p>There are several potential problems with a regression model:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Assumptions about the error structure</strong>. We assume:<br>
</li>
</ol>
<ul>
<li>the errors are normally distributed<br>
</li>
<li>the errors are independent<br>
</li>
<li>the errors have constant variance, <strong>homoskedastic</strong>
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><p><strong>Assumptions about the fit</strong>. We assume that fit of the model is correct. For a simple linear regression, this means that fit specified by the formula in <code>lm</code> is correct.</p></li>
<li><p><strong>Problems with outliers and leverage points</strong>. In this case a small number of points in the data could have an unusually large impact on the parameter estimates. These points may give a mistaken sense that our model has a great fit or conversely that there is not relationship between the variables.</p></li>
<li><p><strong>Missing predictors</strong>. We can potentially improve the fit and predictive performance of our model by including other predictors. We will spend one chapter on this topic, but machine learning courses devote more time to discussing how to build these more complex models. In the case of multivariate linear regression, many of the diagnostic tools discussed next will also be applicable.</p></li>
</ol>
<div id="residual-plots-1" class="section level3" number="27.4.1">
<h3>
<span class="header-section-number">27.4.1</span> Residual plots<a class="anchor" aria-label="anchor" href="#residual-plots-1"><i class="fas fa-link"></i></a>
</h3>
<p>The assumptions about the error structure can be checked with residual plots. We have already done this, but let’s review again and provide a little more depth.</p>
<p>Applying the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function to an “lm” object provides several graphs that allow us to visually evaluate a linear model’s assumptions. There are actually six plots (selected by the <code>which</code> option) available:</p>
<ul>
<li>a plot of residuals against fitted values,<br>
</li>
<li>a Scale-Location plot of <span class="math inline">\(\sqrt(| \text{residuals} |)\)</span> against fitted values,<br>
</li>
<li>a Normal Q-Q plot,<br>
</li>
<li>a plot of Cook’s distances versus row labels,<br>
</li>
<li>a plot of residuals against leverages,<br>
</li>
<li>and a plot of Cook’s distances against leverage/(1-leverage).</li>
</ul>
<p>By default, the first three and the fifth are provided by applying <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> to an “lm” object. To obtain all four at once, simply use <code>plot(my.model)</code> at the command line, Figure <a href="LRDIAG.html#fig:diag281-fig">27.2</a>.</p>
<div class="sourceCode" id="cb889"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">star_mod</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:diag281-fig"></span>
<img src="29-Linear-Regression-Diagnostics_files/figure-html/diag281-fig-1.png" alt="Regression diagnostic plots." width="50%"><img src="29-Linear-Regression-Diagnostics_files/figure-html/diag281-fig-2.png" alt="Regression diagnostic plots." width="50%"><img src="29-Linear-Regression-Diagnostics_files/figure-html/diag281-fig-3.png" alt="Regression diagnostic plots." width="50%"><img src="29-Linear-Regression-Diagnostics_files/figure-html/diag281-fig-4.png" alt="Regression diagnostic plots." width="50%"><p class="caption">
Figure 27.2: Regression diagnostic plots.
</p>
</div>
<p>However, it’s best to walk through each of these four plots in our Starbucks example.</p>
</div>
<div id="residuals-vs-fitted" class="section level3" number="27.4.2">
<h3>
<span class="header-section-number">27.4.2</span> Residuals vs Fitted<a class="anchor" aria-label="anchor" href="#residuals-vs-fitted"><i class="fas fa-link"></i></a>
</h3>
<p>By providing a number in the <code>which</code> option, we can select the plot we want, Figure <a href="LRDIAG.html#fig:diag282-fig">27.3</a> is the first diagnostic plot.</p>
<div class="sourceCode" id="cb890"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">star_mod</span>,which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:diag282-fig"></span>
<img src="29-Linear-Regression-Diagnostics_files/figure-html/diag282-fig-1.png" alt="A diagnostic residual plot." width="672"><p class="caption">
Figure 27.3: A diagnostic residual plot.
</p>
</div>
<p>This plot assesses linearity of the model and homoscedasticity (constant variance). The red line is a smoothed estimate of the fitted values versus the residuals. Ideally, the red line should coincide with the dashed horizontal line and the residuals should be centered around this dashed line. This would indicate that a linear fit is appropriate. Furthermore, the scatter around the dashed line should be relatively constant across the plot, homoscedasticity. In this case, it looks like there is some minor concern over linearity and non-constant error variance. We noted this earlier with the cluster of points in the lower left hand corner of the scatterplot.</p>
<p>Note: the points that are labeled are points with a high residual value. They are extreme. We will discuss outliers and leverage points shortly.</p>
</div>
<div id="normal-q-q-plot" class="section level3" number="27.4.3">
<h3>
<span class="header-section-number">27.4.3</span> Normal Q-Q Plot<a class="anchor" aria-label="anchor" href="#normal-q-q-plot"><i class="fas fa-link"></i></a>
</h3>
<p>As it’s name suggests, this plot evaluates the normality of the residuals. We have seen and used this plot several times in this book. Remember if the number of data points is small, this plot has a greatly reduced effectiveness.</p>
<div class="sourceCode" id="cb891"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">star_mod</span>,which <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:diag283-fig"></span>
<img src="29-Linear-Regression-Diagnostics_files/figure-html/diag283-fig-1.png" alt="The quantile-quantile plot for checking normality." width="672"><p class="caption">
Figure 27.4: The quantile-quantile plot for checking normality.
</p>
</div>
<p>Along the <span class="math inline">\(y\)</span>-axis are the actual standardized residuals. Along the <span class="math inline">\(x\)</span>-axis is where those points should be if the residuals were actually normally distributed. Ideally, the dots should fall along the diagonal dashed line. In Figure <a href="LRDIAG.html#fig:diag283-fig">27.4</a>, it appears there is some skewness to the right or just longer tails than a normal distribution. We can tell this because for the smaller residuals, they don’t increase as they should to match a normal distribution, the points are above the line. This is concerning.</p>
</div>
<div id="scale-location-plot" class="section level3" number="27.4.4">
<h3>
<span class="header-section-number">27.4.4</span> Scale-Location Plot<a class="anchor" aria-label="anchor" href="#scale-location-plot"><i class="fas fa-link"></i></a>
</h3>
<p>The scale-location plot is a better indicator of non-constant error variance. It is a plot of fitted values versus square root of the absolute value of the standardized residuals. A standardized residual is the residual divided by its standard deviation</p>
<p><span class="math display">\[
e^{'}_i=\frac{e_i}{s}
\]</span></p>
<p>This plot illustrates the spread of the residuals over the entire range of the predictor. We are using fitted values because this will generalize well if we have more than one predictor.</p>
<div class="sourceCode" id="cb892"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">star_mod</span>,which<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:diag284-fig"></span>
<img src="29-Linear-Regression-Diagnostics_files/figure-html/diag284-fig-1.png" alt="A scale-location diagnostic residual plot." width="672"><p class="caption">
Figure 27.5: A scale-location diagnostic residual plot.
</p>
</div>
<p>A straight horizontal red line indicates constant error variance. In this case, Figure <a href="LRDIAG.html#fig:diag284-fig">27.5</a>, there is some indication error variance is higher for lower carb counts.</p>
</div>
</div>
<div id="outliers-and-leverage" class="section level2" number="27.5">
<h2>
<span class="header-section-number">27.5</span> Outliers and leverage<a class="anchor" aria-label="anchor" href="#outliers-and-leverage"><i class="fas fa-link"></i></a>
</h2>
<p>Before discussing the last plot, we need to spend some time discussing outliers. Outliers in regression are observations that fall far from the “cloud” of points. These points are especially important because they can have a strong influence on the least squares line.</p>
<p>In regression, there are two types of outliers:<br>
- An outlier in the response variable is one that is not predicted well by the model. This could either be a problem with the data or the model. The residuals for this outlier will be large in absolute value.<br>
- An outlier in the explanatory variable. These are typically called <strong>leverage points</strong> because they can have a undue impact on the parameter estimates. With multiple predictors, we can have a leverage point when we have an unusual combination of the predictors.</p>
<p>An outlier is a <strong>influential point</strong> if it drastically alters the regression output. For example by causing large changes in the estimated slope or hypothesis <span class="math inline">\(p\)</span>-values, if it is omitted.</p>
<blockquote>
<p><strong>Exercise</strong>:<br>
There are six plots shown in Figure <a href="LRDIAG.html#fig:resid282-fig">27.6</a> along with the least squares line and residual plots. For each scatterplot and residual plot pair, identify any obvious outliers and note how they influence the least squares line. Recall that an outlier is any point that doesn’t appear to belong with the vast majority of the other points.</p>
</blockquote>
<div class="figure">
<span style="display:block;" id="fig:resid282-fig"></span>
<img src="29-Linear-Regression-Diagnostics_files/figure-html/resid282-fig-1.png" alt="Examples of outliers and leverage points." width="672"><p class="caption">
Figure 27.6: Examples of outliers and leverage points.
</p>
</div>
<ol style="list-style-type: decimal">
<li>There is one outlier far from the other points, though it only appears to slightly influence the line. This is an outlier in the response and will have a large residual in magnitude.<br>
</li>
<li>There is one outlier on the right, though it is quite close to the least squares line, which suggests it wasn’t very influential although it is a leverage point.<br>
</li>
<li>There is one point far away from the cloud, and this leverage point appears to pull the least squares line up on the right; examine how the line around the primary cloud doesn’t appear to fit very well. This point has a high influence on the estimated slope.<br>
</li>
<li>There is a primary cloud and then a small secondary cloud of four outliers. The secondary cloud appears to be influencing the line somewhat strongly, making the least square line fit poorly almost everywhere. There might be an interesting explanation for the dual clouds, which is something that could be investigated.<br>
</li>
<li>There is no obvious trend in the main cloud of points and the outlier on the right appears to largely control the slope of the least squares line. This point is an outlier in both the response and predictor. It is a highly influential point.<br>
</li>
<li>There is one outlier in both the response and predictor, thus a leverage point, far from the cloud, however, it falls quite close to the least squares line and does not appear to be very influential.</li>
</ol>
<p>Examining the residual plots in Figure <a href="LRDIAG.html#fig:resid282-fig">27.6</a>, you will probably find that there is some trend in the main clouds of (3) and (4). In these cases, the outliers influenced the slope of the least squares lines. In (5), data with no clear trend were assigned a line with a large trend simply due to one outlier!</p>
<blockquote>
<p>Leverage<br>
Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with <strong>high leverage</strong>.</p>
</blockquote>
<p>Points that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line. If one of these high leverage points does appear to actually invoke its influence on the slope of the line – as in cases (3), (4), and (5) – then we call it an <strong>influential point</strong>. Usually we can say a point is influential if, had we fitted the line without it, the influential point would have been unusually far from the least squares line. Leverage can be calculated from what is called the <strong>hat matrix</strong>, the actual mathematics is beyond the scope of this book.</p>
<p>A point can be an outlier but not a leverage point as we have already discussed. It is tempting to remove outliers from your data set. Don’t do this without a very good reason. Models that ignore exceptional (and interesting) cases often perform poorly. For instance, if a financial firm ignored the largest market swings – the ``outliers’’ – they would soon go bankrupt by making poorly thought-out investments.</p>
<div id="residuals-vs-leverage-plot" class="section level3" number="27.5.1">
<h3>
<span class="header-section-number">27.5.1</span> Residuals vs Leverage Plot<a class="anchor" aria-label="anchor" href="#residuals-vs-leverage-plot"><i class="fas fa-link"></i></a>
</h3>
<p>The residuals vs leverage plot is a good way to identify influential observations. Sometimes, influential observations are representative of the population, but they could also indicate an error in recording data, or an otherwise unrepresentative outlier. It could be worth looking into these cases.</p>
<div class="sourceCode" id="cb893"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">star_mod</span>,<span class="fl">5</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:diag286-fig"></span>
<img src="29-Linear-Regression-Diagnostics_files/figure-html/diag286-fig-1.png" alt="Diagnostic plots for Starbucks regression model." width="672"><p class="caption">
Figure 27.7: Diagnostic plots for Starbucks regression model.
</p>
</div>
<p>Figure <a href="LRDIAG.html#fig:diag286-fig">27.7</a> helps us to find influential cases, those leverage points that impact the estimated slope. Unlike the other plots, patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of a dashed line, Cook’s distance. In our particular plot, a dotted line for Cook’s distance was outside the bounds of the plot and thus did not come into play. When cases are outside of the Cook’s distance (meaning they have high Cook’s distance scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases. In this example, there are no points that tend to have undue influence.</p>
</div>
<div id="what-if-our-assumptions-are-violated" class="section level3" number="27.5.2">
<h3>
<span class="header-section-number">27.5.2</span> What If Our Assumptions Are Violated<a class="anchor" aria-label="anchor" href="#what-if-our-assumptions-are-violated"><i class="fas fa-link"></i></a>
</h3>
<p>If the assumptions of the model are violated and/or we have influential points, a linear regression model with normality assumptions is not appropriate. Sometimes it is appropriate to transform the data (either response or predictor), so that the assumptions are met on the transformed data. Other times, it is appropriate to explore other models. There are entire courses on regression where blocks of material are devoted to diagnostics and transformations to reduce the impact of violations of assumptions. We will not go into these methods in this book. Instead, when confronted with clear violated assumptions, we will use resampling as a possible solution. We will learn about this in the next chapter because it does not assume normality in the residuals. This is a limited solution, but as this is an introductory text, this is an excellent first step.</p>
</div>
</div>
<div id="homework-problems-26" class="section level2" number="27.6">
<h2>
<span class="header-section-number">27.6</span> Homework Problems<a class="anchor" aria-label="anchor" href="#homework-problems-26"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>Identify relationships</li>
</ol>
<p>For each of the six plots in Figure <a href="LRDIAG.html#fig:hw1">27.8</a>, identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable. When we ask about the strength of the relationship, we mean:</p>
<ul>
<li>is there a relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and</li>
<li>does that relationship explain most of the variance?</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:hw1"></span>
<img src="figures/association1.png" alt="Homework problem 1." width="33%"><img src="figures/association2.png" alt="Homework problem 1." width="33%"><img src="figures/association3.png" alt="Homework problem 1." width="33%"><img src="figures/association4.png" alt="Homework problem 1." width="33%"><img src="figures/association5.png" alt="Homework problem 1." width="33%"><img src="figures/association6.png" alt="Homework problem 1." width="33%"><p class="caption">
Figure 27.8: Homework problem 1.
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Beer and blood alcohol content</li>
</ol>
<p>We will use the blood alcohol content data again. As a reminder this is the description of the data: <em>Many people believe that gender, weight, drinking habits, and many other factors are much more important in predicting blood alcohol content (BAC) than simply considering the number of drinks a person consumed. Here we examine data from sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer. These students were evenly divided between men and women, and they differed in weight and drinking habits. Thirty minutes later, a police officer measured their blood alcohol content (BAC) in grams of alcohol per deciliter of blood.</em></p>
<p>The data is in the <code>bac.csv</code> file under the <code>data</code> folder.</p>
<ol style="list-style-type: lower-alpha">
<li>Obtain and interpret <span class="math inline">\(R\)</span>-squared for this model.<br>
</li>
<li>Evaluate the assumptions of this model. Do we have anything to be concerned about?</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Outliers</li>
</ol>
<p>Identify the outliers in the scatterplots shown in Figure <a href="LRDIAG.html#fig:hw3">27.9</a> and determine what type of outliers they are. Explain your reasoning.</p>
<div class="figure">
<span style="display:block;" id="fig:hw3"></span>
<img src="figures/outInf1.png" alt="Homework problem 3." width="33%"><img src="figures/outLev1.png" alt="Homework problem 3." width="33%"><img src="figures/outOut1.png" alt="Homework problem 3." width="33%"><img src="figures/outInf2.png" alt="Homework problem 3." width="33%"><img src="figures/outInf3.png" alt="Homework problem 3." width="33%"><img src="figures/outOut2.png" alt="Homework problem 3." width="33%"><p class="caption">
Figure 27.9: Homework problem 3.
</p>
</div>
</div>
<div id="solutions-manual-26" class="section level2 unnumbered">
<h2>
<a href="https://ds-usafa.github.io/PSSE-Solutions-Manual/LRDIAG.html">Solutions Manual</a><a class="anchor" aria-label="anchor" href="#solutions-manual-26"><i class="fas fa-link"></i></a>
</h2>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="LRINF.html"><span class="header-section-number">26</span> Linear Regression Inference</a></div>
<div class="next"><a href="LRSIM.html"><span class="header-section-number">28</span> Simulation-Based Linear Regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#LRDIAG"><span class="header-section-number">27</span> Regression Diagnostics</a></li>
<li><a class="nav-link" href="#objectives-27"><span class="header-section-number">27.1</span> Objectives</a></li>
<li><a class="nav-link" href="#introduction-5"><span class="header-section-number">27.2</span> Introduction</a></li>
<li>
<a class="nav-link" href="#assessing-our-model---understanding-the-output-from-lm"><span class="header-section-number">27.3</span> Assessing our model - understanding the output from lm</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#residual-standard-error"><span class="header-section-number">27.3.1</span> Residual Standard Error</a></li>
<li><a class="nav-link" href="#r-squared"><span class="header-section-number">27.3.2</span> R-squared</a></li>
<li><a class="nav-link" href="#f-statistic"><span class="header-section-number">27.3.3</span> F-Statistic</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#regression-diagnostics"><span class="header-section-number">27.4</span> Regression diagnostics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#residual-plots-1"><span class="header-section-number">27.4.1</span> Residual plots</a></li>
<li><a class="nav-link" href="#residuals-vs-fitted"><span class="header-section-number">27.4.2</span> Residuals vs Fitted</a></li>
<li><a class="nav-link" href="#normal-q-q-plot"><span class="header-section-number">27.4.3</span> Normal Q-Q Plot</a></li>
<li><a class="nav-link" href="#scale-location-plot"><span class="header-section-number">27.4.4</span> Scale-Location Plot</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#outliers-and-leverage"><span class="header-section-number">27.5</span> Outliers and leverage</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#residuals-vs-leverage-plot"><span class="header-section-number">27.5.1</span> Residuals vs Leverage Plot</a></li>
<li><a class="nav-link" href="#what-if-our-assumptions-are-violated"><span class="header-section-number">27.5.2</span> What If Our Assumptions Are Violated</a></li>
</ul>
</li>
<li><a class="nav-link" href="#homework-problems-26"><span class="header-section-number">27.6</span> Homework Problems</a></li>
<li><a class="nav-link" href="#solutions-manual-26">Solutions Manual</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/DS-USAFA/Computational-Probability-and-Statistics/blob/main/29-Linear-Regression-Diagnostics.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/DS-USAFA/Computational-Probability-and-Statistics/edit/main/29-Linear-Regression-Diagnostics.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Probability and Statistics for Scientists and Engineers</strong>" was written by Matthew Davis, Brianna Hitt, Ken Horton, Kris Pruitt, Bradley Warner. It was last built on 2023-01-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
