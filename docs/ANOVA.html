<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 21 Analysis of Variance | Probability and Statistics for Scientists and Engineers</title>
<meta name="author" content="Matthew Davis">
<meta name="author" content="Brianna Hitt">
<meta name="author" content="Ken Horton">
<meta name="author" content="Kris Pruitt">
<meta name="author" content="Bradley Warner">
<meta name="description" content="21.1 Objectives Conduct and interpret a hypothesis test for equality of two or more means using both permutation and the \(F\) distribution. Know and check the assumptions for ANOVA.  21.2...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 21 Analysis of Variance | Probability and Statistics for Scientists and Engineers">
<meta property="og:type" content="book">
<meta property="og:image" content="/figures/Cover_Engineers.png">
<meta property="og:description" content="21.1 Objectives Conduct and interpret a hypothesis test for equality of two or more means using both permutation and the \(F\) distribution. Know and check the assumptions for ANOVA.  21.2...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 21 Analysis of Variance | Probability and Statistics for Scientists and Engineers">
<meta name="twitter:description" content="21.1 Objectives Conduct and interpret a hypothesis test for equality of two or more means using both permutation and the \(F\) distribution. Know and check the assumptions for ANOVA.  21.2...">
<meta name="twitter:image" content="/figures/Cover_Engineers.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Probability and Statistics for Scientists and Engineers</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="objectives.html">Objectives</a></li>
<li class="book-part">Descriptive Statistical Modeling</li>
<li><a class="" href="CS1.html"><span class="header-section-number">1</span> Data Case Study</a></li>
<li><a class="" href="DB.html"><span class="header-section-number">2</span> Data Basics</a></li>
<li><a class="" href="ODCP.html"><span class="header-section-number">3</span> Overview of Data Collection Principles</a></li>
<li><a class="" href="STUDY.html"><span class="header-section-number">4</span> Studies</a></li>
<li><a class="" href="NUMDATA.html"><span class="header-section-number">5</span> Numerical Data</a></li>
<li><a class="" href="CATDATA.html"><span class="header-section-number">6</span> Categorical Data</a></li>
<li class="book-part">Probability Modeling</li>
<li><a class="" href="CS2.html"><span class="header-section-number">7</span> Probability Case Study</a></li>
<li><a class="" href="PROBRULES.html"><span class="header-section-number">8</span> Probability Rules</a></li>
<li><a class="" href="CONDPROB.html"><span class="header-section-number">9</span> Conditional Probability</a></li>
<li><a class="" href="RANDVAR.html"><span class="header-section-number">10</span> Random Variables</a></li>
<li><a class="" href="CONRANDVAR.html"><span class="header-section-number">11</span> Continuous Random Variables</a></li>
<li><a class="" href="DISCRETENAMED.html"><span class="header-section-number">12</span> Named Discrete Distributions</a></li>
<li><a class="" href="CONTNNAMED.html"><span class="header-section-number">13</span> Named Continuous Distributions</a></li>
<li><a class="" href="MULTIDISTS.html"><span class="header-section-number">14</span> Multivariate Distributions</a></li>
<li><a class="" href="MULTIEXP.html"><span class="header-section-number">15</span> Multivariate Expectation</a></li>
<li class="book-part">Inferential Statistical Modeling</li>
<li><a class="" href="CS3.html"><span class="header-section-number">16</span> Hypothesis Testing Case Study</a></li>
<li><a class="" href="HYPTESTSIM.html"><span class="header-section-number">17</span> Hypothesis Testing with Simulation</a></li>
<li><a class="" href="HYPTESTDIST.html"><span class="header-section-number">18</span> Hypothesis Testing with Known Distributions</a></li>
<li><a class="" href="HYPTESTCLT.html"><span class="header-section-number">19</span> Hypothesis Testing with the Central Limit Theorem</a></li>
<li><a class="" href="ADDTESTS.html"><span class="header-section-number">20</span> Additional Hypothesis Tests</a></li>
<li><a class="active" href="ANOVA.html"><span class="header-section-number">21</span> Analysis of Variance</a></li>
<li><a class="" href="CI.html"><span class="header-section-number">22</span> Confidence Intervals</a></li>
<li><a class="" href="BOOT.html"><span class="header-section-number">23</span> Bootstrap</a></li>
<li class="book-part">Predictive Statistical Modeling</li>
<li><a class="" href="CS4.html"><span class="header-section-number">24</span> Linear Regression Case Study</a></li>
<li><a class="" href="LRBASICS.html"><span class="header-section-number">25</span> Linear Regression Basics</a></li>
<li><a class="" href="LRINF.html"><span class="header-section-number">26</span> Linear Regression Inference</a></li>
<li><a class="" href="LRDIAG.html"><span class="header-section-number">27</span> Regression Diagnostics</a></li>
<li><a class="" href="LRSIM.html"><span class="header-section-number">28</span> Simulation-Based Linear Regression</a></li>
<li><a class="" href="LRMULTI.html"><span class="header-section-number">29</span> Multiple Linear Regression</a></li>
<li><a class="" href="LOGREG.html"><span class="header-section-number">30</span> Logistic Regression</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/DS-USAFA/Computational-Probability-and-Statistics">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ANOVA" class="section level1" number="21">
<h1>
<span class="header-section-number">21</span> Analysis of Variance<a class="anchor" aria-label="anchor" href="#ANOVA"><i class="fas fa-link"></i></a>
</h1>
<div id="objectives-21" class="section level2" number="21.1">
<h2>
<span class="header-section-number">21.1</span> Objectives<a class="anchor" aria-label="anchor" href="#objectives-21"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>Conduct and interpret a hypothesis test for equality of two or more means using both permutation and the <span class="math inline">\(F\)</span> distribution.</p></li>
<li><p>Know and check the assumptions for ANOVA.</p></li>
</ol>
</div>
<div id="introduction-3" class="section level2" number="21.2">
<h2>
<span class="header-section-number">21.2</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-3"><i class="fas fa-link"></i></a>
</h2>
<p>In the last chapter, we learned about the chi-squared distribution, and used both mathematically derived tests (Pearson’s chi-squared test) and randomization tests to determine whether two categorical variables are independent. We also examined settings with one categorical and one numerical variable, testing for equality of means and equality of variances in two samples.</p>
<p>In this chapter, we will learn how to compare more than two means simultaneously.</p>
</div>
<div id="comparing-more-than-two-means" class="section level2" number="21.3">
<h2>
<span class="header-section-number">21.3</span> Comparing more than two means<a class="anchor" aria-label="anchor" href="#comparing-more-than-two-means"><i class="fas fa-link"></i></a>
</h2>
<p>In contrast to last chapter, we now want to compare means across more than two groups. Again, we have two variables, where one is continuous (numerical) and the other is categorical. We might initially think to do pairwise comparisons, such as two sample t-tests, as a solution. Suppose we have three groups for which we want to compare means. We might be tempted to compare the first mean with the second, then the first mean with the third, and then finally compare the second and third means, for a total of three comparisons. However, this strategy can be treacherous. If we have many groups and do many comparisons, the Type 1 error is inflated and it is likely that we will eventually find a difference just by chance, even if there is no difference in the populations.</p>
<p>In this chapter, we will learn a new method called <strong>analysis of variance</strong> (ANOVA) and a new test statistic called the <span class="math inline">\(F\)</span> statistic. ANOVA uses a single hypothesis test to determine whether the means across many groups are equal. The hypotheses are:</p>
<p><span class="math inline">\(H_0\)</span>: The mean outcome is the same across all groups. In statistical notation, <span class="math inline">\(\mu_1 = \mu_2 = \cdots = \mu_k\)</span> where <span class="math inline">\(\mu_i\)</span> represents the mean of the outcome for observations in category <span class="math inline">\(i\)</span>.<br><span class="math inline">\(H_A\)</span>: At least one mean is different.</p>
<p>Generally we must check three conditions on the data before performing ANOVA with the <span class="math inline">\(F\)</span> distribution:</p>
<ol style="list-style-type: lower-roman">
<li>the observations are independent within and across groups,<br>
</li>
<li>the data within each group are nearly normal, and<br>
</li>
<li>the variability across the groups is about equal.</li>
</ol>
<p>When these three conditions are met, we may perform an ANOVA, using the <span class="math inline">\(F\)</span> distribution, to determine whether the data provide strong evidence against the null hypothesis that all the group means, <span class="math inline">\(\mu_i\)</span>, are equal.</p>
<div id="mlb-batting-performance-1" class="section level3" number="21.3.1">
<h3>
<span class="header-section-number">21.3.1</span> MLB batting performance<a class="anchor" aria-label="anchor" href="#mlb-batting-performance-1"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s revisit the MLB batting performance example. We would like to discern whether there are real differences between the batting performance of baseball players according to their position. We will now consider all four positions from the dataset: outfielder (<code>OF</code>), infielder (<code>IF</code>), designated hitter (<code>DH</code>), and catcher (<code>C</code>). The data is available in the <code>mlb_obp.csv</code> file. As a reminder, batting performance is measured by on-base percentage.</p>
<p>Read the data into <code>R</code>.</p>
<div class="sourceCode" id="cb625"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlb_obp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span><span class="st">"data/mlb_obp.csv"</span><span class="op">)</span></span></code></pre></div>
<p>Let’s review our data:</p>
<div class="sourceCode" id="cb626"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/mosaicCore/man/inspect.html">inspect</a></span><span class="op">(</span><span class="va">mlb_obp</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## categorical variables:  
##       name     class levels   n missing
## 1 position character      4 327       0
##                                    distribution
## 1 IF (47.1%), OF (36.7%), C (11.9%) ...        
## 
## quantitative variables:  
##   name   class   min    Q1 median     Q3   max     mean         sd   n missing
## 1  obp numeric 0.174 0.309  0.331 0.3545 0.437 0.332159 0.03570249 327       0</code></pre>
<p>Next, change the variable <code>position</code> to a factor to give us greater control.</p>
<div class="sourceCode" id="cb628"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlb_obp</span> <span class="op">&lt;-</span> <span class="va">mlb_obp</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>position <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">position</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Let’s look at summary statistics of the on-base percentage by position, this time considering all four positions.</p>
<div class="sourceCode" id="cb629"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/aggregating.html">favstats</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="va">position</span>, data <span class="op">=</span> <span class="va">mlb_obp</span><span class="op">)</span></span></code></pre></div>
<pre><code>##   position   min      Q1 median      Q3   max      mean         sd   n missing
## 1        C 0.219 0.30000 0.3180 0.35700 0.405 0.3226154 0.04513175  39       0
## 2       DH 0.287 0.31625 0.3525 0.36950 0.412 0.3477857 0.03603669  14       0
## 3       IF 0.174 0.30800 0.3270 0.35275 0.437 0.3315260 0.03709504 154       0
## 4       OF 0.265 0.31475 0.3345 0.35300 0.411 0.3342500 0.02944394 120       0</code></pre>
<p>The means for each group are pretty similar to each other.</p>
<blockquote>
<p><strong>Exercise</strong>:
The null hypothesis under consideration is the following: <span class="math inline">\(\mu_{OF} = \mu_{IF} = \mu_{DH} = \mu_{C}\)</span>.
Write the null and corresponding alternative hypotheses in plain language.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;: The average on-base percentage is equal across the four positions. &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;: The average on-base percentage varies across some (or all) groups. That is, the average on-base percentage for at least one position is different.&lt;/p&gt;'><sup>92</sup></a></p>
</blockquote>
<blockquote>
<p><strong>Exercise</strong>:<br>
Construct side-by-side boxplots.</p>
</blockquote>
<p>Figure <a href="ANOVA.html#fig:box231-fig">21.1</a> shows the side-by-side boxplots for all four positions.</p>
<div class="sourceCode" id="cb631"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlb_obp</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_boxplot</span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="va">position</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Position Played"</span>, y <span class="op">=</span> <span class="st">"On-Base Percentage"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Comparison of OBP for different positions"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:box231-fig"></span>
<img src="23-Analysis-of-Variance_files/figure-html/box231-fig-1.png" alt="Boxplots of on-base percentage by position played." width="672"><p class="caption">
Figure 21.1: Boxplots of on-base percentage by position played.
</p>
</div>
<p>The largest difference between the sample means appears to be between the designated hitter and the catcher positions. Consider again the original hypotheses:</p>
<p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_{OF} = \mu_{IF} = \mu_{DH} = \mu_{C}\)</span><br><span class="math inline">\(H_A\)</span>: The average on-base percentage (<span class="math inline">\(\mu_i\)</span>) varies across some (or all) groups.</p>
<blockquote>
<p><em>Thought question</em>:
Why might it be inappropriate to run the test by simply estimating whether the difference of <span class="math inline">\(\mu_{DH}\)</span> and <span class="math inline">\(\mu_{C}\)</span> is statistically significant at an <span class="math inline">\(\alpha = 0.05\)</span> significance level?</p>
</blockquote>
<p>The primary issue here is that we are inspecting the data before picking the groups that will be compared. It is inappropriate to examine all data by eye (informal testing) and only afterwards decide which parts to formally test. This is called <strong>data snooping</strong> or <strong>data fishing</strong>. Naturally, we would pick the groups with the largest differences for the formal test, leading to an inflation in the Type 1 error rate. To understand this better, let’s consider a slightly different problem.</p>
<p>Suppose we are to measure the aptitude for students in 20 classes of a large elementary school at the beginning of the year. In this school, all students are randomly assigned to classrooms, so any differences we observe between the classes at the start of the year are completely due to chance. However, with so many groups, we will probably observe a few groups that look rather different from each other. If we select only the classes that look really different, we will probably make the wrong conclusion that the assignment wasn’t random. While we might only formally test differences for a few pairs of classes, we informally evaluated the other classes by eye before choosing the most extreme cases for a comparison.</p>
<p>In the next section, we will learn how to use the <span class="math inline">\(F\)</span> statistic and ANOVA to test whether observed differences in means could have happened just by chance, even if there was no true difference in the respective population means.</p>
</div>
<div id="analysis-of-variance-anova-and-the-f-test" class="section level3" number="21.3.2">
<h3>
<span class="header-section-number">21.3.2</span> Analysis of variance (ANOVA) and the F test<a class="anchor" aria-label="anchor" href="#analysis-of-variance-anova-and-the-f-test"><i class="fas fa-link"></i></a>
</h3>
<p>The method of analysis of variance (ANOVA) in this context focuses on answering one question: is the variability in the sample means so large that it seems unlikely to be from chance alone? This question is different from earlier testing procedures because we will <em>simultaneously</em> consider many groups, and evaluate whether their sample means differ more than we would expect from natural variation. We call this variability the <strong>mean square between groups</strong> (<span class="math inline">\(MSG\)</span>), and it has an associated degrees of freedom, <span class="math inline">\(df_{G} = k - 1\)</span>, when there are <span class="math inline">\(k\)</span> groups. The <span class="math inline">\(MSG\)</span> can be thought of as a scaled variance formula for means. If the null hypothesis is true, any variation in the sample means is due to chance and shouldn’t be too large. We typically use software to find the <span class="math inline">\(MSG\)</span>; however, the mathematical derivation follows. Let <span class="math inline">\(\bar{x}_i\)</span> represent the mean outcome for observations in group <span class="math inline">\(i\)</span>, and let <span class="math inline">\(\bar{x}\)</span> represent the mean outcome across all groups. Then, the mean square between groups is computed as</p>
<p><span class="math display">\[
MSG = \frac{1}{df_{G}}SSG = \frac{1}{k - 1}\sum_{i = 1}^{k} n_{i}\left(\bar{x}_{i} - \bar{x}\right)^2,
\]</span></p>
<p>where <span class="math inline">\(SSG\)</span> is called the <strong>sum of squares between groups</strong> and <span class="math inline">\(n_{i}\)</span> is the sample size of group <span class="math inline">\(i\)</span>.</p>
<p>The mean square between the groups is, on its own, quite useless in a hypothesis test. We need a benchmark value for how much variability is expected among the sample means, if the null hypothesis is true. To this end, we compute a pooled variance estimate, often abbreviated as the <strong>mean squared error</strong> (<span class="math inline">\(MSE\)</span>), which has an associated degrees of freedom of <span class="math inline">\(df_E = n - k\)</span>. It is helpful to think of the <span class="math inline">\(MSE\)</span> as a measure of the variability within the groups. To find the <span class="math inline">\(MSE\)</span>, the <strong>sum of squares total</strong> (<span class="math inline">\(SST\)</span>)} is computed as</p>
<p><span class="math display">\[SST = \sum_{i = 1}^{n} \left(x_{i} - \bar{x}\right)^2,\]</span></p>
<p>where the sum is over all observations in the data set. Then we compute the <strong>sum of squared errors</strong> (<span class="math inline">\(SSE\)</span>) in one of two equivalent ways:</p>
<p><span class="math display">\[
SSE = SST - SSG = (n_1 - 1)s_1^2 + (n_2 - 1)s_2^2 + \cdots + (n_k - 1)s_k^2,
\]</span></p>
<p>where <span class="math inline">\(s_i^2\)</span> is the sample variance (square of the standard deviation) of the observations in group <span class="math inline">\(i\)</span>. Then the <span class="math inline">\(MSE\)</span> is the standardized form of <span class="math inline">\(SSE\)</span>:</p>
<p><span class="math display">\[MSE = \frac{1}{df_{E}}SSE\]</span></p>
<p>When the null hypothesis is true, any differences among the sample means are only due to chance, and the <span class="math inline">\(MSG\)</span> and <span class="math inline">\(MSE\)</span> should be about equal. For ANOVA, we examine the ratio of <span class="math inline">\(MSG\)</span> and <span class="math inline">\(MSE\)</span> in the <span class="math inline">\(F\)</span> test statistic:</p>
<p><span class="math display">\[F = \frac{MSG}{MSE}\]</span></p>
<p>The <span class="math inline">\(MSG\)</span> represents a measure of the between-group variability, and <span class="math inline">\(MSE\)</span> measures the variability within each of the groups. Using a randomization test, we could also look at the difference in the mean squared errors as a test statistic instead of the ratio.</p>
<p>We can use the <span class="math inline">\(F\)</span> statistic to evaluate the hypotheses in what is called an <strong>F test</strong>. A <span class="math inline">\(p\)</span>-value can be computed from the <span class="math inline">\(F\)</span> statistic using an <span class="math inline">\(F\)</span> distribution, which has two associated parameters: <span class="math inline">\(df_{1}\)</span> and <span class="math inline">\(df_{2}\)</span>. For the <span class="math inline">\(F\)</span> statistic in ANOVA, <span class="math inline">\(df_{1} = df_{G}\)</span> and <span class="math inline">\(df_{2}= df_{E}\)</span>. The <span class="math inline">\(F\)</span> statistic is really a ratio of chi-squared random variables.</p>
<p>The <span class="math inline">\(F\)</span> distribution, shown in Figure <a href="ANOVA.html#fig:dens232-fig">21.2</a>, takes on positive values and is right skewed, like the chi-squared distribution. The larger the observed variability in the sample means (<span class="math inline">\(MSG\)</span>) relative to the within-group observations (<span class="math inline">\(MSE\)</span>), the larger <span class="math inline">\(F\)</span> will be and the stronger the evidence against the null hypothesis. Because larger values of <span class="math inline">\(F\)</span> represent stronger evidence against the null hypothesis, we use the upper tail of the distribution to compute a <span class="math inline">\(p\)</span>-value.</p>
<div class="figure">
<span style="display:block;" id="fig:dens232-fig"></span>
<img src="23-Analysis-of-Variance_files/figure-html/dens232-fig-1.png" alt="The F distribution" width="672"><p class="caption">
Figure 21.2: The F distribution
</p>
</div>
<blockquote>
<p><strong>The <span class="math inline">\(F\)</span> statistic and the <span class="math inline">\(F\)</span> test</strong><br>
Analysis of variance (ANOVA) is used to test whether the mean outcome differs across two or more groups. ANOVA uses a test statistic <span class="math inline">\(F\)</span>, which represents a standardized ratio of variability in the sample means (across the groups), relative to the variability within the groups. If <span class="math inline">\(H_0\)</span> is true and the model assumptions are satisfied, the statistic <span class="math inline">\(F\)</span> follows an <span class="math inline">\(F\)</span> distribution with parameters <span class="math inline">\(df_{1} = k - 1\)</span> and <span class="math inline">\(df_{2} = n - k\)</span>. The upper tail of the <span class="math inline">\(F\)</span> distribution is used to represent the <span class="math inline">\(p\)</span>-value.</p>
</blockquote>
<div id="anova" class="section level4" number="21.3.2.1">
<h4>
<span class="header-section-number">21.3.2.1</span> ANOVA<a class="anchor" aria-label="anchor" href="#anova"><i class="fas fa-link"></i></a>
</h4>
<p>We will use <code>R</code> to perform the calculations for the ANOVA. But let’s check our assumptions first.</p>
<p>There are three conditions we must check before conducting an ANOVA: 1) all observations must be independent, 2) the data in each group must be nearly normal, and 3) the variance within each group must be approximately equal.</p>
<blockquote>
<p><strong>Independence</strong><br>
All observations must be independent. More specifically, observations must be independent within and across groups. If the data are a simple random sample from less than 10% of the population, this assumption is reasonable. For processes and experiments, we must carefully consider whether the data may be independent (e.g., no paired data). In our MLB data, the data were not sampled; they consist of all players from the 2010 season with at least 200 at bats. However, there are not obvious reasons why independence would not hold for most or all observations, given our intended population is all MLB seasons (or something similar). This requires a bit of hand waving, but remember that independence is often difficult to assess.</p>
</blockquote>
<blockquote>
<p><strong>Approximately normal</strong><br>
As with one- and two-sample testing for means, the normality assumption is especially important when the sample size is quite small. When we have larger data sets (and larger groups) and there are no extreme outliers, the normality condition is not usually a concern. The normal probability plots (quantile-quantile plots) for each group of the MLB data are shown below; there is some deviation from normality for infielders, but this isn’t a substantial concern since there are over 150 observations in that group and the outliers are not extreme. Sometimes in ANOVA, there are so many groups or so few observations per group that checking normality for each group isn’t reasonable. One solution is to combine the groups into one set of data. First, calculate the <strong>residuals</strong> of the baseball data, which are calculated by taking the observed values and subtracting the corresponding group means. For example, an outfielder with OBP of 0.435 would have a residual of <span class="math inline">\(0.435 - \bar{x}_{OF} = 0.082\)</span>. Then, to check the normality condition, create a normal probability plot using all the residuals simultaneously.</p>
</blockquote>
<p>Figure <a href="ANOVA.html#fig:qq233-fig">21.3</a> is the quantile-quantile plot to assess the normality assumption.</p>
<div class="sourceCode" id="cb632"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlb_obp</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_qq</span><span class="op">(</span><span class="op">~</span><span class="va">obp</span> <span class="op">|</span> <span class="va">position</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_qqline</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:qq233-fig"></span>
<img src="23-Analysis-of-Variance_files/figure-html/qq233-fig-1.png" alt="Quantile-quantile plot for two-sample test of means." width="672"><p class="caption">
Figure 21.3: Quantile-quantile plot for two-sample test of means.
</p>
</div>
<blockquote>
<p><strong>Constant variance</strong><br>
The final assumption is that the variance within the groups is about equal from one group to the next. This assumption is important to check, especially when the samples sizes vary widely across the groups. The constant variance assumption can be checked by examining a side-by-side box plot of the outcomes across the groups, which we did previously in Figure <a href="ANOVA.html#fig:box231-fig">21.1</a>. In this case, the variability is similar across the four groups but not identical. We also see in the output of <code>favstats</code> that the standard deviation varies a bit from one group to the next. Whether these differences are from natural variation is unclear, so we should report the uncertainty of meeting this assumption when the final results are reported. The permutation test does not have this assumption and can be used as a check on the results from the ANOVA.</p>
</blockquote>
<p>In summary, independence is always important to an ANOVA analysis, but is often difficult to assess. The normality condition is very important when the sample sizes for each group are relatively small. The constant variance condition is especially important when the sample sizes differ between groups.</p>
<p>Let’s write the hypotheses for the MLB example again:</p>
<p><span class="math inline">\(H_0\)</span>: The average on-base percentage is equal across the four positions.<br><span class="math inline">\(H_A\)</span>: The average on-base percentage varies across some (or all) groups.</p>
<p>The test statistic is the ratio of the between-groups variance (mean square between groups, <span class="math inline">\(MSG\)</span>) and the pooled within-group variance (mean squared error, <span class="math inline">\(MSE\)</span>). We perform ANOVA in <code>R</code> using the <code><a href="https://rdrr.io/r/stats/aov.html">aov()</a></code> function, and use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function to extract the most important information from the output.</p>
<div class="sourceCode" id="cb633"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="va">position</span>, data <span class="op">=</span> <span class="va">mlb_obp</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##              Df Sum Sq  Mean Sq F value Pr(&gt;F)
## position      3 0.0076 0.002519   1.994  0.115
## Residuals   323 0.4080 0.001263</code></pre>
<p>This table contains all the information we need. It has the degrees of freedom, sums of squares, mean squared errors, <span class="math inline">\(F\)</span> test statistic, and <span class="math inline">\(p\)</span>-value. The test statistic <span class="math inline">\(\left(\frac{MSG}{MSE}\right)\)</span> is 1.994, <span class="math inline">\(\frac{0.002519}{0.001263} = 1.994\)</span>. The <span class="math inline">\(p\)</span>-value is larger than 0.05, indicating the evidence is not strong enough to reject the null hypothesis at a significance level of 0.05. That is, the data do not provide strong evidence that the average on-base percentage varies by player’s primary field position.</p>
<p>The calculation of the <span class="math inline">\(p\)</span>-value can also be done by finding the probability associated with the upper tail of the <span class="math inline">\(F\)</span> distribution:</p>
<div class="sourceCode" id="cb635"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">pf</a></span><span class="op">(</span><span class="fl">1.994</span>, <span class="fl">3</span>, <span class="fl">323</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.1147443</code></pre>
<p>Figure <a href="ANOVA.html#fig:dens234-fig">21.4</a> is a plot of the <span class="math inline">\(F\)</span> distribution with the observed <span class="math inline">\(F\)</span> test statistic shown as a red line.</p>
<div class="sourceCode" id="cb637"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">gf_dist</span><span class="op">(</span><span class="st">"f"</span>, df1 <span class="op">=</span> <span class="fl">3</span>, df2 <span class="op">=</span> <span class="fl">323</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">1.994</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_classic</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"F distribution"</span>, x <span class="op">=</span> <span class="st">"F value"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:dens234-fig"></span>
<img src="23-Analysis-of-Variance_files/figure-html/dens234-fig-1.png" alt="The F distribution" width="672"><p class="caption">
Figure 21.4: The F distribution
</p>
</div>
</div>
<div id="permutation-test-1" class="section level4" number="21.3.2.2">
<h4>
<span class="header-section-number">21.3.2.2</span> Permutation test<a class="anchor" aria-label="anchor" href="#permutation-test-1"><i class="fas fa-link"></i></a>
</h4>
<p>We can repeat the same analysis using a permutation test. We will first run it using a ratio of variances (mean squares) and then, for interest, as a difference in variances.</p>
<p>We need a way to extract the mean squares from the output. The <strong>broom</strong> package contains a function called <code><a href="https://generics.r-lib.org/reference/tidy.html">tidy()</a></code> that cleans up output from functions and makes them into data frames.</p>
<div class="sourceCode" id="cb638"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb639"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="va">position</span>, data <span class="op">=</span> <span class="va">mlb_obp</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 2 x 6
##   term         df   sumsq  meansq statistic p.value
##   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 position      3 0.00756 0.00252      1.99   0.115
## 2 Residuals   323 0.408   0.00126     NA     NA</code></pre>
<p>Let’s summarize the values in the <code>meansq</code> column and develop our test statistic, the ratio of mean squares. We could just pull the statistic (using the <code><a href="https://dplyr.tidyverse.org/reference/pull.html">pull()</a></code> function) but we want to be able to generate a different test statistic, the difference of mean squares, as well.</p>
<div class="sourceCode" id="cb641"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="va">position</span>, data <span class="op">=</span> <span class="va">mlb_obp</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">/</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> </span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##    stat
##   &lt;dbl&gt;
## 1  1.99</code></pre>
<p>Now we are ready. First, get our observed test statistic using <code><a href="https://dplyr.tidyverse.org/reference/pull.html">pull()</a></code>.</p>
<div class="sourceCode" id="cb643"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="va">position</span>, data <span class="op">=</span> <span class="va">mlb_obp</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">/</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">obs</span></span></code></pre></div>
<pre><code>## [1] 1.994349</code></pre>
<p>Let’s put our test statistic into a function that includes shuffling the <code>position</code> variable.</p>
<div class="sourceCode" id="cb645"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_stat</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/resample.html">shuffle</a></span><span class="op">(</span><span class="va">position</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">/</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Now, we run our function.</p>
<div class="sourceCode" id="cb646"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">5321</span><span class="op">)</span></span>
<span><span class="fu">f_stat</span><span class="op">(</span><span class="va">mlb_obp</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.1185079</code></pre>
<p>Next, we run the randomization test using the <code><a href="https://dplyr.tidyverse.org/reference/do.html">do()</a></code> function. There is an easier way to do all of this work with the <strong>purrr</strong> package but we will continue with the work we have started.</p>
<div class="sourceCode" id="cb648"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">5321</span><span class="op">)</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/do.html">do</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="fu">f_stat</span><span class="op">(</span><span class="va">mlb_obp</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The above code is slow in executing because the <strong>tidyverse</strong> functions inside our <code>f_stat()</code> function are slow.</p>
<p>Figure <a href="ANOVA.html#fig:hist235-fig">21.5</a> is a plot of the sampling distribution from the randomization test. The <span class="math inline">\(F\)</span> distribution is overlaid as a dark blue curve.</p>
<div class="sourceCode" id="cb649"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_dhistogram</span><span class="op">(</span><span class="op">~</span><span class="va">result</span>, fill <span class="op">=</span> <span class="st">"cyan"</span>, color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_dist</span><span class="op">(</span><span class="st">"f"</span>, df1 <span class="op">=</span> <span class="fl">3</span>, df2 <span class="op">=</span> <span class="fl">323</span>, color <span class="op">=</span> <span class="st">"darkblue"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">1.994</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_classic</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Randomization test sampling distribution"</span>,</span>
<span>          subtitle <span class="op">=</span> <span class="st">"Test statistic is the ratio of variances"</span>,</span>
<span>          x <span class="op">=</span> <span class="st">"Test statistic"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:hist235-fig"></span>
<img src="23-Analysis-of-Variance_files/figure-html/hist235-fig-1.png" alt="The sampling distribution of the ratio of variances randomization test statistic." width="672"><p class="caption">
Figure 21.5: The sampling distribution of the ratio of variances randomization test statistic.
</p>
</div>
<p>The <span class="math inline">\(p\)</span>-value is</p>
<div class="sourceCode" id="cb650"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/mosaicCore/man/prop.html">prop1</a></span><span class="op">(</span><span class="op">~</span><span class="op">(</span><span class="va">result</span> <span class="op">&gt;=</span> <span class="va">obs</span><span class="op">)</span>, <span class="va">results</span><span class="op">)</span></span></code></pre></div>
<pre><code>## prop_TRUE 
## 0.0959041</code></pre>
<p>This is a similar <span class="math inline">\(p\)</span>-value to the ANOVA output.</p>
<p>Now, let’s repeat the analysis but use the difference in variance (mean squares) as our test statistic. We’ll define a new function that calculates the test statistic and then run the randomization test.</p>
<div class="sourceCode" id="cb652"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_stat2</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/resample.html">shuffle</a></span><span class="op">(</span><span class="va">position</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">stat</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="sourceCode" id="cb653"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">5321</span><span class="op">)</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/do.html">do</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="fu">f_stat2</span><span class="op">(</span><span class="va">mlb_obp</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Figure <a href="ANOVA.html#fig:hist236-fig">21.6</a> is the plot of the sampling distribution of the difference in variances.</p>
<div class="sourceCode" id="cb654"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_dhistogram</span><span class="op">(</span><span class="op">~</span><span class="va">result</span>, fill <span class="op">=</span> <span class="st">"cyan"</span>, color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0.001255972</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_classic</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Randomization test sampling distribution"</span>,</span>
<span>          subtitle <span class="op">=</span> <span class="st">"Test statistic is the difference in variances"</span>,</span>
<span>          x <span class="op">=</span> <span class="st">"Test statistic"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:hist236-fig"></span>
<img src="23-Analysis-of-Variance_files/figure-html/hist236-fig-1.png" alt="The sampling distribution of the difference in variances randomization test statistic." width="672"><p class="caption">
Figure 21.6: The sampling distribution of the difference in variances randomization test statistic.
</p>
</div>
<p>We need the observed test statistic in order to calculate a <span class="math inline">\(p\)</span>-value.</p>
<div class="sourceCode" id="cb655"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span><span class="va">obp</span> <span class="op">~</span> <span class="va">position</span>, data <span class="op">=</span> <span class="va">mlb_obp</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>stat <span class="op">=</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">meansq</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="va">stat</span><span class="op">)</span></span>
<span><span class="va">obs</span></span></code></pre></div>
<pre><code>## [1] 0.001255972</code></pre>
<p>The <span class="math inline">\(p\)</span>-value is</p>
<div class="sourceCode" id="cb657"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/mosaicCore/man/prop.html">prop1</a></span><span class="op">(</span><span class="op">~</span><span class="op">(</span><span class="va">result</span> <span class="op">&gt;=</span> <span class="va">obs</span><span class="op">)</span>, <span class="va">results</span><span class="op">)</span></span></code></pre></div>
<pre><code>## prop_TRUE 
## 0.0959041</code></pre>
<p>Again, we find a similar <span class="math inline">\(p\)</span>-value.</p>
<blockquote>
<p><strong>Exercise</strong>:
If we reject the null hypothesis in the ANOVA test, we know that at least one mean is different but we don’t know which one(s). How would you approach answering the question of which means are different?</p>
</blockquote>
</div>
</div>
</div>
<div id="homework-problems-20" class="section level2" number="21.4">
<h2>
<span class="header-section-number">21.4</span> Homework Problems<a class="anchor" aria-label="anchor" href="#homework-problems-20"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>
<strong>Census data.</strong> A group of researchers wants to conduct an Analysis of Variance to investigate whether there is a relationship between marital status and total personal income. They obtained a random sample of 500 observations from the 2000 U.S. Census. The data is available in the <code>census</code> data set in the <strong>openintro</strong> package.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>State the null and alternative hypotheses in context of the research problem. Note: there are six different marital status types.</p></li>
<li><p>Using the <code><a href="https://rdrr.io/r/stats/aov.html">aov()</a></code> function, conduct an ANOVA using a significance level of <span class="math inline">\(\alpha = 0.05\)</span>. Clearly state your conclusion.</p></li>
<li><p>Is an ANOVA, using the <span class="math inline">\(F\)</span> distribution, appropriate for this data set? Why or why not? Clearly communicate your answer, including appropriate data visualizations.</p></li>
<li><p>Repeat part b), but use a randomization test this time. You should use <span class="math inline">\(F\)</span>, the test statistic for ANOVA, as your test statistic.</p></li>
<li><p>How do we determine which groups are different?</p></li>
<li><p>Below is a boxplot of the total personal income by marital status. We have “zoomed in” on the <span class="math inline">\(y\)</span>-axis, considering only total personal income between $0 and $100,000, and rotated the text on the <span class="math inline">\(x\)</span>-axis. The largest difference between sample means appears to be between the separated and married/spouse absent groups. Why can’t we simply determine whether there is a statistically significant difference between <span class="math inline">\(\mu_{\text{absent}}\)</span> and <span class="math inline">\(\mu_{\text{separated}}\)</span>?</p></li>
</ol>
<div class="sourceCode" id="cb659"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">census</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_boxplot</span><span class="op">(</span><span class="va">total_personal_income</span> <span class="op">~</span> <span class="va">marital_status</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_lims</span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">100000</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">gf_theme</span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">45</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="23-Analysis-of-Variance_files/figure-html/unnamed-chunk-19-1.png" width="672"></div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="ADDTESTS.html"><span class="header-section-number">20</span> Additional Hypothesis Tests</a></div>
<div class="next"><a href="CI.html"><span class="header-section-number">22</span> Confidence Intervals</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ANOVA"><span class="header-section-number">21</span> Analysis of Variance</a></li>
<li><a class="nav-link" href="#objectives-21"><span class="header-section-number">21.1</span> Objectives</a></li>
<li><a class="nav-link" href="#introduction-3"><span class="header-section-number">21.2</span> Introduction</a></li>
<li>
<a class="nav-link" href="#comparing-more-than-two-means"><span class="header-section-number">21.3</span> Comparing more than two means</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#mlb-batting-performance-1"><span class="header-section-number">21.3.1</span> MLB batting performance</a></li>
<li><a class="nav-link" href="#analysis-of-variance-anova-and-the-f-test"><span class="header-section-number">21.3.2</span> Analysis of variance (ANOVA) and the F test</a></li>
</ul>
</li>
<li><a class="nav-link" href="#homework-problems-20"><span class="header-section-number">21.4</span> Homework Problems</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/DS-USAFA/Computational-Probability-and-Statistics/blob/master/23-Analysis-of-Variance.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/DS-USAFA/Computational-Probability-and-Statistics/edit/master/23-Analysis-of-Variance.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Probability and Statistics for Scientists and Engineers</strong>" was written by Matthew Davis, Brianna Hitt, Ken Horton, Kris Pruitt, Bradley Warner. It was last built on 2022-07-21.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
